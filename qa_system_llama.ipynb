{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ab726f-533a-48ce-b70a-020807fe1971",
   "metadata": {},
   "source": [
    "## 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1230f-3706-44af-a1bd-46fde623ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "# ===== 모델 매핑 =====\n",
    "MODEL_NAME = \"bge-ko\"   # ← 여기만 바꿔주면 됨 (\"e5\", \"koe5\", \"bge-ko\", \"bge-m3\", \"kure\")\n",
    "\n",
    "MODEL_MAP = {\n",
    "    \"e5\": \"intfloat/multilingual-e5-small\",\n",
    "    \"e5-base\": \"intfloat/multilingual-e5-base\",\n",
    "    \"e5-large\": \"intfloat/multilingual-e5-large\",\n",
    "    \"koe5\": \"nlpai-lab/KoE5\",\n",
    "    \"bge-ko\": \"dragonkue/bge-m3-ko\",\n",
    "    \"bge-m3\": \"BAAI/bge-m3\",\n",
    "    \"kure\": \"nlpai-lab/KURE-v1\",\n",
    "}\n",
    "\n",
    "EMBED_MODEL_NAME = MODEL_MAP[MODEL_NAME]\n",
    "COLLECTION       = \"audit_chunks\"\n",
    "BATCH            = 64\n",
    "QDRANT_PATH      = f\"data/vector_store/final-sjchunk/{MODEL_NAME}-qdrant_db\"\n",
    "CHUNK_FILE       = Path(\"data/structured/enhanced_vector_chunks_9_24.jsonl\")\n",
    "\n",
    "# 토크나이저 포크 경고 끄기 (권장)\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# ===== 디바이스 선택 (MPS > CUDA > CPU) =====\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# ===== 모델 로드 =====\n",
    "model = SentenceTransformer(EMBED_MODEL_NAME, device=device)\n",
    "dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"[INFO] Loaded model: {MODEL_NAME} ({EMBED_MODEL_NAME}), dim={dim}\")\n",
    "\n",
    "# ===== corpus 로드 =====\n",
    "assert CHUNK_FILE.exists(), f\"청킹 파일을 찾을 수 없습니다: {CHUNK_FILE}\"\n",
    "\n",
    "corpus = []\n",
    "with open(CHUNK_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        corpus.append(json.loads(line))\n",
    "\n",
    "print(f\"[INFO] Loaded corpus: {len(corpus)} chunks\")\n",
    "\n",
    "client = None\n",
    "try:\n",
    "    # ===== Qdrant 연결 (임베디드 모드) =====\n",
    "    client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "    # 컬렉션 생성 (존재하지 않을 때만)\n",
    "    if not client.collection_exists(COLLECTION):\n",
    "        client.create_collection(\n",
    "            collection_name=COLLECTION,\n",
    "            vectors_config=qmodels.VectorParams(\n",
    "                size=dim,\n",
    "                distance=qmodels.Distance.COSINE\n",
    "            ),\n",
    "            optimizers_config=qmodels.OptimizersConfigDiff(indexing_threshold=20000),\n",
    "            hnsw_config=qmodels.HnswConfigDiff(m=32, ef_construct=256),\n",
    "        )\n",
    "        print(f\"[INFO] Created collection: {COLLECTION}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Using existing collection: {COLLECTION}\")\n",
    "\n",
    "    # ===== 업서트 =====\n",
    "    pending_points = []\n",
    "    for i in tqdm(range(0, len(corpus), BATCH), desc=f\"Upserting ({MODEL_NAME})\"):\n",
    "        batch = corpus[i:i+BATCH]\n",
    "        texts = [x[\"text\"] for x in batch]\n",
    "\n",
    "        vecs = model.encode(\n",
    "            texts,\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            batch_size=BATCH,\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        pending_points.clear()\n",
    "        for x, v in zip(batch, vecs):\n",
    "            pid = str(uuid.uuid4())   # ✅ 무조건 올바른 UUID 생성\n",
    "            payload = {**x.get(\"metadata\", {}), \"text\": x[\"text\"]}\n",
    "            pending_points.append(\n",
    "                qmodels.PointStruct(id=pid, vector=v.tolist(), payload=payload)\n",
    "            )\n",
    "\n",
    "\n",
    "        client.upsert(collection_name=COLLECTION, points=pending_points, wait=False)\n",
    "\n",
    "    try:\n",
    "        client.update_collection(\n",
    "            collection_name=COLLECTION,\n",
    "            hnsw_config=qmodels.HnswConfigDiff(ef_construct=256),\n",
    "            optimizers_config=qmodels.OptimizersConfigDiff(default_segment_number=4),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[INFO] Upsert done: {len(corpus)} → DB path={QDRANT_PATH}\")\n",
    "\n",
    "finally:\n",
    "    if client is not None:\n",
    "        try:\n",
    "            client.close()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e5306-14fc-46b1-9858-3be6ef089de6",
   "metadata": {},
   "source": [
    "## Retriever 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a34639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "질문: 2014년 재무상태표 상 당기 유동자산은 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2014년 (당기) 유동자산는 62,054,773백만원입니다.\n",
      "   출처: 2014년, (score=0.8690)\n",
      "\n",
      "2위: 재무상태표에서 2014년 (당기) 비유동자산는 102,005,810백만원입니다.\n",
      "   출처: 2014년, (score=0.7909)\n",
      "\n",
      "3위: 재무상태표에서 2014년 (당기) 기타유동자산는 821,079백만원입니다.\n",
      "   출처: 2014년, (score=0.7762)\n",
      "\n",
      "\n",
      "[정답]\n",
      "62,054,773\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2014년 현금흐름표 상 당기 영업활동 현금흐름은 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 현금흐름표에서 2014년 (당기) 영업활동현금흐름는 18,653,817백만원입니다.\n",
      "   출처: 2014년, (score=0.8249)\n",
      "\n",
      "2위: 현금흐름표에서 2014년 (당기) 투자활동현금흐름는 -15,951,220백만원입니다.\n",
      "   출처: 2014년, (score=0.7903)\n",
      "\n",
      "3위: 현금흐름표에서 2014년 (당기) 재무활동현금흐름는 -3,089,586백만원입니다.\n",
      "   출처: 2014년, (score=0.7843)\n",
      "\n",
      "\n",
      "[정답]\n",
      "18,653,817\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2015년 당기 비유동자산은 재무상태표에서 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2015년 (당기) 비유동자산는 101,967,575백만원입니다.\n",
      "   출처: 2015년, (score=0.8659)\n",
      "\n",
      "2위: 재무상태표에서 2015년 (당기) 기타비유동자산는 1,331,840백만원입니다.\n",
      "   출처: 2015년, (score=0.7951)\n",
      "\n",
      "3위: 재무상태표에서 2015년 (당기) 비유동부채는 2,910,887백만원입니다.\n",
      "   출처: 2015년, (score=0.7820)\n",
      "\n",
      "\n",
      "[정답]\n",
      "101,967,575\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2015년 손익계산서 상 당기순이익은 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 손익계산서에서 2015년 (당기) 당기순이익는 12,238,469백만원입니다.\n",
      "   출처: 2015년, (score=0.8365)\n",
      "\n",
      "2위: 손익계산서에서 2015년 (당기) 영업이익는 13,398,215백만원입니다.\n",
      "   출처: 2015년, (score=0.7839)\n",
      "\n",
      "3위: 포괄손익계산서에서 2015년 (당기) 당기순이익는 12,238,469백만원입니다.\n",
      "   출처: 2015년, (score=0.7779)\n",
      "\n",
      "\n",
      "[정답]\n",
      "12,238,469\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2016년 재무상태표 상 당기 단기금융상품은 얼마인가요?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2016년 (당기) 단기금융상품는 30,170,656백만원입니다. 주석: 5, 6, 7, 31\n",
      "   출처: 2016년, (score=0.8399)\n",
      "\n",
      "2위: 재무상태표에서 2016년 (당기) 단기차입금는 9,061,167백만원입니다. 주석: 6, 8, 15, 31\n",
      "   출처: 2016년, (score=0.7083)\n",
      "\n",
      "3위: 현금흐름표에서 2016년 (당기) 단기금융상품의순증가는 -1,407,068백만원입니다.\n",
      "   출처: 2016년, (score=0.6852)\n",
      "\n",
      "\n",
      "[정답]\n",
      "30,170,656\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2016년 포괄손익계산서 상 당기 총포괄이익은 얼마니?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 포괄손익계산서에서 2016년 (당기) 총포괄이익는 11,887,806백만원입니다.\n",
      "   출처: 2016년, (score=0.8544)\n",
      "\n",
      "2위: 포괄손익계산서에서 2016년 (당기) 당기순이익는 11,579,749백만원입니다.\n",
      "   출처: 2016년, (score=0.8298)\n",
      "\n",
      "3위: 포괄손익계산서에서 2016년 (당기) 후속적으로당기손익으로재분류되는포괄손익는 -421,577백만원입니다.\n",
      "   출처: 2016년, (score=0.7659)\n",
      "\n",
      "\n",
      "[정답]\n",
      "11,887,806\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2016년 자본변동표 상 자기주식의 취득은 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 현금흐름표에서 2016년 (당기) 자기주식의취득는 -7,707,938백만원입니다.\n",
      "   출처: 2016년, (score=0.7548)\n",
      "\n",
      "2위: 현금흐름표에서 2016년 (당기) 종속기업,관계기업및공동기업투자의취득는 -4,648,008백만원입니다.\n",
      "   출처: 2016년, (score=0.6351)\n",
      "\n",
      "3위: 현금흐름표에서 2015년 (전기) 자기주식의취득는 -5,015,112백만원입니다.\n",
      "   출처: 2016년, (score=0.6057)\n",
      "\n",
      "\n",
      "[정답]\n",
      "(7,707,938)\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2017년 당기 매출채권은 재무상태표에 따르면 얼마냐?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2017년 (당기) 매출채권는 27,881,777백만원입니다. 주석: 6, 7, 10, 31\n",
      "   출처: 2017년, (score=0.7888)\n",
      "\n",
      "2위: 재무상태표에서 2017년 (당기) 유동부채는 44,495,084백만원입니다.\n",
      "   출처: 2017년, (score=0.6933)\n",
      "\n",
      "3위: 재무상태표에서 2017년 (당기) 비유동부채는 2,176,501백만원입니다.\n",
      "   출처: 2017년, (score=0.6674)\n",
      "\n",
      "\n",
      "[정답]\n",
      "27,881,777\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2017년 재무상태표상 전기 현금및현금성자산은 얼마입니까?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2017년 (당기) 현금및현금성자산는 2,763,768백만원입니다. 주석: 4, 6, 7, 31\n",
      "   출처: 2017년, (score=0.7735)\n",
      "\n",
      "2위: 재무상태표에서 2016년 (전기) 현금및현금성자산는 3,778,371백만원입니다. 주석: 4, 6, 7, 31\n",
      "   출처: 2017년, (score=0.7730)\n",
      "\n",
      "3위: 현금흐름표에서 2017년 (당기) 기말의현금및현금성자산는 2,763,768백만원입니다.\n",
      "   출처: 2017년, (score=0.7186)\n",
      "\n",
      "\n",
      "[정답]\n",
      "3,778,371\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=0.63, MRR=0.50\n",
      "====================================================================================================\n",
      "질문: 2018년 당기 미수금은 재무상태표에서 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2018년 (당기) 미수금는 1,515,079백만원입니다.\n",
      "   출처: 2018년, (score=0.8794)\n",
      "\n",
      "2위: 재무상태표에서 2018년 (당기) 미지급금는 8,385,752백만원입니다.\n",
      "   출처: 2018년, (score=0.8013)\n",
      "\n",
      "3위: 재무상태표에서 2018년 (당기) 미지급비용는 6,129,837백만원입니다.\n",
      "   출처: 2018년, (score=0.7870)\n",
      "\n",
      "\n",
      "[정답]\n",
      "1,515,079\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2018년 손익계산서상 매출총이익은 얼마인가요?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 손익계산서에서 2018년 (당기) 매출총이익는 68,715,364백만원입니다.\n",
      "   출처: 2018년, (score=0.8518)\n",
      "\n",
      "2위: 손익계산서에서 2018년 (당기) 매출액는 170,381,870백만원입니다.\n",
      "   출처: 2018년, (score=0.8069)\n",
      "\n",
      "3위: 손익계산서에서 2018년 (당기) 매출원가는 101,666,506백만원입니다.\n",
      "   출처: 2018년, (score=0.7938)\n",
      "\n",
      "\n",
      "[정답]\n",
      "68,715,364\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2019년 재무상태표상 종속기업, 관계기업 및 공동기업 투자는 얼마인가요?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2019년 (당기) 종속기업,관계기업및공동기업투자는 56,571,252백만원입니다.\n",
      "   출처: 2019년, (score=0.8636)\n",
      "\n",
      "2위: 재무상태표에서 2018년 (전기) 종속기업,관계기업및공동기업투자는 55,959,745백만원입니다.\n",
      "   출처: 2019년, (score=0.7233)\n",
      "\n",
      "3위: 현금흐름표에서 2019년 (당기) 종속기업,관계기업및공동기업투자의처분는 58,677백만원입니다.\n",
      "   출처: 2019년, (score=0.6817)\n",
      "\n",
      "\n",
      "[정답]\n",
      "56,571,252\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2019년 현금흐름표 상 이익잉여금 배당은 얼마인가요?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 현금흐름표에서 2019년 (당기) 배당금의지급는 -9,618,210백만원입니다.\n",
      "   출처: 2019년, (score=0.7412)\n",
      "\n",
      "2위: 현금흐름표에서 2019년 (당기) 배당금수입는 4,625,181백만원입니다.\n",
      "   출처: 2019년, (score=0.7369)\n",
      "\n",
      "3위: 재무상태표에서 2019년 (당기) 이익잉여금는 172,288,326백만원입니다.\n",
      "   출처: 2019년, (score=0.6984)\n",
      "\n",
      "\n",
      "[정답]\n",
      "(9,618,210)\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2019년 손익계산서상 기본주당이익은 얼마인가요?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 손익계산서에서 2019년 (당기) 기본주당이익(단위:원)는 2,260백만원입니다.\n",
      "   출처: 2019년, (score=0.8476)\n",
      "\n",
      "2위: 손익계산서에서 2019년 (당기) 영업이익는 14,115,067백만원입니다.\n",
      "   출처: 2019년, (score=0.6939)\n",
      "\n",
      "3위: 재무상태표에서 2019년 (당기) 보통주자본금는 778,047백만원입니다.\n",
      "   출처: 2019년, (score=0.6910)\n",
      "\n",
      "\n",
      "[정답]\n",
      "2,260\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2020년 재무상태표 상 자산총계는?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2020년 (당기) 자산총계는 229,664,427백만원입니다.\n",
      "   출처: 2020년, (score=0.8454)\n",
      "\n",
      "2위: 재무상태표에서 2020년 (당기) 부채와자본총계는 229,664,427백만원입니다.\n",
      "   출처: 2020년, (score=0.7670)\n",
      "\n",
      "3위: 재무상태표에서 2020년 (당기) 자본총계는 183,316,724백만원입니다.\n",
      "   출처: 2020년, (score=0.7505)\n",
      "\n",
      "\n",
      "[정답]\n",
      "229,664,427\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2020년 손익계산서 상 판매비와관리비는 얼마인가요?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 손익계산서에서 2020년 (당기) 판매비와관리비는 29,038,798백만원입니다.\n",
      "   출처: 2020년, (score=0.9002)\n",
      "\n",
      "2위: 손익계산서에서 2020년 (당기) 기타판매비와관리비는 1,878,587백만원입니다.\n",
      "   출처: 2020년, (score=0.8431)\n",
      "\n",
      "3위: 손익계산서에서 2020년 (당기) 판매촉진비는 896,646백만원입니다.\n",
      "   출처: 2020년, (score=0.7333)\n",
      "\n",
      "\n",
      "[정답]\n",
      "29,038,798\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2021년 재무상태표상 당기 기타포괄손익-공정가치금융자산은 얼마인가요?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2021년 (당기) 기타포괄손익-공정가치금융자산는 1,662,532백만원입니다. 주석: 4, 6, 28\n",
      "   출처: 2021년, (score=0.8443)\n",
      "\n",
      "2위: 재무상태표에서 2021년 (당기) 당기손익-공정가치금융자산는 2,135백만원입니다. 주석: 4, 6, 28\n",
      "   출처: 2021년, (score=0.8137)\n",
      "\n",
      "3위: 포괄손익계산서에서 2021년 (당기) 기타포괄손익-공정가치금융자산평가손익는 -99,916백만원입니다.\n",
      "   출처: 2021년, (score=0.8094)\n",
      "\n",
      "\n",
      "[정답]\n",
      "1,662,532\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2021년 손익계산서 상 당기 금융비용은 얼마인가요?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 손익계산서에서 2021년 (당기) 금융비용는 3,698,675백만원입니다.\n",
      "   출처: 2021년, (score=0.8720)\n",
      "\n",
      "2위: 손익계산서에서 2021년 (당기) 금융수익는 3,796,979백만원입니다.\n",
      "   출처: 2021년, (score=0.7867)\n",
      "\n",
      "3위: 손익계산서에서 2021년 (당기) 서비스비는 1,675,628백만원입니다.\n",
      "   출처: 2021년, (score=0.7448)\n",
      "\n",
      "\n",
      "[정답]\n",
      "3,698,675\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2022년 재무상태표상 당기 비유동부채는 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2022년 (당기) 비유동부채는 4,581,512백만원입니다.\n",
      "   출처: 2022년, (score=0.8602)\n",
      "\n",
      "2위: 재무상태표에서 2022년 (당기) 비유동자산는 201,021,092백만원입니다.\n",
      "   출처: 2022년, (score=0.7865)\n",
      "\n",
      "3위: 재무상태표에서 2022년 (당기) 유동부채는 46,086,047백만원입니다.\n",
      "   출처: 2022년, (score=0.7676)\n",
      "\n",
      "\n",
      "[정답]\n",
      "4,581,512\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2022년 손익계산서 상 당기 법인세비용은 얼마니?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 손익계산서에서 2022년 (당기) 법인세비용는 4,273,142백만원입니다.\n",
      "   출처: 2022년, (score=0.8786)\n",
      "\n",
      "2위: 손익계산서에서 2022년 (당기) 법인세비용차감전순이익는 29,691,920백만원입니다.\n",
      "   출처: 2022년, (score=0.7640)\n",
      "\n",
      "3위: 손익계산서에서 2022년 (당기) 계는 33,958,761백만원입니다.\n",
      "   출처: 2022년, (score=0.7184)\n",
      "\n",
      "\n",
      "[정답]\n",
      "4,273,142\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2022년 당기 현금흐름표 상 투자활동 현금흐름은 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 현금흐름표에서 2022년 (당기) 투자활동현금흐름는 -28,123,886백만원입니다.\n",
      "   출처: 2022년, (score=0.8411)\n",
      "\n",
      "2위: 현금흐름표에서 2022년 (당기) 영업활동현금흐름는 44,788,749백만원입니다.\n",
      "   출처: 2022년, (score=0.7865)\n",
      "\n",
      "3위: 현금흐름표에서 2022년 (당기) 재무활동현금흐름는 -16,665,064백만원입니다.\n",
      "   출처: 2022년, (score=0.7702)\n",
      "\n",
      "\n",
      "[정답]\n",
      "(28,123,886)\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2023년 재무상태표 상 재고자산은 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2023년 (당기) 재고자산는 29,338,151백만원입니다. 주석: 8\n",
      "   출처: 2023년, (score=0.8332)\n",
      "\n",
      "2위: 재무상태표에서 2023년 (당기) 유동자산는 68,548,442백만원입니다.\n",
      "   출처: 2023년, (score=0.7451)\n",
      "\n",
      "3위: 재무상태표에서 2023년 (당기) 비유동자산는 228,308,847백만원입니다.\n",
      "   출처: 2023년, (score=0.7322)\n",
      "\n",
      "\n",
      "[정답]\n",
      "29,338,151\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2023년 손익계산서 상 당기 영업이익은 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 손익계산서에서 2023년 (당기) 영업이익(손실)는 -11,526,297백만원입니다.\n",
      "   출처: 2023년, (score=0.8269)\n",
      "\n",
      "2위: 손익계산서에서 2023년 (당기) 당기순이익는 25,397,099백만원입니다.\n",
      "   출처: 2023년, (score=0.8227)\n",
      "\n",
      "3위: 손익계산서에서 2023년 (당기) 매출총이익는 26,350,538백만원입니다.\n",
      "   출처: 2023년, (score=0.8162)\n",
      "\n",
      "\n",
      "[정답]\n",
      "(11,526,297)\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2024년에는 재무상태표상 당기 무형자산이 얼마야?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2024년 (당기) 무형자산는 10,496,956백만원입니다. 주석: 11\n",
      "   출처: 2024년, (score=0.7829)\n",
      "\n",
      "2위: 재무상태표에서 2024년 (당기) 비유동자산는 242,645,805백만원입니다.\n",
      "   출처: 2024년, (score=0.7448)\n",
      "\n",
      "3위: 재무상태표에서 2024년 (당기) 유동자산는 82,320,322백만원입니다.\n",
      "   출처: 2024년, (score=0.7144)\n",
      "\n",
      "\n",
      "[정답]\n",
      "10,496,956\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2024년 재무상태표 상 당기 우선주자본금은 얼마인가?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 재무상태표에서 2024년 (당기) 우선주자본금는 119,467백만원입니다.\n",
      "   출처: 2024년, (score=0.8726)\n",
      "\n",
      "2위: 재무상태표에서 2024년 (당기) 보통주자본금는 778,047백만원입니다.\n",
      "   출처: 2024년, (score=0.7104)\n",
      "\n",
      "3위: 재무상태표에서 2023년 (전기) 우선주자본금는 119,467백만원입니다.\n",
      "   출처: 2024년, (score=0.7053)\n",
      "\n",
      "\n",
      "[정답]\n",
      "119,467\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "질문: 2024년 손익계산서상 당기 법인세비용은 얼마야?\n",
      "\n",
      "[검색 결과 Top-3]\n",
      "1위: 손익계산서에서 2024년 (당기) 법인세비용(수익)는 -1,832,987백만원입니다.\n",
      "   출처: 2024년, (score=0.8317)\n",
      "\n",
      "2위: 손익계산서에서 2024년 (당기) 법인세비용차감전순이익는 21,749,578백만원입니다.\n",
      "   출처: 2024년, (score=0.7575)\n",
      "\n",
      "3위: 손익계산서에서 2024년 (당기) 소계는 14,471,718백만원입니다.\n",
      "   출처: 2024년, (score=0.7231)\n",
      "\n",
      "\n",
      "[정답]\n",
      "(1,832,987)\n",
      "\n",
      "[성능 지표]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "\n",
      "====================================================================================================\n",
      "=== 평균 성능 지표 ===\n",
      "Precision@3    0.333\n",
      "Recall@3       1.000\n",
      "F1@3           0.500\n",
      "MRR            0.981\n",
      "nDCG@3         0.986\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "\n",
    "# ===== 임베딩 모델 로드 =====\n",
    "embed_model = SentenceTransformer(\"dragonkue/bge-m3-ko\")\n",
    "\n",
    "# ===== 숫자만 추출 함수 =====\n",
    "def extract_numbers(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return \"\".join(re.findall(r\"\\d+\", text.replace(\",\", \"\")))\n",
    "\n",
    "# ===== Retriever 함수 =====\n",
    "def extract_year_from_query(query: str):\n",
    "    \"\"\"질문에서 연도(4자리 숫자) 추출\"\"\"\n",
    "    match = re.search(r\"(20\\d{2}|19\\d{2})\", query)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "    \n",
    "def dense_search(query: str, model, client, collection_name: str,\n",
    "                 top_k: int = 3, ground_truth=None):\n",
    "    \"\"\"\n",
    "    Qdrant query_points 기반 Dense Retriever 함수 + 성능지표 계산\n",
    "    \"\"\"\n",
    "    # 0. 질문에서 연도 추출\n",
    "    year = extract_year_from_query(query)\n",
    "    \n",
    "    # 1. 쿼리 임베딩 생성\n",
    "    qv = model.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "    # 2. Qdrant 검색 실행 (연도 필터 있으면 적용)\n",
    "    query_filter = None\n",
    "    if year:\n",
    "        query_filter = qmodels.Filter(\n",
    "            must=[\n",
    "                qmodels.FieldCondition(\n",
    "                    key=\"report_year\",\n",
    "                    match=qmodels.MatchValue(value=year)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=query_filter  # ✅ 필터 적용\n",
    "    )\n",
    "\n",
    "    # 3. 결과 정리 (payload 전체 반영)\n",
    "    output = []\n",
    "    for r in results.points:\n",
    "        payload = r.payload or {}\n",
    "        result_item = {\n",
    "            \"score\": r.score,\n",
    "            \"text\": payload.get(\"text\")\n",
    "        }\n",
    "        # metadata 안의 모든 키-값을 추가\n",
    "        if \"metadata\" in payload:\n",
    "            result_item.update(payload[\"metadata\"])\n",
    "        else:\n",
    "            # 혹시 metadata 키 없이 flat하게 들어온 경우\n",
    "            result_item.update(payload)\n",
    "        output.append(result_item)\n",
    "\n",
    "    # 4. 성능 지표 계산\n",
    "    metrics = {}\n",
    "    if ground_truth:\n",
    "        # 정답 숫자만 추출\n",
    "        normalized_gt = [extract_numbers(gt) for gt in ground_truth]\n",
    "\n",
    "        # 각 검색 결과가 정답과 매칭되는지 여부\n",
    "        used = set()\n",
    "        relevances = []\n",
    "        for r in output:\n",
    "            nums = extract_numbers(r[\"text\"])\n",
    "            hit = 0\n",
    "            for gt in normalized_gt:\n",
    "                if gt and gt in nums and gt not in used:\n",
    "                    hit = 1\n",
    "                    used.add(gt)\n",
    "                    break\n",
    "            relevances.append(hit)\n",
    "\n",
    "        # Precision@3\n",
    "        precision = sum(relevances) / top_k if top_k > 0 else 0.0\n",
    "        metrics[\"Precision@3\"] = precision\n",
    "\n",
    "        # Recall@3 (전체 정답 대비 비율)\n",
    "        recall = min(sum(relevances), len(normalized_gt)) / len(normalized_gt) if normalized_gt else 0.0\n",
    "        metrics[\"Recall@3\"] = recall\n",
    "\n",
    "        # F1@3\n",
    "        if precision + recall > 0:\n",
    "            metrics[\"F1@3\"] = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            metrics[\"F1@3\"] = 0.0\n",
    "\n",
    "        # MRR\n",
    "        rr = 0.0\n",
    "        for rank, rel in enumerate(relevances, 1):\n",
    "            if rel == 1:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        metrics[\"MRR\"] = rr\n",
    "\n",
    "        # nDCG@k\n",
    "        dcg = sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
    "        ideal_hits = min(len(normalized_gt), top_k)   # 최대 정답 수\n",
    "        idcg = sum(1.0 / np.log2(idx + 2) for idx in range(ideal_hits))\n",
    "        metrics[\"nDCG@3\"] = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    return output, metrics\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 실행 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    QDRANT_PATH = \"data/vector_store/final-sjchunk/bge-ko-qdrant_db\"\n",
    "    client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "    collection_name = \"audit_chunks\"\n",
    "\n",
    "    questions = [\n",
    "    \"2014년 재무상태표 상 당기 유동자산은 얼마인가?\",\n",
    "    \"2014년 현금흐름표 상 당기 영업활동 현금흐름은 얼마인가?\",\n",
    "    \"2015년 당기 비유동자산은 재무상태표에서 얼마인가?\",\n",
    "    \"2015년 손익계산서 상 당기순이익은 얼마인가?\",\n",
    "    \"2016년 재무상태표 상 당기 단기금융상품은 얼마인가요?\",\n",
    "    \"2016년 포괄손익계산서 상 당기 총포괄이익은 얼마니?\",\n",
    "    \"2016년 자본변동표 상 자기주식의 취득은 얼마인가?\",\n",
    "    \"2017년 당기 매출채권은 재무상태표에 따르면 얼마냐?\",\n",
    "    \"2017년 재무상태표상 전기 현금및현금성자산은 얼마입니까?\",\n",
    "    \"2018년 당기 미수금은 재무상태표에서 얼마인가?\",\n",
    "    \"2018년 손익계산서상 매출총이익은 얼마인가요?\",\n",
    "    \"2019년 재무상태표상 종속기업, 관계기업 및 공동기업 투자는 얼마인가요?\",\n",
    "    \"2019년 현금흐름표 상 이익잉여금 배당은 얼마인가요?\",\n",
    "    \"2019년 손익계산서상 기본주당이익은 얼마인가요?\",\n",
    "    \"2020년 재무상태표 상 자산총계는?\",\n",
    "    \"2020년 손익계산서 상 판매비와관리비는 얼마인가요?\",\n",
    "    \"2021년 재무상태표상 당기 기타포괄손익-공정가치금융자산은 얼마인가요?\",\n",
    "    \"2021년 재무상태표에서 당기 유동비율을 계산하면 얼마인가요?\",\n",
    "    \"2021년 손익계산서 상 당기 금융비용은 얼마인가요?\",\n",
    "    \"2022년 재무상태표상 당기 비유동부채는 얼마인가?\",\n",
    "    \"2022년 손익계산서 상 당기 법인세비용은 얼마니?\",\n",
    "    \"2022년 당기 현금흐름표 상 투자활동 현금흐름은 얼마인가?\",\n",
    "    \"2023년 재무상태표 상 재고자산은 얼마인가?\",\n",
    "    \"2023년 손익계산서 상 당기 영업이익은 얼마인가?\",\n",
    "    \"2024년에는 재무상태표상 당기 무형자산이 얼마야?\",\n",
    "    \"2024년 재무상태표 상 당기 우선주자본금은 얼마인가?\",\n",
    "    \"2024년 손익계산서상 당기 법인세비용은 얼마야?\",\n",
    "    ]\n",
    "    \n",
    "    answers = [\n",
    "        \"62,054,773\",\n",
    "        \"18,653,817\",\n",
    "        \"101,967,575\",\n",
    "        \"12,238,469\",\n",
    "        \"30,170,656\",\n",
    "        \"11,887,806\",\n",
    "        \"(7,707,938)\",\n",
    "        \"27,881,777\",\n",
    "        \"3,778,371\",\n",
    "        \"1,515,079\",\n",
    "        \"68,715,364\",\n",
    "        \"56,571,252\",\n",
    "        \"(9,618,210)\",\n",
    "        \"2,260\",\n",
    "        \"229,664,427\",\n",
    "        \"29,038,798\",\n",
    "        \"1,662,532\",\n",
    "        \"1.38\",\n",
    "        \"3,698,675\",\n",
    "        \"4,581,512\",\n",
    "        \"4,273,142\",\n",
    "        \"(28,123,886)\",\n",
    "        \"29,338,151\",\n",
    "        \"(11,526,297)\",\n",
    "        \"10,496,956\",\n",
    "        \"119,467\",\n",
    "        \"(1,832,987)\",\n",
    "    ]\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    for q, a in zip(questions, answers):\n",
    "        results, metrics = dense_search(\n",
    "            query=q,\n",
    "            model=embed_model,\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            top_k=3,\n",
    "            ground_truth=[a]\n",
    "        )\n",
    "\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"질문: {q}\")\n",
    "\n",
    "        # Top-3 결과 모두 출력\n",
    "        if results:\n",
    "            print(\"\\n[검색 결과 Top-3]\")\n",
    "            for rank, r in enumerate(results, 1):\n",
    "                print(f\"{rank}위: {r['text']}\")\n",
    "                print(f\"   출처: {r['report_year']}년, (score={r['score']:.4f})\\n\")\n",
    "        else:\n",
    "            print(\"검색 결과 없음\")\n",
    "\n",
    "        print(\"\\n[정답]\")\n",
    "        print(a)\n",
    "\n",
    "        print(\"\\n[성능 지표]\")\n",
    "        print(\n",
    "            f\" Precision@3={metrics.get('Precision@3', 0):.2f},\"\n",
    "            f\" Recall@3={metrics.get('Recall@3', 0):.2f},\"\n",
    "            f\" F1@3={metrics.get('F1@3', 0):.2f},\"\n",
    "            f\" nDCG@3={metrics.get('nDCG@3', 0):.2f},\"\n",
    "            f\" MRR={metrics.get('MRR', 0):.2f}\"\n",
    "        )\n",
    "\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    # 평균 성능 지표\n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"=== 평균 성능 지표 ===\")\n",
    "    print(df.mean().round(3))\n",
    "\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f052b-f8dd-4242-9fd3-156c7afa1c41",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8baeed47-7894-48d8-9d80-c8ec9417eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/parsing_conda/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.1.0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/opt/anaconda3/envs/parsing_conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 7886 MiB free\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /Users/bag-yebin/Desktop/흠/자연어처리/samsun-audit-rag-qa/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.58 GiB (4.89 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 임베딩 모델 로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Meta Llama 3.1 8B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device Metal, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4685.33 MiB, ( 7721.05 / 10922.67)\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  4685.31 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
      ".......................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x3235d1dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x3235d2000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x31f9b1480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x3235d2230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x3235d2460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x3258b6a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x325c26050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x325c26280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x177be8450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x31ab56030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x325c264b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x325c266e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x3258b6c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x325c26910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x3258b6e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x325c26b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x325c26d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x31b7ffbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x32514f150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x3258b70b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x325c26fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x325774c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x325774e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x12ec258d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x3235d2890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x3257750c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x3257752f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x3257758a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x325c271d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x325c27db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x3258b76e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x325c28260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x3258b7ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x325c28710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x325775d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x325775f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x325776450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x325c28bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x3258b82f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x325776900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x3258b87b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x325776db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x3258b8c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x3258b9110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x3258b9340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x3258b9800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x3258b9a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x3235d3050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x325776fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x3235d3280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x325777210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x325777440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x3235d3510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x3235d3740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x3235d3970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x325777790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x3235d3ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x3257779c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x3235d3dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x325777bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x3235d4000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x3258b9c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x325777e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x3258b9e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x3258ba0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x3258ba2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x325778050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x3258ba520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x325778280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x3258ba750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x3257784b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x3235d4230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x3257786e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x3258ba980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x3235d4460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x3235d4690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x3235d48c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x325778910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x3258babb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x3258bade0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x3235d4af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x3235d4d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x325778b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x325778d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x3235d4f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x3235d5180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x3235d53b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x325c29490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x3235d5630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x3235d5860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x325c296c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x3235d5a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x3258bb010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x3235d5cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x3235d5f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x3258bb2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x325c298f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x3258bb4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x3258bb700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x3258bb930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x325c29b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x3258bbb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x3258bbd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x3258bbfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x3235d2d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x3258bc1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x3258bc420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x3258bc650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x3235d6180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x3235d63b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x3258bc880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x3258bcab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x325c29d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x325c29f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x3258bcce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x3235d65e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x3258bcf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x3235d6810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x325c2a1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x325c2a3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x3258bd140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x325c2a610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x3258bd370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x325c2a840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x3235d6a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x325c2aa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x3235d6c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x3258bd5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x325c2aca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x3258bd7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x3258bda00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x325c2aed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x325c2b100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x325c2b330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x3258bdc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x3258bde60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x325c2b560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x325c2b790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x3235d6ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x3258be090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x3258be2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x3258be4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x325c2b9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x325c2be50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x325778fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x3258be720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x3257791d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x325779400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x325c2c080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x325c2c2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x325c2c4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x3258be950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x3235d70d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x3258bec60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x325c2c8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x3258bee90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x3235d7300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x3258bf0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x3235d75a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x325c2cb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x3235d77d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x3235d7a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x3235d7c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x3235d7e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x3258bf2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x3258bf520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x325c2cd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x3235d8090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x3258bf750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x325c2cf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x3258bf980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x3258bfbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x3258bfde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x3235d82c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x3235d84f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x325c2d1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x3258c0010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x3235d8720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x3258c0240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x325c2d3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x3235d8950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x3258c0470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x3235d8b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x3235d8db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x325c2d600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x3235d8fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x3235d9210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x3235d9440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x325c2d830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x3258c06a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x325c2da60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x325c2dc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x325c2dec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12eb59e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e913380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x325c2e0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x3258c08d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x3235d9670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x325c2e320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x325c2e550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x3258c0b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x3235d98a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x3258c0d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x3235d9ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x325c2e780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x325c2e9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x325c2ebe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x3235d9d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x325c2ee10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x325c2f040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x3258c0f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x3258c1190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x3258c13c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x3258c15f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x3235d9f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x325c2f270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x3235da160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x325c2f4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x325c2f6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x325c2f900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x3258c1820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x3258c1a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x3235da390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x325c2fb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x3235da5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x3258c1c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x3235da7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x3258c1eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x325c2fd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x3258c20e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x3235daa20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x3235dac50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x325c2ff90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x325c301c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x3235dae80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x3235db0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x3235db2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x3235db510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x3235db740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x3258c2310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x3235db970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x3235dbba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x3258c2540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x325c303f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x3235dbfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x3235dc1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x3235dc410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x3235dc640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x3235dc870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x3235dcaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x3258c2770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x325c30680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x3235dccd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x3235dcf00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x325c308b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x3258c2ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x3235dd130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x3258c30b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x3258c32e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x325c30ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x3258c3510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x325c30d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x3235dd360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x3258c3740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x325c30f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x3235dd590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x325c31170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x325c313a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x325c315d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x325779630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x325779860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x325c31800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x325779a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x325779cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x325c31a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x325c31c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x3258c3970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x3258c3d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x3258c3f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x3258c4180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x325779ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x32577a120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x3258c43b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x3258c45e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x325c31f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x325c32190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x3258c4810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x32577a350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x325c323c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x3258c4a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x32577a580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x32577a7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x32577a9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x3258c4c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x325c325f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x325c32820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x3258c4ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x32577ac10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x32577ae40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x32577b070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x32577b380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x3235dd7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x3258c50d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x3235dd9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x3235ddc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x3258c5300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x3235dde50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x32577b5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x3235de080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x3235de2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x3258c5530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x32577b7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x32577bad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x3258c5760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x3235de4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x3258c5990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x3258c5bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x3235de710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x3235de940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x32577bd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x32577bf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x3258c5df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x3258c6020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x3235deb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x3258c6250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x3258c6480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x3235deda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x3258c66b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x3258c68e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x3235defd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x32577c220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x3258c6b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x3258c6d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x3235df200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x3258c6f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x32577c450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x3258c71a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x3235df430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x32577c680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x3235df660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x3235df890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x325c32a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x3258c73d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x3258c7600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x3235dfac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x3235dfcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x3235dff20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x3235e0150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x3258c7830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x3258c7a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x3258c7c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x3235e0380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x3235e05b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x3258c7ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x325c32e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x3258c81f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x325c33090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x3258c8420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x3258c8650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x3235e0930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x325c332c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x325c334f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x3258c8880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x325c33720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x3235e0b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x3258c8ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x325c33950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x3235e0d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x3258c8ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x3235e0fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x3235e11f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x3235e1420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x3235e1650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x3235e1880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x3235e1ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x325c33b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x3235e1ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x3258c9190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x325c34030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x3235e21f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x3235e26a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x3258c9640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x3235e28d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x3235e2b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x3258c9870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x3258c9aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x3235e2d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x3235e2f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x3235e3190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x3235e33c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x325c34260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x325c34490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x3235e36b0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = Metal\n",
      "llama_kv_cache_unified: layer   1: dev = Metal\n",
      "llama_kv_cache_unified: layer   2: dev = Metal\n",
      "llama_kv_cache_unified: layer   3: dev = Metal\n",
      "llama_kv_cache_unified: layer   4: dev = Metal\n",
      "llama_kv_cache_unified: layer   5: dev = Metal\n",
      "llama_kv_cache_unified: layer   6: dev = Metal\n",
      "llama_kv_cache_unified: layer   7: dev = Metal\n",
      "llama_kv_cache_unified: layer   8: dev = Metal\n",
      "llama_kv_cache_unified: layer   9: dev = Metal\n",
      "llama_kv_cache_unified: layer  10: dev = Metal\n",
      "llama_kv_cache_unified: layer  11: dev = Metal\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2336\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   564.01 MiB\n",
      "llama_context:        CPU compute buffer size =    28.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '15', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM 모델 로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG 평가 진행중:   0%|          | 0/28 [00:00<?, ?질문/s]llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =    6427.05 ms /   329 tokens (   19.54 ms per token,    51.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     363.05 ms /     6 runs   (   60.51 ms per token,    16.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    6793.35 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:   4%|▎         | 1/28 [00:07<03:19,  7.39s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2014년 재무상태표 상 당기 유동자산은 얼마인가?\n",
      "생성 답변: 62,054,773\n",
      "정답: 62,054,773\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     907.68 ms /   137 tokens (    6.63 ms per token,   150.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     395.60 ms /     6 runs   (   65.93 ms per token,    15.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1311.96 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "RAG 평가 진행중:   7%|▋         | 2/28 [00:09<01:52,  4.31s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2014년 현금흐름표 상 당기 영업활동 현금흐름은 얼마인가?\n",
      "생성 답변: 18,653,817\n",
      "정답: 18,653,817\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     728.59 ms /   122 tokens (    5.97 ms per token,   167.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     349.36 ms /     6 runs   (   58.23 ms per token,    17.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1082.42 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  11%|█         | 3/28 [00:11<01:15,  3.01s/질문]Llama.generate: 212 prefix-match hit, remaining 121 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2015년 당기 비유동자산은 재무상태표에서 얼마인가?\n",
      "생성 답변: 101,967,575\n",
      "정답: 101,967,575\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     743.66 ms /   121 tokens (    6.15 ms per token,   162.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     348.99 ms /     6 runs   (   58.16 ms per token,    17.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1097.06 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  14%|█▍        | 4/28 [00:12<00:55,  2.31s/질문]Llama.generate: 212 prefix-match hit, remaining 156 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2015년 손익계산서 상 당기순이익은 얼마인가?\n",
      "생성 답변: 12,238,469\n",
      "정답: 12,238,469\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     969.85 ms /   156 tokens (    6.22 ms per token,   160.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     360.51 ms /     6 runs   (   60.09 ms per token,    16.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1333.70 ms /   162 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  18%|█▊        | 5/28 [00:13<00:45,  1.98s/질문]Llama.generate: 212 prefix-match hit, remaining 145 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2016년 재무상태표 상 당기 단기금융상품은 얼마인가요?\n",
      "생성 답변: 30,170,656\n",
      "정답: 30,170,656\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     989.33 ms /   145 tokens (    6.82 ms per token,   146.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     367.69 ms /     6 runs   (   61.28 ms per token,    16.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.65 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  21%|██▏       | 6/28 [00:15<00:39,  1.81s/질문]Llama.generate: 212 prefix-match hit, remaining 135 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2016년 포괄손익계산서 상 당기 총포괄이익은 얼마니?\n",
      "생성 답변: 11,887,806\n",
      "정답: 11,887,806\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     999.47 ms /   135 tokens (    7.40 ms per token,   135.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     350.34 ms /     6 runs   (   58.39 ms per token,    17.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.81 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "RAG 평가 진행중:  25%|██▌       | 7/28 [00:16<00:36,  1.72s/질문]Llama.generate: 212 prefix-match hit, remaining 136 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2016년 자본변동표 상 자기주식의 취득은 얼마인가?\n",
      "생성 답변: -7,707,938\n",
      "정답: (7,707,938)\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =    1014.97 ms /   136 tokens (    7.46 ms per token,   133.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     356.17 ms /     6 runs   (   59.36 ms per token,    16.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.71 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "RAG 평가 진행중:  29%|██▊       | 8/28 [00:18<00:32,  1.64s/질문]Llama.generate: 226 prefix-match hit, remaining 158 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2017년 당기 매출채권은 재무상태표에 따르면 얼마냐?\n",
      "생성 답변: 27,881,777\n",
      "정답: 27,881,777\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =    1046.52 ms /   158 tokens (    6.62 ms per token,   150.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     357.33 ms /     6 runs   (   59.56 ms per token,    16.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.21 ms /   164 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  32%|███▏      | 9/28 [00:19<00:30,  1.61s/질문]Llama.generate: 220 prefix-match hit, remaining 117 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2017년 재무상태표상 전기 현금및현금성자산은 얼마입니까?\n",
      "생성 답변: 3,778,371\n",
      "정답: 3,778,371\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 0.5, 'nDCG@3': 0.6309297535714575}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     830.60 ms /   117 tokens (    7.10 ms per token,   140.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =     176.87 ms /     3 runs   (   58.96 ms per token,    16.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1010.59 ms /   120 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "RAG 평가 진행중:  36%|███▌      | 10/28 [00:20<00:26,  1.47s/질문]Llama.generate: 220 prefix-match hit, remaining 108 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2017년 재무상태표상 당기 매각예정분류자산은 얼마인가요?\n",
      "생성 답변: 문서에 없음\n",
      "정답: -\n",
      "Retriever 성능지표: {'Precision@3': 1.0, 'Recall@3': 1.0, 'F1@3': 1.0, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     850.80 ms /   108 tokens (    7.88 ms per token,   126.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     349.81 ms /     6 runs   (   58.30 ms per token,    17.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1204.63 ms /   114 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  39%|███▉      | 11/28 [00:22<00:23,  1.41s/질문]Llama.generate: 212 prefix-match hit, remaining 116 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2018년 당기 미수금은 재무상태표에서 얼마인가?\n",
      "생성 답변: 1,515,079\n",
      "정답: 1,515,079\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     850.51 ms /   116 tokens (    7.33 ms per token,   136.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =     371.13 ms /     6 runs   (   61.85 ms per token,    16.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.23 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  43%|████▎     | 12/28 [00:23<00:22,  1.39s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2018년 손익계산서상 매출총이익은 얼마인가요?\n",
      "생성 답변: 68,715,364\n",
      "정답: 68,715,364\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =    1052.18 ms /   148 tokens (    7.11 ms per token,   140.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     372.86 ms /     6 runs   (   62.14 ms per token,    16.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1431.28 ms /   154 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  46%|████▋     | 13/28 [00:25<00:22,  1.49s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2019년 재무상태표상 종속기업, 관계기업 및 공동기업 투자는 얼마인가요?\n",
      "생성 답변: 56,571,252\n",
      "정답: 56,571,252\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     853.18 ms /   128 tokens (    6.67 ms per token,   150.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.76 ms /    20 runs   (   59.69 ms per token,    16.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    2055.35 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "RAG 평가 진행중:  50%|█████     | 14/28 [00:27<00:24,  1.73s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2019년 현금흐름표 상 이익잉여금 배당은 얼마인가요?\n",
      "생성 답변: 4,625,181 - 9,618,210 = -5,000,029\n",
      "정답: (9,618,210)\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     862.04 ms /   121 tokens (    7.12 ms per token,   140.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     241.30 ms /     4 runs   (   60.32 ms per token,    16.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.65 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "RAG 평가 진행중:  54%|█████▎    | 15/28 [00:28<00:21,  1.62s/질문]Llama.generate: 212 prefix-match hit, remaining 116 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2019년 손익계산서상 기본주당이익은 얼마인가요?\n",
      "생성 답변: 2,260\n",
      "정답: 2,260\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     859.75 ms /   116 tokens (    7.41 ms per token,   134.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     369.90 ms /     6 runs   (   61.65 ms per token,    16.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.70 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  57%|█████▋    | 16/28 [00:30<00:18,  1.56s/질문]Llama.generate: 212 prefix-match hit, remaining 120 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2020년 재무상태표 상 자산총계는?\n",
      "생성 답변: 229,664,427\n",
      "정답: 229,664,427\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     872.48 ms /   120 tokens (    7.27 ms per token,   137.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =     365.38 ms /     6 runs   (   60.90 ms per token,    16.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.88 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  61%|██████    | 17/28 [00:31<00:16,  1.52s/질문]Llama.generate: 212 prefix-match hit, remaining 187 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2020년 손익계산서 상 판매비와관리비는 얼마인가요?\n",
      "생성 답변: 29,038,798\n",
      "정답: 29,038,798\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =    1315.46 ms /   187 tokens (    7.03 ms per token,   142.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     357.26 ms /     6 runs   (   59.54 ms per token,    16.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1676.42 ms /   193 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  64%|██████▍   | 18/28 [00:33<00:16,  1.62s/질문]Llama.generate: 226 prefix-match hit, remaining 122 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2021년 재무상태표상 당기 기타포괄손익-공정가치금융자산은 얼마인가요?\n",
      "생성 답변: 1,662,532\n",
      "정답: 1,662,532\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     870.86 ms /   122 tokens (    7.14 ms per token,   140.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1511.17 ms /    24 runs   (   62.97 ms per token,    15.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    2400.28 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         22\n",
      "RAG 평가 진행중:  68%|██████▊   | 19/28 [00:36<00:17,  1.89s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2021년 재무상태표에서 당기 유동비율을 계산하면 얼마인가요?\n",
      "생성 답변: 53,067,303 / 73,553,416 = 0.72\n",
      "    0.72\n",
      "정답: 1.38\n",
      "Retriever 성능지표: {'Precision@3': 0.0, 'Recall@3': 0.0, 'F1@3': 0.0, 'MRR': 0.0, 'nDCG@3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     858.61 ms /   116 tokens (    7.40 ms per token,   135.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =     378.95 ms /     6 runs   (   63.16 ms per token,    15.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.79 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  71%|███████▏  | 20/28 [00:37<00:14,  1.83s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2021년 손익계산서 상 당기 금융비용은 얼마인가요?\n",
      "생성 답변: 3,698,675\n",
      "정답: 3,698,675\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     878.61 ms /   120 tokens (    7.32 ms per token,   136.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     361.48 ms /     6 runs   (   60.25 ms per token,    16.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1249.49 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  75%|███████▌  | 21/28 [00:39<00:12,  1.73s/질문]Llama.generate: 212 prefix-match hit, remaining 123 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2022년 재무상태표상 당기 비유동부채는 얼마인가?\n",
      "생성 답변: 4,581,512\n",
      "정답: 4,581,512\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     879.69 ms /   123 tokens (    7.15 ms per token,   139.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     369.51 ms /     6 runs   (   61.59 ms per token,    16.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.87 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  79%|███████▊  | 22/28 [00:40<00:09,  1.64s/질문]Llama.generate: 212 prefix-match hit, remaining 137 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2022년 손익계산서 상 당기 법인세비용은 얼마니?\n",
      "생성 답변: 4,273,142\n",
      "정답: 4,273,142\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =    1082.20 ms /   137 tokens (    7.90 ms per token,   126.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     365.09 ms /     6 runs   (   60.85 ms per token,    16.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1455.10 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "RAG 평가 진행중:  82%|████████▏ | 23/28 [00:42<00:08,  1.64s/질문]Llama.generate: 212 prefix-match hit, remaining 122 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2022년 당기 현금흐름표 상 투자활동 현금흐름은 얼마인가?\n",
      "생성 답변: -28,123,886\n",
      "정답: (28,123,886)\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     875.95 ms /   122 tokens (    7.18 ms per token,   139.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     361.90 ms /     6 runs   (   60.32 ms per token,    16.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.65 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  86%|████████▌ | 24/28 [00:43<00:06,  1.58s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2023년 재무상태표 상 재고자산은 얼마인가?\n",
      "생성 답변: 29,338,151\n",
      "정답: 29,338,151\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     866.16 ms /   112 tokens (    7.73 ms per token,   129.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2465.59 ms /    39 runs   (   63.22 ms per token,    15.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    3440.56 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         37\n",
      "RAG 평가 진행중:  89%|████████▉ | 25/28 [00:47<00:06,  2.22s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2023년 당기 영업이익은 얼마인가?\n",
      "생성 답변: 25,397,099 - 144,023,552 = -118,626,453\n",
      "    2023년 당기 영업이익은 -118,626,453\n",
      "정답: (11,526,297)\n",
      "Retriever 성능지표: {'Precision@3': 0.0, 'Recall@3': 0.0, 'F1@3': 0.0, 'MRR': 0.0, 'nDCG@3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 212 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     899.39 ms /   124 tokens (    7.25 ms per token,   137.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     377.12 ms /     6 runs   (   62.85 ms per token,    15.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.05 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  93%|█████████▎| 26/28 [00:49<00:04,  2.12s/질문]Llama.generate: 226 prefix-match hit, remaining 105 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2024년에는 재무상태표상 당기 무형자산이 얼마야?\n",
      "생성 답변: 10,496,956\n",
      "정답: 10,496,956\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     893.89 ms /   105 tokens (    8.51 ms per token,   117.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     254.61 ms /     4 runs   (   63.65 ms per token,    15.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.71 ms /   109 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "RAG 평가 진행중:  96%|█████████▋| 27/28 [00:50<00:01,  1.89s/질문]Llama.generate: 212 prefix-match hit, remaining 127 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2024년 재무상태표 상 당기 우선주자본금은 얼마인가?\n",
      "생성 답변: 119,467\n",
      "정답: 119,467\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    6427.52 ms\n",
      "llama_perf_context_print: prompt eval time =     895.68 ms /   127 tokens (    7.05 ms per token,   141.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     381.14 ms /     6 runs   (   63.52 ms per token,    15.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.10 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중: 100%|██████████| 28/28 [00:52<00:00,  1.86s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2024년 손익계산서상 당기 법인세비용은 얼마야?\n",
      "생성 답변: -1,832,987\n",
      "정답: (1,832,987)\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n",
      "\n",
      "=== 평균 성능 지표 ===\n",
      "Precision@3    0.333333\n",
      "Recall@3       0.928571\n",
      "F1@3           0.482143\n",
      "MRR            0.910714\n",
      "nDCG@3         0.915390\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# RAGAS\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas import evaluate\n",
    "\n",
    "# ===== 임베딩 모델 로드 =====\n",
    "MODEL_NAME = \"bge-m3-ko\"\n",
    "EMBED_MODEL_NAME = \"dragonkue/bge-m3-ko\"\n",
    "COLLECTION = \"audit_chunks\"\n",
    "\n",
    "# ===== Qdrant 클라이언트 =====\n",
    "QDRANT_PATH = \"data/vector_store/final-sjchunk/bge-ko-qdrant_db\"\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "# 토크나이저 포크 경고 끄기 (권장)\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# ===== 디바이스 선택 (MPS > CUDA > CPU) =====\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device=device)\n",
    "print(\"✅ 임베딩 모델 로드 완료!\")\n",
    "\n",
    "# ===== dense_search 함수 import =====\n",
    "# (예빈씨가 이전에 정의한 dense_search 그대로 사용한다고 가정)\n",
    "def extract_year_from_query(query: str):\n",
    "    \"\"\"질문에서 연도(4자리 숫자) 추출\"\"\"\n",
    "    match = re.search(r\"(20\\d{2}|19\\d{2})\", query)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_numbers(text: str) -> str:\n",
    "    \"\"\"텍스트에서 숫자만 추출\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return \"\".join(re.findall(r\"\\d+\", text.replace(\",\", \"\")))\n",
    "    \n",
    "def dense_search(query: str, model, client, collection_name: str,\n",
    "                 top_k: int = 3, ground_truth=None, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Qdrant query_points 기반 Dense Retriever 함수 + 성능지표 계산 (+ 디버그 로그 옵션)\n",
    "    \"\"\"\n",
    "    year = extract_year_from_query(query)\n",
    "    qv = model.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "    if isinstance(ground_truth, str):\n",
    "        ground_truth = [ground_truth]\n",
    "    \n",
    "    query_filter = None\n",
    "    if year:\n",
    "        query_filter = qmodels.Filter(\n",
    "            must=[qmodels.FieldCondition(\n",
    "                key=\"report_year\",\n",
    "                match=qmodels.MatchValue(value=year)\n",
    "            )]\n",
    "        )\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=query_filter\n",
    "    )\n",
    "\n",
    "    output = []\n",
    "    for r in results.points:\n",
    "        payload = r.payload or {}\n",
    "        result_item = {\"score\": r.score, \"text\": payload.get(\"text\")}\n",
    "        if \"metadata\" in payload:\n",
    "            result_item.update(payload[\"metadata\"])\n",
    "        else:\n",
    "            result_item.update(payload)\n",
    "        output.append(result_item)\n",
    "\n",
    "    metrics = {}\n",
    "    if ground_truth:\n",
    "        normalized_gt = [extract_numbers(gt) for gt in ground_truth]\n",
    "        used = set()\n",
    "        relevances = []\n",
    "\n",
    "        for r in output:\n",
    "            nums = extract_numbers(r[\"text\"])\n",
    "            hit = 0\n",
    "            for gt in normalized_gt:\n",
    "                if gt and gt in nums and gt not in used:\n",
    "                    hit = 1\n",
    "                    used.add(gt)\n",
    "                    break\n",
    "            relevances.append(hit)\n",
    "\n",
    "        precision = sum(relevances) / top_k if top_k > 0 else 0.0\n",
    "        metrics[\"Precision@3\"] = precision\n",
    "        recall = min(sum(relevances), len(normalized_gt)) / len(normalized_gt) if normalized_gt else 0.0\n",
    "        metrics[\"Recall@3\"] = recall\n",
    "        metrics[\"F1@3\"] = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        rr = 0.0\n",
    "        for rank, rel in enumerate(relevances, 1):\n",
    "            if rel == 1:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        metrics[\"MRR\"] = rr\n",
    "        dcg = sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
    "        ideal_hits = min(len(normalized_gt), top_k)\n",
    "        idcg = sum(1.0 / np.log2(idx + 2) for idx in range(ideal_hits))\n",
    "        metrics[\"nDCG@3\"] = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    return output, metrics\n",
    "    \n",
    "# ===== LLM 모델 로드 =====\n",
    "model_path = Path(\"/Users/bag-yebin/Desktop/흠/자연어처리/samsun-audit-rag-qa/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\").resolve()\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(model_path),\n",
    "    n_ctx=8192,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=35\n",
    ")\n",
    "print(\"✅ LLM 모델 로드 완료!\")\n",
    "\n",
    "# ===== RAG Pipeline =====\n",
    "def rag_pipeline(query: str, model, client, collection_name: str = COLLECTION, top_k: int = 3, ground_truth=None):\n",
    "    # 1) Retriever 단계\n",
    "    results, metrics = dense_search(\n",
    "        query=query,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k,\n",
    "        ground_truth=ground_truth  \n",
    "\n",
    "    )\n",
    "\n",
    "    # 2) 검색 결과 합치기\n",
    "    context_texts = [r[\"text\"] for r in results if r.get(\"text\")]\n",
    "    context_text = \"\\n\".join(context_texts)\n",
    "\n",
    "    # 3) Reader 호출 (Zephyr LLM) - 튜닝된 프롬프트 적용\n",
    "    prompt = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    너는 재무보고서 전문가이자 **숫자 추출기**다. \n",
    "    검색된 문서와 metadata만 근거로 반드시 답해야 한다. \n",
    "    규칙:\n",
    "    1. 정답은 반드시 숫자(예: 62,054,773) 또는 \"문서에 없음\" 만 출력한다. \n",
    "    2. 단위(백만원, 억원 등)는 출력하지 않는다. \n",
    "    3. 계산이 필요한 경우 (예: 비율) 정확히 계산하고 소수점 둘째 자리까지 출력한다.\n",
    "    4. 검색된 문서에 정답이 없으면 반드시 \"문서에 없음\"이라고 출력한다.\n",
    "    5. 불필요한 설명, 해석, 텍스트는 절대 쓰지 말고, **최종 숫자만 출력**한다.\n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    검색된 문서:\n",
    "    {context_text}\n",
    "    \n",
    "    질문: {query}\n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=512, stop=[\"<|eot_id|>\"])\n",
    "    answer = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # ✅ \"없음\" 케이스 예외 처리\n",
    "    if ground_truth and ground_truth[0] == \"-\":\n",
    "        if \"없음\" in answer:\n",
    "            metrics = {\"Precision@3\": 1.0, \"Recall@3\": 1.0, \"F1@3\": 1.0, \"MRR\": 1.0, \"nDCG@3\": 1.0}\n",
    "        else:\n",
    "            metrics = {\"Precision@3\": 0.0, \"Recall@3\": 0.0, \"F1@3\": 0.0, \"MRR\": 0.0, \"nDCG@3\": 0.0}\n",
    "\n",
    "\n",
    "    return answer, metrics, context_texts   # ✅ contexts도 반환\n",
    "# ===== 실행 + RAGAS 평가 =====\n",
    "if __name__ == \"__main__\":\n",
    "    # 질문 & 정답 데이터셋\n",
    "    questions = [\n",
    "    \"2014년 재무상태표 상 당기 유동자산은 얼마인가?\",\n",
    "    \"2014년 현금흐름표 상 당기 영업활동 현금흐름은 얼마인가?\",\n",
    "    \"2015년 당기 비유동자산은 재무상태표에서 얼마인가?\",\n",
    "    \"2015년 손익계산서 상 당기순이익은 얼마인가?\",\n",
    "    \"2016년 재무상태표 상 당기 단기금융상품은 얼마인가요?\",\n",
    "    \"2016년 포괄손익계산서 상 당기 총포괄이익은 얼마니?\",\n",
    "    \"2016년 자본변동표 상 자기주식의 취득은 얼마인가?\",\n",
    "    \"2017년 당기 매출채권은 재무상태표에 따르면 얼마냐?\",\n",
    "    \"2017년 재무상태표상 전기 현금및현금성자산은 얼마입니까?\",\n",
    "    \"2017년 재무상태표상 당기 매각예정분류자산은 얼마인가요?\",\n",
    "    \"2018년 당기 미수금은 재무상태표에서 얼마인가?\",\n",
    "    \"2018년 손익계산서상 매출총이익은 얼마인가요?\",\n",
    "    \"2019년 재무상태표상 종속기업, 관계기업 및 공동기업 투자는 얼마인가요?\",\n",
    "    \"2019년 현금흐름표 상 이익잉여금 배당은 얼마인가요?\",\n",
    "    \"2019년 손익계산서상 기본주당이익은 얼마인가요?\",\n",
    "    \"2020년 재무상태표 상 자산총계는?\",\n",
    "    \"2020년 손익계산서 상 판매비와관리비는 얼마인가요?\",\n",
    "    \"2021년 재무상태표상 당기 기타포괄손익-공정가치금융자산은 얼마인가요?\",\n",
    "    \"2021년 재무상태표에서 당기 유동비율을 계산하면 얼마인가요?\",\n",
    "    \"2021년 손익계산서 상 당기 금융비용은 얼마인가요?\",\n",
    "    \"2022년 재무상태표상 당기 비유동부채는 얼마인가?\",\n",
    "    \"2022년 손익계산서 상 당기 법인세비용은 얼마니?\",\n",
    "    \"2022년 당기 현금흐름표 상 투자활동 현금흐름은 얼마인가?\",\n",
    "    \"2023년 재무상태표 상 재고자산은 얼마인가?\",\n",
    "    \"2023년 당기 영업이익은 얼마인가?\",\n",
    "    \"2024년에는 재무상태표상 당기 무형자산이 얼마야?\",\n",
    "    \"2024년 재무상태표 상 당기 우선주자본금은 얼마인가?\",\n",
    "    \"2024년 손익계산서상 당기 법인세비용은 얼마야?\",\n",
    "    ]\n",
    "    \n",
    "    answers = [\n",
    "        \"62,054,773\",\n",
    "        \"18,653,817\",\n",
    "        \"101,967,575\",\n",
    "        \"12,238,469\",\n",
    "        \"30,170,656\",\n",
    "        \"11,887,806\",\n",
    "        \"(7,707,938)\",\n",
    "        \"27,881,777\",\n",
    "        \"3,778,371\",\n",
    "        \"-\",\n",
    "        \"1,515,079\",\n",
    "        \"68,715,364\",\n",
    "        \"56,571,252\",\n",
    "        \"(9,618,210)\",\n",
    "        \"2,260\",\n",
    "        \"229,664,427\",\n",
    "        \"29,038,798\",\n",
    "        \"1,662,532\",\n",
    "        \"1.38\",\n",
    "        \"3,698,675\",\n",
    "        \"4,581,512\",\n",
    "        \"4,273,142\",\n",
    "        \"(28,123,886)\",\n",
    "        \"29,338,151\",\n",
    "        \"(11,526,297)\",\n",
    "        \"10,496,956\",\n",
    "        \"119,467\",\n",
    "        \"(1,832,987)\",\n",
    "    ]\n",
    "    \n",
    "    qa_pairs = [{\"question\": q, \"ground_truth\": [a]} for q, a in zip(questions, answers)]\n",
    "\n",
    "    eval_data = {\"question\": [], \"answer\": [], \"contexts\": [], \"reference\": []}\n",
    "    metrics_list = []   # ✅ 성능지표 저장 리스트 추가\n",
    "\n",
    "    for qa in tqdm(qa_pairs, desc=\"RAG 평가 진행중\", unit=\"질문\"):\n",
    "        q = qa[\"question\"]\n",
    "        gt = qa[\"ground_truth\"][0]\n",
    "\n",
    "        answer, metrics, contexts = rag_pipeline(q, embed_model, client, COLLECTION, ground_truth=gt)\n",
    "        eval_data[\"question\"].append(q)\n",
    "        eval_data[\"answer\"].append(answer)\n",
    "        eval_data[\"contexts\"].append(contexts)\n",
    "        eval_data[\"reference\"].append(gt)   # RAGAS가 요구하는 필드명\n",
    "\n",
    "        metrics_list.append(metrics)  # ✅ 각 성능지표 저장\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"질문:\", q)\n",
    "        print(\"생성 답변:\", answer)\n",
    "        print(\"정답:\", gt)\n",
    "        print(\"Retriever 성능지표:\", metrics)\n",
    "\n",
    "    dataset = Dataset.from_dict(eval_data)\n",
    "\n",
    "    # ✅ 성능지표 평균 계산\n",
    "    df_metrics = pd.DataFrame(metrics_list)\n",
    "    print(\"\\n=== 평균 성능 지표 ===\")\n",
    "    print(df_metrics.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5701c-a536-4724-a632-a0fb9453d053",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/parsing_conda/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.1.0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/opt/anaconda3/envs/parsing_conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 임베딩 모델 로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/parsing_conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 7886 MiB free\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /Users/bag-yebin/Desktop/흠/자연어처리/samsun-audit-rag-qa/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.58 GiB (4.89 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 리랭커 로드 완료! (BAAI/bge-reranker-base, device=mps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Meta Llama 3.1 8B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device Metal, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4685.33 MiB, ( 7721.03 / 10922.67)\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  4685.31 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
      ".......................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x294ac0930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x2e7effd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x2865a1f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x31bb607b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x31bba98c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x31bb4d100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x294ac1140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x2e6dcbd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x3221bbc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x31bc4b690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x31ddae410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x31c8a26d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x294ac0b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x294ac1370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x31c143cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x294ac15a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x31c6efcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x2e74c0410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x294ac17d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x2e85da9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x294ac1a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x31da087a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x2f08a8180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x2f08a86a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x2f08a88d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x31c2d58e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x31c2a6b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x294ac2070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x2f08a8e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x31d7c2240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x2e73cbd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x2e7314fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x2f08a9350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x2f08a9800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x294ac2530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x2f08a9cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x294ac29f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x294ac2eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x294ac3370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x2f08a9ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x2e6978750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x2f08aa3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x294ac3aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x294ac3f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x2e6faa000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x31d912800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x294ac4180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x2e6b4e000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x2f08aa860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x31ec8c060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x2f08aaa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x31ec8c730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x294ac43b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x294ac45e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x31ec8c9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x2f08a83b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x294ac4810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x2f08aad80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x294ac4a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x31ec8cbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x2f08aafb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x294ac4c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x31ec8ce20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x294ac4ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x2f08ab1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x2f08ab410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x2e7df31f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x294ac50d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x294ac5300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x2e6862a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x2e94bf200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x2e75cde30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x2e75d8890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x2e77f4510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x2e772d250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x2e7706270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x294ac5530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x294ac58d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x2f08ab7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x294ac5b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x2f08ab9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x2e7220320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x2e78feac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x294ac5d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x294ac5f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x294ac6190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x2e786b4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x2e7628140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x2f08abc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x294ac63c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x31ec8d050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x294ac65f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x31ec8d280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x294ac6820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x294ac6ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x31ec8d510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x2e7b7dda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x294ac6ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x31ec8d740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x2f08abe40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x2e6e4ef20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x2f08ac070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x294ac6f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x31ec8d970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x2f08ac2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x2f08ac4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x294ac7140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x2f08ac700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x294ac7370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x2e7c23280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x2e6cde2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x31ec8dba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x294ac75a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x294ac77d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x31ec8ddd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x294ac7a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x31ec8e000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x31ec8e230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x294ac7c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x2e76db3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x2e6d3e790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x31ec8e460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x2e7267470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x2e97ba760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x31ec8e690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x294ac7e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x294ac8090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x31ec8e8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x31ec8eaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x2f08ac930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x2f08acb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x2f08acd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x31ec8ed20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x31ec8ef50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x2f08acfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x2e9230170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x2f08ad1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x31ec8f180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x2f08ad420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x294ac82c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x31ec8f3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x31ec8f5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x2f08ad650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x2f08ad880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x31ec8f810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x31ec8fa40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x294ac84f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x2f08adab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x31ec8fc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x2f08adce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x31ec8fea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x294ac8720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x2f08ae040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x2f08ae270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x294ac8a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x2f08ae4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x31ec900d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x31ec90440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x31ec90670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x2f08ae6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x31ec908a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x2e7af44c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x2e7ae0a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x31ec90ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x2f08ae900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x294ac8c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1053b2ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x294ac8e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x31ec90d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x31ec91190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x2f08aeb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x2f08aed60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x2f08aef90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x2f08af1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x31ec913c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x2f08af3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x31ec915f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x2f08af620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x31ec91820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x294ac90b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x31ec91a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x294ac92e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11c637e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x294ac9510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x2e90df1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x2e90ad280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x2e961d000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x2eb26b120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x2e772fa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x2e79a2f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x294ac9740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x2e6ed9280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x294ac9970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x2e6e771a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x2f08af850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x2e7b62380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x2f08afa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x2e7b8d020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x2f08afcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x2f08afee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x31ec91c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x31ec91eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x2e8b36130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x31ec920e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x2e9dbfb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x294ac9ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x294ac9dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x31ec92310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x294aca000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x31ec92540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x294aca230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x2e7c4bd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x2e7cf8a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x294aca460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x294aca690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x2e75233b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x2e7534a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x294aca8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x294acaaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x294acad20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x294acaf50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x294acb180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x294acb3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x2e8965830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x294acb5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x2e8407490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x2e84e10b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x294acb810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x2e84aa9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x294acba40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x31ec92770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x31ec929a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x2e9edd980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x2e70d7d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x294acbc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x2f08b0110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x2f08b0340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x2e7073110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x294acbea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x2f08b0570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x2e69df030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x2f08b07a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x2f08b09d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x2f08b0c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x294acc130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x2e71a6b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x2e7138070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x294acc360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x2e71d5db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x2e71e0300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x294acc590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x294acc820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x294acca50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x31ec92bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x2f08b0fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x294accc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x294acceb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x294acd4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x31ec92e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x2e7104f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x2e71163f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x294acd6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x2f08b11e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x2e8053240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x31ec93030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x2e8012590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x294acd900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x2f08b1410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x2e80e0ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x2e8097500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x294acdb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x2f08b1640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x294acdd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x2e7458890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x2e74aa650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x2f08b1870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x31ec93260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x31ec93490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x31ec936c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x294acdf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x2f08b1aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x2f08b1eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x294ace1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x294ace3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x294ace620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x31ec938f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x31ec93b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x31ec93d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x294ace850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x31ec93f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x294acea80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x2f08b20e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c63cda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x31ec941b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x31ec943e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x2f08b2310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x2f08b2540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x31ec94610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x2f08b2770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x31ec94840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x31ec94a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x2f08b29a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x2f08b2bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x2e6880d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x2f08b2e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x2f08b3030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x31ec94ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x294acecb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x294aceee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x31ec94ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x2e68af330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x294acf110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x2f08b3260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x294acf340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x31ec95100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x106f40200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x294acf570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x294acf7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x31ec95330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x294acf9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x31ec95560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x31ec95790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x31ec959c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x294acfc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x31ec95bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x31ec95eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x31ec960e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x294acfe30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x31ec96310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x31ec96600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x2f08b3490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x31ec96830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x31ec96a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x31ec96c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x31ec96ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x31ec970f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x31ec97320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x31ec97550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x2f08b37f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x2f08b3a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x31ec97780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x31ec979b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x31ec97be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x2f08b3c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x294ad0060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x294ad0290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x31ec97e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x2f08b3e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x294ad04c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x2f08b40b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x2f08b42e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x31ec98040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x2f08b4510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x31ec98270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x2f08b4740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x2f08b4980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x31ec984a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x31ec986f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x2e7e212f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x2e7e37600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x31ec98920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x31ec98b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x2f08b4bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x2f08b4de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x2f08b5010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x31ec98d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x31ec98fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x2f08b5240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x31ec991e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x2e7e48ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x3221ad4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x31ec99410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x3221b31c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x2f08b5470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x31ec99640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x2f08b56a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x2f08b58d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x2f08b5d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x2f08b6230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x2f08b66e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x2f08b6b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x2e6c10130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x2e6c58030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x2f08b6dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x31ec99870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x2f08b6ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x2f08b7220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x2e6c2b480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x31ec99aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x31ec99cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x2f08b7450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x2f08b7680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x2e851dff0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = Metal\n",
      "llama_kv_cache_unified: layer   1: dev = Metal\n",
      "llama_kv_cache_unified: layer   2: dev = Metal\n",
      "llama_kv_cache_unified: layer   3: dev = Metal\n",
      "llama_kv_cache_unified: layer   4: dev = Metal\n",
      "llama_kv_cache_unified: layer   5: dev = Metal\n",
      "llama_kv_cache_unified: layer   6: dev = Metal\n",
      "llama_kv_cache_unified: layer   7: dev = Metal\n",
      "llama_kv_cache_unified: layer   8: dev = Metal\n",
      "llama_kv_cache_unified: layer   9: dev = Metal\n",
      "llama_kv_cache_unified: layer  10: dev = Metal\n",
      "llama_kv_cache_unified: layer  11: dev = Metal\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2336\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   564.01 MiB\n",
      "llama_context:        CPU compute buffer size =    28.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '15', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM 모델 로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG 평가 진행중:   0%|          | 0/28 [00:00<?, ?질문/s]llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    7474.55 ms /   201 tokens (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2778.34 ms /    46 runs   (   60.40 ms per token,    16.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   10270.17 ms /   247 tokens\n",
      "llama_perf_context_print:    graphs reused =         44\n",
      "RAG 평가 진행중:   4%|▎         | 1/28 [00:12<05:37, 12.48s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2014년 재무상태표 상 당기 유동자산은 얼마인가?\n",
      "생성 답변: 유동자산은 재무상태표에 명시되어 있습니다. 따라서, 2014년 재무상태표 상 당기 유동자산은 62,054,773백만원입니다.\n",
      "정답: 62,054,773\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     913.35 ms /   140 tokens (    6.52 ms per token,   153.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1786.25 ms /    31 runs   (   57.62 ms per token,    17.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    2711.38 ms /   171 tokens\n",
      "llama_perf_context_print:    graphs reused =         29\n",
      "RAG 평가 진행중:   7%|▋         | 2/28 [00:16<03:12,  7.39s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2014년 현금흐름표 상 당기 영업활동 현금흐름은 얼마인가?\n",
      "생성 답변: 2014년 현금흐름표 상 당기 영업활동 현금흐름은 18,653,817백만원입니다.\n",
      "정답: 18,653,817\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     745.20 ms /   120 tokens (    6.21 ms per token,   161.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     228.33 ms /     4 runs   (   57.08 ms per token,    17.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     977.69 ms /   124 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "RAG 평가 진행중:  11%|█         | 3/28 [00:18<02:02,  4.90s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2015년 당기 비유동자산은 재무상태표에서 얼마인가?\n",
      "생성 답변: 문서에 없음.\n",
      "정답: 101,967,575\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1030.24 ms /   130 tokens (    7.92 ms per token,   126.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     174.05 ms /     3 runs   (   58.02 ms per token,    17.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.73 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "RAG 평가 진행중:  14%|█▍        | 4/28 [00:20<01:29,  3.72s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2015년 손익계산서 상 당기순이익은 얼마인가?\n",
      "생성 답변: 문서에 없음\n",
      "정답: 12,238,469\n",
      "Retriever 성능지표: {'Precision@3': 0.0, 'Recall@3': 0.0, 'F1@3': 0.0, 'MRR': 0.0, 'nDCG@3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 143 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     984.65 ms /   143 tokens (    6.89 ms per token,   145.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1637.14 ms /    28 runs   (   58.47 ms per token,    17.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    2757.93 ms /   171 tokens\n",
      "llama_perf_context_print:    graphs reused =         27\n",
      "RAG 평가 진행중:  18%|█▊        | 5/28 [00:23<01:23,  3.65s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2016년 재무상태표 상 당기 단기금융상품은 얼마인가요?\n",
      "생성 답변: 2016년 재무상태표 상 당기 단기금융상품은 30,170,656백만원입니다.\n",
      "정답: 30,170,656\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1035.66 ms /   145 tokens (    7.14 ms per token,   140.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1799.23 ms /    32 runs   (   56.23 ms per token,    17.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    2848.24 ms /   177 tokens\n",
      "llama_perf_context_print:    graphs reused =         30\n",
      "RAG 평가 진행중:  21%|██▏       | 6/28 [00:27<01:21,  3.70s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2016년 포괄손익계산서 상 당기 총포괄이익은 얼마니?\n",
      "생성 답변: 2016년 포괄손익계산서 상 당기 총포괄이익은 11,887,806백만원입니다.\n",
      "정답: 11,887,806\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     838.54 ms /   128 tokens (    6.55 ms per token,   152.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     352.14 ms /     6 runs   (   58.69 ms per token,    17.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1201.61 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  25%|██▌       | 7/28 [00:29<01:05,  3.13s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2016년 자본변동표 상 자기주식의 취득은 얼마인가?\n",
      "생성 답변: \"문서에 없음\"\n",
      "정답: (7,707,938)\n",
      "Retriever 성능지표: {'Precision@3': 0.0, 'Recall@3': 0.0, 'F1@3': 0.0, 'MRR': 0.0, 'nDCG@3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     841.98 ms /   128 tokens (    6.58 ms per token,   152.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     188.43 ms /     3 runs   (   62.81 ms per token,    15.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1034.44 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "RAG 평가 진행중:  29%|██▊       | 8/28 [00:31<00:54,  2.73s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2017년 당기 매출채권은 재무상태표에 따르면 얼마냐?\n",
      "생성 답변: 문서에 없음\n",
      "정답: 27,881,777\n",
      "Retriever 성능지표: {'Precision@3': 0.0, 'Recall@3': 0.0, 'F1@3': 0.0, 'MRR': 0.0, 'nDCG@3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1044.51 ms /   140 tokens (    7.46 ms per token,   134.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     374.32 ms /     6 runs   (   62.39 ms per token,    16.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.44 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "RAG 평가 진행중:  32%|███▏      | 9/28 [00:33<00:49,  2.62s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2017년 재무상태표상 전기 현금및현금성자산은 얼마입니까?\n",
      "생성 답변: \"문서에 없음\"\n",
      "정답: 3,778,371\n",
      "Retriever 성능지표: {'Precision@3': 0.0, 'Recall@3': 0.0, 'F1@3': 0.0, 'MRR': 0.0, 'nDCG@3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 92 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1086.48 ms /   129 tokens (    8.42 ms per token,   118.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     175.16 ms /     3 runs   (   58.39 ms per token,    17.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1268.81 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "RAG 평가 진행중:  36%|███▌      | 10/28 [00:35<00:45,  2.51s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2017년 재무상태표상 당기 매각예정분류자산은 얼마인가요?\n",
      "생성 답변: 문서에 없음\n",
      "정답: -\n",
      "Retriever 성능지표: {'Precision@3': 1.0, 'Recall@3': 1.0, 'F1@3': 1.0, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     838.12 ms /   111 tokens (    7.55 ms per token,   132.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4394.87 ms /    76 runs   (   57.83 ms per token,    17.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    5263.46 ms /   187 tokens\n",
      "llama_perf_context_print:    graphs reused =         73\n",
      "RAG 평가 진행중:  39%|███▉      | 11/28 [00:42<01:01,  3.61s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2018년 당기 미수금은 재무상태표에서 얼마인가?\n",
      "생성 답변: 문서에 없음. \n",
      "     문서에는 재무상태표에서 2018년 (당기) 미수금이 기록되어 있지만, 문서에는 2018년 (당기) 미수금으로만 기록되어 있으니, 2018년 당기 미수금은 재무상태표에서 \"문서에 없음\"이라고 합니다.\n",
      "정답: 1,515,079\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1084.75 ms /   130 tokens (    8.34 ms per token,   119.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     257.47 ms /     4 runs   (   64.37 ms per token,    15.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.91 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "RAG 평가 진행중:  43%|████▎     | 12/28 [00:44<00:50,  3.15s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2018년 손익계산서상 매출총이익은 얼마인가요?\n",
      "생성 답변: 문서에 없음.\n",
      "정답: 68,715,364\n",
      "Retriever 성능지표: {'Precision@3': 0.0, 'Recall@3': 0.0, 'F1@3': 0.0, 'MRR': 0.0, 'nDCG@3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1160.38 ms /   129 tokens (    9.00 ms per token,   111.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5669.70 ms /    94 runs   (   60.32 ms per token,    16.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    6935.17 ms /   223 tokens\n",
      "llama_perf_context_print:    graphs reused =         90\n",
      "RAG 평가 진행중:  46%|████▋     | 13/28 [00:51<01:07,  4.51s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2019년 재무상태표상 종속기업, 관계기업 및 공동기업 투자는 얼마인가요?\n",
      "생성 답변: 2019년 재무상태표상 종속기업, 관계기업 및 공동기업 투자는 56,571,252백만원입니다.\n",
      "    \n",
      "     (기재의 직접적인 언급은 없지만, 재무상태표에서 2019년 당기 자산총계에서 종속기업, 관계기업 및 공동기업 투자 금액을 뺀 결과가 유형자산이므로, 차이로 구할 수 있습니다.)\n",
      "정답: 56,571,252\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1106.32 ms /   129 tokens (    8.58 ms per token,   116.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     177.76 ms /     3 runs   (   59.25 ms per token,    16.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.52 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "RAG 평가 진행중:  50%|█████     | 14/28 [00:54<00:54,  3.86s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2019년 현금흐름표 상 이익잉여금 배당은 얼마인가요?\n",
      "생성 답변: 문서에 없음\n",
      "정답: (9,618,210)\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 0.5, 'nDCG@3': 0.6309297535714575}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     910.33 ms /   126 tokens (    7.22 ms per token,   138.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     179.58 ms /     3 runs   (   59.86 ms per token,    16.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.05 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "RAG 평가 진행중:  54%|█████▎    | 15/28 [00:55<00:41,  3.22s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2019년 손익계산서상 기본주당이익은 얼마인가요?\n",
      "생성 답변: 문서에 없음\n",
      "정답: 2,260\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 0.5, 'nDCG@3': 0.6309297535714575}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     942.04 ms /   123 tokens (    7.66 ms per token,   130.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =     236.71 ms /     4 runs   (   59.18 ms per token,    16.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1181.54 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "RAG 평가 진행중:  57%|█████▋    | 16/28 [00:57<00:34,  2.84s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2020년 재무상태표 상 자산총계는?\n",
      "생성 답변: 문서에 있음.\n",
      "정답: 229,664,427\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     894.92 ms /   126 tokens (    7.10 ms per token,   140.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1001.39 ms /    16 runs   (   62.59 ms per token,    15.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1914.04 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "RAG 평가 진행중:  61%|██████    | 17/28 [01:00<00:30,  2.77s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2020년 손익계산서 상 판매비와관리비는 얼마인가요?\n",
      "생성 답변: 문서에 29,038,798백만원으로 기재되어 있습니다.\n",
      "정답: 29,038,798\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1353.28 ms /   155 tokens (    8.73 ms per token,   114.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =     480.77 ms /     8 runs   (   60.10 ms per token,    16.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1843.07 ms /   163 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "RAG 평가 진행중:  64%|██████▍   | 18/28 [01:03<00:28,  2.85s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2021년 재무상태표상 당기 기타포괄손익-공정가치금융자산은 얼마인가요?\n",
      "생성 답변: 다른 항목에서 찾을 수 없음\n",
      "정답: 1,662,532\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 97 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     968.32 ms /   106 tokens (    9.14 ms per token,   109.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     209.39 ms /     3 runs   (   69.80 ms per token,    14.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1181.55 ms /   109 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "RAG 평가 진행중:  68%|██████▊   | 19/28 [01:05<00:23,  2.62s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2021년 재무상태표에서 당기 유동비율을 계산하면 얼마인가요?\n",
      "생성 답변: 문서에 없음\n",
      "정답: 1.38\n",
      "Retriever 성능지표: {'Precision@3': 0.0, 'Recall@3': 0.0, 'F1@3': 0.0, 'MRR': 0.0, 'nDCG@3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     976.51 ms /   115 tokens (    8.49 ms per token,   117.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     716.86 ms /    12 runs   (   59.74 ms per token,    16.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1699.66 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "RAG 평가 진행중:  71%|███████▏  | 20/28 [01:08<00:20,  2.61s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2021년 손익계산서 상 당기 금융비용은 얼마인가요?\n",
      "생성 답변: 문서에 있음: 3,698,675백만원\n",
      "정답: 3,698,675\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     919.29 ms /   120 tokens (    7.66 ms per token,   130.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =     182.35 ms /     3 runs   (   60.78 ms per token,    16.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.57 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "RAG 평가 진행중:  75%|███████▌  | 21/28 [01:09<00:16,  2.31s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2022년 재무상태표상 당기 비유동부채는 얼마인가?\n",
      "생성 답변: 문서에 없음\n",
      "정답: 4,581,512\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1135.01 ms /   130 tokens (    8.73 ms per token,   114.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =     357.94 ms /     6 runs   (   59.66 ms per token,    16.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1497.86 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중:  79%|███████▊  | 22/28 [01:11<00:13,  2.24s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2022년 손익계산서 상 당기 법인세비용은 얼마니?\n",
      "생성 답변: 4,273,142\n",
      "정답: 4,273,142\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1146.89 ms /   140 tokens (    8.19 ms per token,   122.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     712.84 ms /    12 runs   (   59.40 ms per token,    16.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1866.43 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "RAG 평가 진행중:  82%|████████▏ | 23/28 [01:14<00:11,  2.31s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2022년 당기 현금흐름표 상 투자활동 현금흐름은 얼마인가?\n",
      "생성 답변: 문서에 있음. -28,123,886백만원\n",
      "정답: (28,123,886)\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     963.66 ms /   125 tokens (    7.71 ms per token,   129.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     891.19 ms /    15 runs   (   59.41 ms per token,    16.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1863.83 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "RAG 평가 진행중:  86%|████████▌ | 24/28 [01:16<00:09,  2.32s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2023년 재무상태표 상 재고자산은 얼마인가?\n",
      "생성 답변: 재고자산은 29,338,151백만원입니다.\n",
      "정답: 29,338,151\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 0.3333333333333333, 'nDCG@3': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     966.37 ms /   118 tokens (    8.19 ms per token,   122.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =     477.54 ms /     8 runs   (   59.69 ms per token,    16.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1449.41 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "RAG 평가 진행중:  89%|████████▉ | 25/28 [01:19<00:07,  2.37s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2023년 당기 영업이익은 얼마인가?\n",
      "생성 답변: \"문서에 없음\"입니다.\n",
      "정답: (11,526,297)\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 0.3333333333333333, 'nDCG@3': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =     965.00 ms /   128 tokens (    7.54 ms per token,   132.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.43 ms /    23 runs   (   60.28 ms per token,    16.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    2365.33 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         21\n",
      "RAG 평가 진행중:  93%|█████████▎| 26/28 [01:22<00:05,  2.65s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2024년에는 재무상태표상 당기 무형자산이 얼마야?\n",
      "생성 답변: 재무상태표상 당기 무형자산은 10,496,956백만원입니다.\n",
      "정답: 10,496,956\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 0.5, 'nDCG@3': 0.6309297535714575}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 97 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1178.92 ms /   109 tokens (   10.82 ms per token,    92.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2421.93 ms /    29 runs   (   83.51 ms per token,    11.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    3615.08 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         27\n",
      "RAG 평가 진행중:  96%|█████████▋| 27/28 [01:26<00:03,  3.17s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2024년 재무상태표 상 당기 우선주자본금은 얼마인가?\n",
      "생성 답변: 재무상태표에서 2024년 (당기) 우선주자본금은 119,467백만원입니다.\n",
      "정답: 119,467\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7474.82 ms\n",
      "llama_perf_context_print: prompt eval time =    1104.63 ms /   126 tokens (    8.77 ms per token,   114.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     395.04 ms /     6 runs   (   65.84 ms per token,    15.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1503.26 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "RAG 평가 진행중: 100%|██████████| 28/28 [01:29<00:00,  3.19s/질문]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: 2024년 손익계산서상 당기 법인세비용은 얼마야?\n",
      "생성 답변: 1,832,987\n",
      "정답: (1,832,987)\n",
      "Retriever 성능지표: {'Precision@3': 0.3333333333333333, 'Recall@3': 1.0, 'F1@3': 0.5, 'MRR': 1.0, 'nDCG@3': 1.0}\n",
      "\n",
      "=== 평균 성능 지표 ===\n",
      "Precision@3    0.285714\n",
      "Recall@3       0.785714\n",
      "F1@3           0.410714\n",
      "MRR            0.684524\n",
      "nDCG@3         0.710457\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#CrossEndcoder 리랭커 실험\n",
    "\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder  # ✅ 추가: 리랭커\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# RAGAS\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas import evaluate\n",
    "\n",
    "# ===== TOP-K 전역 설정 (여기만 바꾸면 전체에 반영) =====\n",
    "TOP_K = 3  # ← 1/3/5 등 원하는 k로 변경\n",
    "\n",
    "# ===== 임베딩 모델 로드 =====\n",
    "MODEL_NAME = \"bge-m3-ko\"\n",
    "EMBED_MODEL_NAME = \"dragonkue/bge-m3-ko\"\n",
    "COLLECTION = \"audit_chunks\"\n",
    "\n",
    "# ===== Qdrant 클라이언트 =====\n",
    "QDRANT_PATH = \"data/vector_store/final-sjchunk/bge-ko-qdrant_db\"\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "# 토크나이저 포크 경고 끄기 (권장)\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# ===== 디바이스 선택 (MPS > CUDA > CPU) =====\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device=device)\n",
    "print(\"✅ 임베딩 모델 로드 완료!\")\n",
    "# ===== (옵션) 리랭커 로드 설정 =====\n",
    "USE_RERANKER = True                 # ✅ 기본은 끔 (기존과 동일 동작)\n",
    "RERANKER_MODEL = \"BAAI/bge-reranker-base\"\n",
    "CANDIDATES_K = 50                    # ✅ 리랭킹 전에 넓게 뽑아올 개수\n",
    "\n",
    "reranker = None\n",
    "if USE_RERANKER:\n",
    "    if torch.cuda.is_available():\n",
    "        ce_device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        ce_device = \"mps\"\n",
    "    else:\n",
    "        ce_device = \"cpu\"\n",
    "    reranker = CrossEncoder(RERANKER_MODEL, device=ce_device)\n",
    "    print(f\"✅ 리랭커 로드 완료! ({RERANKER_MODEL}, device={ce_device})\")\n",
    "\n",
    "# ===== dense_search 함수 =====\n",
    "def extract_year_from_query(query: str):\n",
    "    match = re.search(r\"(20\\d{2}|19\\d{2})\", query)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_numbers(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return \"\".join(re.findall(r\"\\d+\", text.replace(\",\", \"\")))\n",
    "    \n",
    "def dense_search(\n",
    "    query: str,\n",
    "    model,\n",
    "    client,\n",
    "    collection_name: str,\n",
    "    top_k: int = TOP_K,              # ✅ 기본값: TOP_K\n",
    "    ground_truth=None,\n",
    "    debug: bool = False,\n",
    "    use_reranker: bool = False,\n",
    "    reranker_model=None,\n",
    "    candidates_k: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Qdrant query_points 기반 Dense Retriever 함수 + (옵션) 리랭커 + 성능지표 계산\n",
    "    \"\"\"\n",
    "    year = extract_year_from_query(query)\n",
    "    qv = model.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "    if isinstance(ground_truth, str):\n",
    "        ground_truth = [ground_truth]\n",
    "    \n",
    "    query_filter = None\n",
    "    if year:\n",
    "        query_filter = qmodels.Filter(\n",
    "            must=[qmodels.FieldCondition(\n",
    "                key=\"report_year\",\n",
    "                match=qmodels.MatchValue(value=year)\n",
    "            )]\n",
    "        )\n",
    "\n",
    "    # ✅ 리랭커 켜면 candidates_k만큼 넓게 뽑기, 아니면 top_k만\n",
    "    limit_n = candidates_k if (use_reranker and (candidates_k is not None)) else top_k\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=limit_n,\n",
    "        with_payload=True,\n",
    "        query_filter=query_filter\n",
    "    )\n",
    "\n",
    "    # 1차 결과 수집\n",
    "    raw = []\n",
    "    for r in results.points:\n",
    "        payload = r.payload or {}\n",
    "        item = {\"score\": r.score, \"text\": payload.get(\"text\")}\n",
    "        if \"metadata\" in payload:\n",
    "            item.update(payload[\"metadata\"])\n",
    "        else:\n",
    "            item.update(payload)\n",
    "        raw.append(item)\n",
    "\n",
    "    # ✅ (옵션) 리랭킹\n",
    "    if use_reranker and reranker_model is not None and len(raw) > 0:\n",
    "        pairs = [(query, doc.get(\"text\", \"\") or \"\") for doc in raw]\n",
    "        ce_scores = reranker_model.predict(pairs, convert_to_numpy=True)\n",
    "        for i, s in enumerate(ce_scores):\n",
    "            raw[i][\"ce_score\"] = float(s)\n",
    "        raw.sort(key=lambda x: x.get(\"ce_score\", 0.0), reverse=True)\n",
    "        output = raw[:top_k]\n",
    "    else:\n",
    "        output = raw[:top_k]\n",
    "\n",
    "    # ===== 성능지표 (Retriever 기준) =====\n",
    "    metrics = {}\n",
    "    if ground_truth:\n",
    "        normalized_gt = [extract_numbers(gt) for gt in ground_truth]\n",
    "        used = set()\n",
    "        relevances = []\n",
    "\n",
    "        for r in output:\n",
    "            nums = extract_numbers(r.get(\"text\", \"\"))\n",
    "            hit = 0\n",
    "            for gt in normalized_gt:\n",
    "                if gt and gt in nums and gt not in used:\n",
    "                    hit = 1\n",
    "                    used.add(gt)\n",
    "                    break\n",
    "            relevances.append(hit)\n",
    "\n",
    "        # ✅ 메트릭 키를 TOP_K에 맞춰 동적으로 부여\n",
    "        at_key = f\"@{top_k}\"\n",
    "        precision = sum(relevances) / top_k if top_k > 0 else 0.0\n",
    "        metrics[f\"Precision{at_key}\"] = precision\n",
    "        recall = min(sum(relevances), len(normalized_gt)) / len(normalized_gt) if normalized_gt else 0.0\n",
    "        metrics[f\"Recall{at_key}\"] = recall\n",
    "        metrics[f\"F1{at_key}\"] = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "        rr = 0.0\n",
    "        for rank, rel in enumerate(relevances, 1):\n",
    "            if rel == 1:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        metrics[\"MRR\"] = rr\n",
    "\n",
    "        dcg = sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
    "        ideal_hits = min(len(normalized_gt), top_k)\n",
    "        idcg = sum(1.0 / np.log2(idx + 2) for idx in range(ideal_hits))\n",
    "        metrics[f\"nDCG{at_key}\"] = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    return output, metrics\n",
    "\n",
    "# ===== LLM 모델 로드 =====\n",
    "model_path = Path(\"/Users/bag-yebin/Desktop/흠/자연어처리/samsun-audit-rag-qa/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\").resolve()\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(model_path),\n",
    "    n_ctx=8192,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=35\n",
    ")\n",
    "print(\"✅ LLM 모델 로드 완료!\")\n",
    "\n",
    "# ===== RAG Pipeline =====\n",
    "def rag_pipeline(\n",
    "    query: str,\n",
    "    model,\n",
    "    client,\n",
    "    collection_name: str = COLLECTION,\n",
    "    top_k: int = TOP_K,              # ✅ 기본값: TOP_K\n",
    "    ground_truth=None,\n",
    "    use_reranker: bool = False,\n",
    "    reranker_model=None,\n",
    "    candidates_k: int = None\n",
    "):\n",
    "    # 1) Retriever 단계 (+옵션 리랭커)\n",
    "    results, metrics = dense_search(\n",
    "        query=query,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k,\n",
    "        ground_truth=ground_truth,\n",
    "        use_reranker=use_reranker,\n",
    "        reranker_model=reranker_model,\n",
    "        candidates_k=candidates_k\n",
    "    )\n",
    "\n",
    "    # 2) 검색 결과 합치기\n",
    "    context_texts = [r.get(\"text\") for r in results if r.get(\"text\")]\n",
    "    context_text = \"\\n\".join(context_texts)\n",
    "\n",
    "    # 3) Reader 호출\n",
    "    prompt = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    너는 재무보고서 전문가다. \n",
    "    검색된 문서와 metadata만 근거로 답해야 한다. \n",
    "    없는 값은 반드시 \"문서에 없음\"이라고 하라. \n",
    "    추측하거나 새로 만들어내지 마라. \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    검색된 문서:\n",
    "    {context_text}\n",
    "    \n",
    "    질문: {query}\n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=512, stop=[\"<|eot_id|>\"])\n",
    "    answer = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # ✅ \"없음\" 케이스 예외 처리 (키도 TOP_K에 맞춰 부여)\n",
    "    if ground_truth and isinstance(ground_truth, list) and len(ground_truth) > 0 and ground_truth[0] == \"-\":\n",
    "        at_key = f\"@{top_k}\"\n",
    "        if \"없음\" in answer:\n",
    "            metrics = {f\"Precision{at_key}\": 1.0, f\"Recall{at_key}\": 1.0, f\"F1{at_key}\": 1.0, \"MRR\": 1.0, f\"nDCG{at_key}\": 1.0}\n",
    "        else:\n",
    "            metrics = {f\"Precision{at_key}\": 0.0, f\"Recall{at_key}\": 0.0, f\"F1{at_key}\": 0.0, \"MRR\": 0.0, f\"nDCG{at_key}\": 0.0}\n",
    "\n",
    "    return answer, metrics, context_texts\n",
    "\n",
    "# ===== 실행 + 평가 루프 (그대로) =====\n",
    "if __name__ == \"__main__\":\n",
    "    # 질문 & 정답 데이터셋\n",
    "    questions = [\n",
    "    \"2014년 재무상태표 상 당기 유동자산은 얼마인가?\",\n",
    "    \"2014년 현금흐름표 상 당기 영업활동 현금흐름은 얼마인가?\",\n",
    "    \"2015년 당기 비유동자산은 재무상태표에서 얼마인가?\",\n",
    "    \"2015년 손익계산서 상 당기순이익은 얼마인가?\",\n",
    "    \"2016년 재무상태표 상 당기 단기금융상품은 얼마인가요?\",\n",
    "    \"2016년 포괄손익계산서 상 당기 총포괄이익은 얼마니?\",\n",
    "    \"2016년 자본변동표 상 자기주식의 취득은 얼마인가?\",\n",
    "    \"2017년 당기 매출채권은 재무상태표에 따르면 얼마냐?\",\n",
    "    \"2017년 재무상태표상 전기 현금및현금성자산은 얼마입니까?\",\n",
    "    \"2017년 재무상태표상 당기 매각예정분류자산은 얼마인가요?\",\n",
    "    \"2018년 당기 미수금은 재무상태표에서 얼마인가?\",\n",
    "    \"2018년 손익계산서상 매출총이익은 얼마인가요?\",\n",
    "    \"2019년 재무상태표상 종속기업, 관계기업 및 공동기업 투자는 얼마인가요?\",\n",
    "    \"2019년 현금흐름표 상 이익잉여금 배당은 얼마인가요?\",\n",
    "    \"2019년 손익계산서상 기본주당이익은 얼마인가요?\",\n",
    "    \"2020년 재무상태표 상 자산총계는?\",\n",
    "    \"2020년 손익계산서 상 판매비와관리비는 얼마인가요?\",\n",
    "    \"2021년 재무상태표상 당기 기타포괄손익-공정가치금융자산은 얼마인가요?\",\n",
    "    \"2021년 재무상태표에서 당기 유동비율을 계산하면 얼마인가요?\",\n",
    "    \"2021년 손익계산서 상 당기 금융비용은 얼마인가요?\",\n",
    "    \"2022년 재무상태표상 당기 비유동부채는 얼마인가?\",\n",
    "    \"2022년 손익계산서 상 당기 법인세비용은 얼마니?\",\n",
    "    \"2022년 당기 현금흐름표 상 투자활동 현금흐름은 얼마인가?\",\n",
    "    \"2023년 재무상태표 상 재고자산은 얼마인가?\",\n",
    "    \"2023년 당기 영업이익은 얼마인가?\",\n",
    "    \"2024년에는 재무상태표상 당기 무형자산이 얼마야?\",\n",
    "    \"2024년 재무상태표 상 당기 우선주자본금은 얼마인가?\",\n",
    "    \"2024년 손익계산서상 당기 법인세비용은 얼마야?\",\n",
    "    ]\n",
    "    \n",
    "    answers = [\n",
    "        \"62,054,773\",\n",
    "        \"18,653,817\",\n",
    "        \"101,967,575\",\n",
    "        \"12,238,469\",\n",
    "        \"30,170,656\",\n",
    "        \"11,887,806\",\n",
    "        \"(7,707,938)\",\n",
    "        \"27,881,777\",\n",
    "        \"3,778,371\",\n",
    "        \"-\",\n",
    "        \"1,515,079\",\n",
    "        \"68,715,364\",\n",
    "        \"56,571,252\",\n",
    "        \"(9,618,210)\",\n",
    "        \"2,260\",\n",
    "        \"229,664,427\",\n",
    "        \"29,038,798\",\n",
    "        \"1,662,532\",\n",
    "        \"1.38\",\n",
    "        \"3,698,675\",\n",
    "        \"4,581,512\",\n",
    "        \"4,273,142\",\n",
    "        \"(28,123,886)\",\n",
    "        \"29,338,151\",\n",
    "        \"(11,526,297)\",\n",
    "        \"10,496,956\",\n",
    "        \"119,467\",\n",
    "        \"(1,832,987)\",\n",
    "    ]\n",
    "    \n",
    "    qa_pairs = [{\"question\": q, \"ground_truth\": [a]} for q, a in zip(questions, answers)]\n",
    "\n",
    "    eval_data = {\"question\": [], \"answer\": [], \"contexts\": [], \"reference\": []}\n",
    "    metrics_list = []   # ✅ 성능지표 저장 리스트\n",
    "\n",
    "    for qa in tqdm(qa_pairs, desc=\"RAG 평가 진행중\", unit=\"질문\"):\n",
    "        q = qa[\"question\"]\n",
    "        gt = qa[\"ground_truth\"][0]\n",
    "\n",
    "        # ✅ TOP_K 한 번에 적용\n",
    "        answer, metrics, contexts = rag_pipeline(\n",
    "            q,\n",
    "            embed_model,\n",
    "            client,\n",
    "            COLLECTION,\n",
    "            top_k=TOP_K,                 # ← 여기만 TOP_K로\n",
    "            ground_truth=[gt],\n",
    "            use_reranker=USE_RERANKER,\n",
    "            reranker_model=reranker,\n",
    "            candidates_k=CANDIDATES_K\n",
    "        )\n",
    "\n",
    "        eval_data[\"question\"].append(q)\n",
    "        eval_data[\"answer\"].append(answer)\n",
    "        eval_data[\"contexts\"].append(contexts)\n",
    "        eval_data[\"reference\"].append(gt)\n",
    "\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"질문:\", q)\n",
    "        print(\"생성 답변:\", answer)\n",
    "        print(\"정답:\", gt)\n",
    "        print(\"Retriever 성능지표:\", metrics\n",
    "\n",
    "    dataset = Dataset.from_dict(eval_data)\n",
    "\n",
    "    # ✅ 성능지표 평균 계산\n",
    "    df_metrics = pd.DataFrame(metrics_list)\n",
    "    print(\"\\n=== 평균 성능 지표 ===\")\n",
    "    print(df_metrics.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180e48c-be14-48d2-8824-7f0e13d0f599",
   "metadata": {},
   "source": [
    "## RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c33380f-42c4-4f57-b579-c31284cadda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8l/bz01tth90yv2m27jxhbcncfw0000gn/T/ipykernel_12194/4160336032.py:18: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embeddings = LangchainEmbeddingsWrapper(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c8947b8d484c668c35117273964a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f424f42f4a4b6493717c0ea78fa6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1/56:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8690, 'answer_relevancy': 0.1014, 'context_precision': 0.8690, 'context_recall': 0.8571}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# --- LLM (OpenAI GPT-4o-mini 같은 최신 모델 추천) ---\n",
    "llm_langchain = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # 또는 \"gpt-4o\", \"gpt-4-turbo\"\n",
    "    temperature=0,\n",
    "    max_tokens=512\n",
    ")\n",
    "ragas_llm = LangchainLLMWrapper(llm_langchain)\n",
    "\n",
    "# --- Embeddings (OpenAI 기본 임베딩) ---\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\")  # 또는 \"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# --- RAGAS 평가 실행 ---\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdd9fe-84f5-45be-b9e2-6bfec62cf7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
