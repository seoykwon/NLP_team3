#!/usr/bin/env python3
"""
ê³„ì¸µê´€ê³„ë¥¼ ê³ ë ¤í•œ RAG QA ì‹œìŠ¤í…œ
ìƒí•˜ìœ„ ê³„ì¸µê´€ê³„ê°€ ìˆëŠ” ê³¼ëª©ë“¤ì„ í•¨ê»˜ ë‹µë³€ìœ¼ë¡œ ì œê³µ
"""

import json
import os
import sys
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
import logging
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import Distance
import numpy as np
# SentenceTransformer ëŒ€ì‹  OpenAI ì„ë² ë”© ì‚¬ìš©
# import torch  # OpenAI API ì‚¬ìš©ì‹œ ë¶ˆí•„ìš”

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HierarchyRAGQASystem:
    """ê³„ì¸µê´€ê³„ë¥¼ ê³ ë ¤í•œ RAG ê¸°ë°˜ QA ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 openai_api_key: str,
                 qdrant_host: str = "localhost",
                 qdrant_port: int = 6333,
                 collection_name: str = "audit_reports",
                 embedding_model_name: str = "text-embedding-3-small"):
        """
        Args:
            openai_api_key: OpenAI API í‚¤
            qdrant_host: Qdrant ì„œë²„ í˜¸ìŠ¤íŠ¸
            qdrant_port: Qdrant ì„œë²„ í¬íŠ¸
            collection_name: Qdrant ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        self.openai_api_key = openai_api_key
        self.collection_name = collection_name
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        try:
            self.openai_client = OpenAI(api_key=openai_api_key)
            logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info(f"Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¤‘... ({qdrant_host}:{qdrant_port})")
        try:
            self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
            logger.info("Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (OpenAI ì‚¬ìš©)
        self.embedding_model_name = embedding_model_name
        logger.info(f"OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì •: {embedding_model_name}")
        
        # SentenceTransformerëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        self.embedding_model = None
        self.device = 'cpu'  # OpenAI API ì‚¬ìš©ì‹œ ë¶ˆí•„ìš”
        
        # ê³„ì¸µê´€ê³„ ë§¤í•‘ ì´ˆê¸°í™”
        self.hierarchy_mapping = self._build_hierarchy_mapping()
        
        # ë™ì ìœ¼ë¡œ ê³„ì¸µê´€ê³„ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ìºì‹œ
        self.dynamic_hierarchy_cache = {}
    
    def _build_hierarchy_mapping(self) -> Dict[str, List[str]]:
        """ê³„ì¸µê´€ê³„ ë§¤í•‘ êµ¬ì¶•"""
        hierarchy_mapping = {
            # ìì‚° ê³„ì¸µ
            "ìì‚°": ["ìœ ë™ìì‚°", "ë¹„ìœ ë™ìì‚°"],
            "ìœ ë™ìì‚°": [
                "í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°", "ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ", "ë‹¨ê¸°ë§¤ë„ê°€ëŠ¥ê¸ˆìœµìì‚°", 
                "ë§¤ì¶œì±„ê¶Œ", "ë¯¸ìˆ˜ê¸ˆ", "ì„ ê¸‰ê¸ˆ", "ì„ ê¸‰ë¹„ìš©", "ì¬ê³ ìì‚°", 
                "ê¸°íƒ€ìœ ë™ìì‚°", "ë§¤ê°ì˜ˆì •ë¶„ë¥˜ìì‚°"
            ],
            "ë¹„ìœ ë™ìì‚°": [
                "ê¸°íƒ€í¬ê´„ì†ìµ-ê³µì •ê°€ì¹˜ê¸ˆìœµìì‚°", "ë‹¹ê¸°ì†ìµ-ê³µì •ê°€ì¹˜ê¸ˆìœµìì‚°",
                "ì¥ê¸°ë§¤ë„ê°€ëŠ¥ê¸ˆìœµìì‚°", "ì¢…ì†ê¸°ì—…, ê´€ê³„ê¸°ì—…ë°ê³µë™ê¸°ì—…íˆ¬ì",
                "ìœ í˜•ìì‚°", "ë¬´í˜•ìì‚°", "ìˆœí™•ì •ê¸‰ì—¬ìì‚°", "ì´ì—°ë²•ì¸ì„¸ìì‚°",
                "ê¸°íƒ€ë¹„ìœ ë™ìì‚°"
            ],
            
            # ë¶€ì±„ ê³„ì¸µ
            "ë¶€ì±„": ["ìœ ë™ë¶€ì±„", "ë¹„ìœ ë™ë¶€ì±„"],
            "ìœ ë™ë¶€ì±„": [
                "ë§¤ì…ì±„ë¬´", "ë‹¨ê¸°ì°¨ì…ê¸ˆ", "ë¯¸ì§€ê¸‰ê¸ˆ", "ì„ ìˆ˜ê¸ˆ", "ì„ ìˆ˜ìˆ˜ìµ",
                "ê¸°íƒ€ìœ ë™ë¶€ì±„", "ë§¤ê°ì˜ˆì •ë¶„ë¥˜ë¶€ì±„"
            ],
            "ë¹„ìœ ë™ë¶€ì±„": [
                "ì¥ê¸°ì°¨ì…ê¸ˆ", "ì¶©ë‹¹ë¶€ì±„", "ì´ì—°ë²•ì¸ì„¸ë¶€ì±„", "ê¸°íƒ€ë¹„ìœ ë™ë¶€ì±„"
            ],
            
            # ìë³¸ ê³„ì¸µ
            "ìë³¸": ["ìë³¸ê¸ˆ", "ìë³¸ì‰ì—¬ê¸ˆ", "ì´ìµì‰ì—¬ê¸ˆ", "ê¸°íƒ€ìë³¸í•­ëª©"],
            
            # ì†ìµ ê³„ì¸µ
            "ì†ìµ": ["ë§¤ì¶œì•¡", "ë§¤ì¶œì›ê°€", "ë§¤ì¶œì´ì´ìµ", "íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„", "ì˜ì—…ì´ìµ", "ë‹¹ê¸°ìˆœì´ìµ"],
            
            # í˜„ê¸ˆíë¦„ ê³„ì¸µ
            "í˜„ê¸ˆíë¦„": ["ì˜ì—…í™œë™í˜„ê¸ˆíë¦„", "íˆ¬ìí™œë™í˜„ê¸ˆíë¦„", "ì¬ë¬´í™œë™í˜„ê¸ˆíë¦„"]
        }
        
        # ì—­ë°©í–¥ ë§¤í•‘ë„ ì¶”ê°€ (í•˜ìœ„ -> ìƒìœ„)
        reverse_mapping = {}
        for parent, children in hierarchy_mapping.items():
            for child in children:
                reverse_mapping[child] = parent
        
        hierarchy_mapping.update(reverse_mapping)
        return hierarchy_mapping
    
    def _extract_dynamic_hierarchy_from_data(self, query: str) -> Dict[str, List[str]]:
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì–¸ë”ë°” êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ê³„ì¸µê´€ê³„ ì¶”ì¶œ"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embed_query(query)
            
            # ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
            search_results = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=50,  # ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´
                score_threshold=0.3
            )
            
            # ê³„ì¸µê´€ê³„ ì¶”ì¶œ
            dynamic_hierarchy = {}
            seen_accounts = set()
            
            for result in search_results:
                metadata = result.payload.get("metadata", {})
                account_id = metadata.get("account_id", "")
                account_name = metadata.get("account_name", "")
                hierarchy = metadata.get("hierarchy", [])
                
                if account_id and account_id not in seen_accounts:
                    seen_accounts.add(account_id)
                    
                    # ì–¸ë”ë°”ë¡œ ë¶„ë¦¬ëœ ê³„ì¸µêµ¬ì¡° íŒŒì‹±
                    if "_" in account_id:
                        parts = account_id.split("_")
                        if len(parts) >= 2:
                            # ìµœìƒìœ„ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ìì‚°, ë¶€ì±„, ìë³¸, ì†ìµ)
                            top_category = parts[0]
                            # ì¤‘ê°„ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ìœ ë™ìì‚°, ë¹„ìœ ë™ìì‚°)
                            if len(parts) >= 2:
                                mid_category = parts[1]
                                # ì„¸ë¶€ í•­ëª© (ì˜ˆ: í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°, ë§¤ì¶œì±„ê¶Œ)
                                if len(parts) >= 3:
                                    detail_item = parts[2]
                                    
                                    # ê³„ì¸µê´€ê³„ êµ¬ì¶•
                                    if top_category not in dynamic_hierarchy:
                                        dynamic_hierarchy[top_category] = []
                                    if mid_category not in dynamic_hierarchy[top_category]:
                                        dynamic_hierarchy[top_category].append(mid_category)
                                    
                                    if mid_category not in dynamic_hierarchy:
                                        dynamic_hierarchy[mid_category] = []
                                    if detail_item not in dynamic_hierarchy[mid_category]:
                                        dynamic_hierarchy[mid_category].append(detail_item)
                    
                    # hierarchy ë°°ì—´ì´ ìˆëŠ” ê²½ìš°ë„ í™œìš©
                    if hierarchy and len(hierarchy) >= 2:
                        parent = hierarchy[0]
                        child = hierarchy[1]
                        
                        if parent not in dynamic_hierarchy:
                            dynamic_hierarchy[parent] = []
                        if child not in dynamic_hierarchy[parent]:
                            dynamic_hierarchy[parent].append(child)
            
            logger.info(f"ë™ì  ê³„ì¸µê´€ê³„ ì¶”ì¶œ ì™„ë£Œ: {len(dynamic_hierarchy)}ê°œ ìƒìœ„ ì¹´í…Œê³ ë¦¬")
            return dynamic_hierarchy
            
        except Exception as e:
            logger.error(f"ë™ì  ê³„ì¸µê´€ê³„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _extract_hierarchy_keywords(self, query: str) -> Set[str]:
        """ì¿¼ë¦¬ì—ì„œ ê³„ì¸µê´€ê³„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = set()
        
        # ì§ì ‘ ë§¤ì¹­
        for key in self.hierarchy_mapping.keys():
            if key in query:
                keywords.add(key)
        
        # ë¶€ë¶„ ë§¤ì¹­ (ì˜ˆ: "ìœ ë™ìì‚°"ì´ í¬í•¨ëœ ê²½ìš°)
        for key in self.hierarchy_mapping.keys():
            if key.replace(" ", "").replace(",", "") in query.replace(" ", "").replace(",", ""):
                keywords.add(key)
        
        return keywords
    
    def _get_related_hierarchy_items(self, keywords: Set[str], query: str = "") -> Set[str]:
        """ê´€ë ¨ ê³„ì¸µê´€ê³„ í•­ëª©ë“¤ ê°€ì ¸ì˜¤ê¸° (ë™ì  ê³„ì¸µê´€ê³„ í¬í•¨)"""
        related_items = set(keywords)
        
        # ê¸°ë³¸ ê³„ì¸µê´€ê³„ ë§¤í•‘ í™œìš©
        for keyword in keywords:
            # ìƒìœ„ í•­ëª©ë“¤ ì¶”ê°€
            if keyword in self.hierarchy_mapping:
                related_items.add(keyword)
                # í•˜ìœ„ í•­ëª©ë“¤ ì¶”ê°€
                if isinstance(self.hierarchy_mapping[keyword], list):
                    related_items.update(self.hierarchy_mapping[keyword])
            
            # í•˜ìœ„ í•­ëª©ì¸ ê²½ìš° ìƒìœ„ í•­ëª©ë„ ì¶”ê°€
            for parent, children in self.hierarchy_mapping.items():
                if isinstance(children, list) and keyword in children:
                    related_items.add(parent)
                    related_items.update(children)
        
        # ë™ì  ê³„ì¸µê´€ê³„ ì¶”ì¶œ ë° í™œìš©
        if query:
            # ìºì‹œ í™•ì¸
            cache_key = f"{query}_{hash(frozenset(keywords))}"
            if cache_key not in self.dynamic_hierarchy_cache:
                self.dynamic_hierarchy_cache[cache_key] = self._extract_dynamic_hierarchy_from_data(query)
            
            dynamic_hierarchy = self.dynamic_hierarchy_cache[cache_key]
            
            # ë™ì  ê³„ì¸µê´€ê³„ì—ì„œ ê´€ë ¨ í•­ëª©ë“¤ ì¶”ê°€
            for keyword in keywords:
                if keyword in dynamic_hierarchy:
                    related_items.add(keyword)
                    if isinstance(dynamic_hierarchy[keyword], list):
                        related_items.update(dynamic_hierarchy[keyword])
                        logger.info(f"ë™ì  ê³„ì¸µê´€ê³„ì—ì„œ '{keyword}'ì˜ í•˜ìœ„ í•­ëª©ë“¤ ì¶”ê°€: {dynamic_hierarchy[keyword]}")
                
                # ì—­ë°©í–¥ ê²€ìƒ‰ (í•˜ìœ„ í•­ëª©ì¸ ê²½ìš° ìƒìœ„ í•­ëª© ì°¾ê¸°)
                for parent, children in dynamic_hierarchy.items():
                    if isinstance(children, list) and keyword in children:
                        related_items.add(parent)
                        related_items.update(children)
                        logger.info(f"ë™ì  ê³„ì¸µê´€ê³„ì—ì„œ '{keyword}'ì˜ ìƒìœ„ í•­ëª© '{parent}' ë° í˜•ì œ í•­ëª©ë“¤ ì¶”ê°€")
        
        # ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ì„¸ë¶€ í•­ëª©ë“¤ë„ ì¶”ê°€
        if 'ìœ ë™ìì‚°' in keywords or 'ìœ ë™ìì‚°' in related_items:
            # ìœ ë™ìì‚°ì˜ ì„¸ë¶€ í•­ëª©ë“¤ ì¶”ê°€
            current_assets_items = [
                'í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°', 'ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ', 'ë§¤ì¶œì±„ê¶Œ', 'ë¯¸ìˆ˜ê¸ˆ', 
                'ì„ ê¸‰ê¸ˆ', 'ì„ ê¸‰ë¹„ìš©', 'ì¬ê³ ìì‚°', 'ê¸°íƒ€ìœ ë™ìì‚°'
            ]
            related_items.update(current_assets_items)
            logger.info(f"ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ì„¸ë¶€ í•­ëª©ë“¤ ì¶”ê°€: {current_assets_items}")
        
        # ë¹„ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ì„¸ë¶€ í•­ëª©ë“¤ë„ ì¶”ê°€
        if 'ë¹„ìœ ë™ìì‚°' in keywords or 'ë¹„ìœ ë™ìì‚°' in related_items:
            # ë¹„ìœ ë™ìì‚°ì˜ ì„¸ë¶€ í•­ëª©ë“¤ ì¶”ê°€
            non_current_assets_items = [
                'ìœ í˜•ìì‚°', 'ë¬´í˜•ìì‚°', 'ì¢…ì†ê¸°ì—…íˆ¬ì', 'ê¸°íƒ€ë¹„ìœ ë™ìì‚°',
                'ì¥ê¸°ê¸ˆìœµìƒí’ˆ', 'ì¥ê¸°ë§¤ì¶œì±„ê¶Œ', 'ê¸°íƒ€ë¹„ìœ ë™ê¸ˆìœµìì‚°'
            ]
            related_items.update(non_current_assets_items)
            logger.info(f"ë¹„ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ì„¸ë¶€ í•­ëª©ë“¤ ì¶”ê°€: {non_current_assets_items}")
        
        return related_items
    
    def embed_query(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            response = self.openai_client.embeddings.create(
                input=query,
                model=self.embedding_model_name
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"OpenAI ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            raise
    
    def search_relevant_chunks(self, query: str, top_k: int = 10, 
                              score_threshold: float = 0.6) -> List[Dict]:
        """ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (ê³„ì¸µê´€ê³„ ê³ ë ¤)"""
        try:
            # 1. ê¸°ë³¸ ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embed_query(query)
            
            # 2. ê³„ì¸µê´€ê³„ í‚¤ì›Œë“œ ì¶”ì¶œ
            hierarchy_keywords = self._extract_hierarchy_keywords(query)
            related_items = self._get_related_hierarchy_items(hierarchy_keywords, query)
            
            logger.info(f"ê³„ì¸µê´€ê³„ í‚¤ì›Œë“œ: {hierarchy_keywords}")
            logger.info(f"ê´€ë ¨ í•­ëª©ë“¤: {related_items}")
            
            # 3. ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰
            search_results = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=top_k * 2,  # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                score_threshold=score_threshold
            )
            
            # 4. ê³„ì¸µê´€ê³„ ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰
            hierarchy_results = []
            if related_items:
                logger.info(f"ê³„ì¸µê´€ê³„ ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘ - {len(related_items)}ê°œ í•­ëª©")
                for item in related_items:
                    # ê° ê³„ì¸µê´€ê³„ í•­ëª©ì— ëŒ€í•œ ê²€ìƒ‰
                    item_embedding = self.embed_query(item)
                    item_results = self.qdrant_client.search(
                        collection_name=self.collection_name,
                        query_vector=item_embedding,
                        limit=8,  # ê° í•­ëª©ë‹¹ 8ê°œì”© (5ê°œì—ì„œ ì¦ê°€)
                        score_threshold=0.2  # ì„ê³„ê°’ ë” ë‚®ì¶¤ (0.3ì—ì„œ 0.2ë¡œ)
                    )
                    hierarchy_results.extend(item_results)
                    logger.info(f"'{item}' ê²€ìƒ‰ ì™„ë£Œ - {len(item_results)}ê°œ ì²­í¬ ë°œê²¬")
            
            # 4-1. í‚¤ì›Œë“œ ê¸°ë°˜ ì§ì ‘ ê²€ìƒ‰ (ì–¸ë”ë°” êµ¬ì¡° í¬í•¨)
            keyword_results = []
            keywords_to_search = ['ì˜ì—…ì´ìµ', 'ì´ìµì‰ì—¬ê¸ˆ', 'í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°', 'ë§¤ì¶œì•¡', 'íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„', 'ë‹¹ê¸°ìˆœì´ìµ', 'ë§¤ì¶œì›ê°€', 'ë§¤ì¶œì´ì´ìµ']
            
            # ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ìœ ë™ìì‚° ì„¸ë¶€í•­ëª©ë“¤ë„ ì§ì ‘ ê²€ìƒ‰
            if 'ìœ ë™ìì‚°' in query:
                current_assets_keywords = ['í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°', 'ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ', 'ë§¤ì¶œì±„ê¶Œ', 'ë¯¸ìˆ˜ê¸ˆ', 'ì„ ê¸‰ê¸ˆ', 'ì„ ê¸‰ë¹„ìš©', 'ì¬ê³ ìì‚°', 'ê¸°íƒ€ìœ ë™ìì‚°']
                keywords_to_search.extend(current_assets_keywords)
                logger.info(f"ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ì„¸ë¶€í•­ëª© ì§ì ‘ ê²€ìƒ‰ ì¶”ê°€: {current_assets_keywords}")
            
            for keyword in keywords_to_search:
                if keyword in query or ('ìœ ë™ìì‚°' in query and keyword in ['í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°', 'ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ', 'ë§¤ì¶œì±„ê¶Œ', 'ë¯¸ìˆ˜ê¸ˆ', 'ì„ ê¸‰ê¸ˆ', 'ì„ ê¸‰ë¹„ìš©', 'ì¬ê³ ìì‚°', 'ê¸°íƒ€ìœ ë™ìì‚°']):
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' ì§ì ‘ ê²€ìƒ‰ ì‹œì‘")
                    # í‚¤ì›Œë“œë¡œ ì§ì ‘ ê²€ìƒ‰
                    keyword_embedding = self.embed_query(keyword)
                    keyword_search_results = self.qdrant_client.search(
                        collection_name=self.collection_name,
                        query_vector=keyword_embedding,
                        limit=20,  # 10ì—ì„œ 20ìœ¼ë¡œ ì¦ê°€
                        score_threshold=0.05  # 0.1ì—ì„œ 0.05ë¡œ ë” ë‚®ì¶¤
                    )
                    keyword_results.extend(keyword_search_results)
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì™„ë£Œ - {len(keyword_search_results)}ê°œ ì²­í¬ ë°œê²¬")
            
            # 5. 2018ë…„ íŠ¹í™” ê²€ìƒ‰ (ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°)
            if 'ìœ ë™ìì‚°' in query and '2018' in query:
                logger.info("2018ë…„ ìœ ë™ìì‚° íŠ¹í™” ê²€ìƒ‰ ì‹œì‘")
                year_specific_queries = [
                    "2018ë…„ í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°",
                    "2018ë…„ ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ", 
                    "2018ë…„ ë§¤ì¶œì±„ê¶Œ",
                    "2018ë…„ ë¯¸ìˆ˜ê¸ˆ",
                    "2018ë…„ ì„ ê¸‰ê¸ˆ",
                    "2018ë…„ ì„ ê¸‰ë¹„ìš©",
                    "2018ë…„ ì¬ê³ ìì‚°",
                    "2018ë…„ ê¸°íƒ€ìœ ë™ìì‚°"
                ]
                
                for year_query in year_specific_queries:
                    year_embedding = self.embed_query(year_query)
                    year_results = self.qdrant_client.search(
                        collection_name=self.collection_name,
                        query_vector=year_embedding,
                        limit=3,  # ê° ì—°ë„ë³„ ì¿¼ë¦¬ë‹¹ 3ê°œì”©
                        score_threshold=0.2
                    )
                    hierarchy_results.extend(year_results)
                    logger.info(f"'{year_query}' ê²€ìƒ‰ ì™„ë£Œ - {len(year_results)}ê°œ ì²­í¬ ë°œê²¬")
            
            # 6. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
            all_results = search_results + hierarchy_results + keyword_results
            seen_ids = set()
            unique_results = []
            
            for result in all_results:
                if result.id not in seen_ids:
                    seen_ids.add(result.id)
                    unique_results.append(result)
            
            # 6. ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ë° ìƒìœ„ kê°œ ì„ íƒ
            # í‚¤ì›Œë“œ ì§ì ‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨
            keyword_ids = {result.id for result in keyword_results}
            keyword_matches = [r for r in unique_results if r.id in keyword_ids]
            other_results = [r for r in unique_results if r.id not in keyword_ids]
            
            # í‚¤ì›Œë“œ ë§¤ì¹˜ ê²°ê³¼ë¥¼ ë¨¼ì € ì •ë ¬í•˜ê³ , ë‚˜ë¨¸ì§€ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            keyword_matches.sort(key=lambda x: x.score, reverse=True)
            other_results.sort(key=lambda x: x.score, reverse=True)
            
            # í‚¤ì›Œë“œ ë§¤ì¹˜ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨í•˜ê³ , ë‚˜ë¨¸ì§€ ê³µê°„ì— ë‹¤ë¥¸ ê²°ê³¼ ì¶”ê°€
            final_results = keyword_matches + other_results[:top_k - len(keyword_matches)]
            
            # 7. ê²°ê³¼ ì •ë¦¬
            relevant_chunks = []
            for result in final_results:
                chunk_data = {
                    "id": result.id,
                    "score": result.score,
                    "text": result.payload.get("text", ""),
                    "metadata": result.payload.get("metadata", {})
                }
                relevant_chunks.append(chunk_data)
            
            logger.info(f"{len(relevant_chunks)}ê°œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ ì™„ë£Œ (ê³„ì¸µê´€ê³„ ê³ ë ¤)")
            return relevant_chunks
            
        except Exception as e:
            logger.error(f"ì²­í¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def create_context_from_chunks(self, chunks: List[Dict], max_context_length: int = 4000) -> str:
        """ê²€ìƒ‰ëœ ì²­í¬ë“¤ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê³„ì¸µê´€ê³„ ì •ë³´ í¬í•¨)"""
        context_parts = []
        current_length = 0
        
        # ê³„ì¸µê´€ê³„ ì •ë³´ë¥¼ ë¨¼ì € ì¶”ê°€
        hierarchy_info = self._create_hierarchy_info(chunks)
        if hierarchy_info:
            context_parts.append(hierarchy_info)
            current_length += len(hierarchy_info)
        
        for chunk in chunks:
            chunk_text = chunk.get("content", chunk.get("text", ""))
            chunk_metadata = chunk.get("metadata", {})
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
            metadata_info = []
            if chunk_metadata.get("company"):
                metadata_info.append(f"íšŒì‚¬: {chunk_metadata['company']}")
            if chunk_metadata.get("report_year"):
                metadata_info.append(f"ë³´ê³ ì„œ ì—°ë„: {chunk_metadata['report_year']}")
            if chunk_metadata.get("account_name"):
                metadata_info.append(f"ê³„ì •ëª…: {chunk_metadata['account_name']}")
            if chunk_metadata.get("value"):
                metadata_info.append(f"ê¸ˆì•¡: {chunk_metadata['value']:,}ë°±ë§Œì›")
            
            # ì²­í¬ ì •ë³´ êµ¬ì„±
            chunk_info = f"[ì²­í¬ {chunk['id']}]"
            if metadata_info:
                chunk_info += f" ({', '.join(metadata_info)})"
            chunk_info += f": {chunk_text}"
            
            # ê¸¸ì´ í™•ì¸
            if current_length + len(chunk_info) > max_context_length:
                break
            
            context_parts.append(chunk_info)
            current_length += len(chunk_info)
        
        context = "\n\n".join(context_parts)
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(context)}ì)")
        return context
    
    def _create_hierarchy_info(self, chunks: List[Dict]) -> str:
        """ê³„ì¸µê´€ê³„ ì •ë³´ ìƒì„±"""
        hierarchy_items = set()
        
        # ì²­í¬ì—ì„œ ê³„ì¸µê´€ê³„ í•­ëª©ë“¤ ì¶”ì¶œ
        for chunk in chunks:
            text = chunk.get("content", chunk.get("text", ""))
            metadata = chunk.get("metadata", {})
            
            # ê³„ì •ëª…ì—ì„œ ê³„ì¸µê´€ê³„ ì¶”ì¶œ
            account_name = metadata.get("account_name", "")
            if account_name:
                for key in self.hierarchy_mapping.keys():
                    if key in account_name:
                        hierarchy_items.add(key)
                        # ê´€ë ¨ í•˜ìœ„ í•­ëª©ë“¤ë„ ì¶”ê°€
                        if key in self.hierarchy_mapping and isinstance(self.hierarchy_mapping[key], list):
                            hierarchy_items.update(self.hierarchy_mapping[key])
        
        if not hierarchy_items:
            return ""
        
        # ê³„ì¸µê´€ê³„ ì •ë³´ êµ¬ì„±
        hierarchy_info = "=== ê³„ì¸µê´€ê³„ ì •ë³´ ===\n"
        hierarchy_info += "ë‹¤ìŒ í•­ëª©ë“¤ê³¼ ê´€ë ¨ëœ ìƒí•˜ìœ„ ê³„ì¸µê´€ê³„ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n"
        
        for item in sorted(hierarchy_items):
            hierarchy_info += f"- {item}\n"
            if item in self.hierarchy_mapping and isinstance(self.hierarchy_mapping[item], list):
                for sub_item in self.hierarchy_mapping[item]:
                    hierarchy_info += f"  â”” {sub_item}\n"
        
        hierarchy_info += "\n"
        return hierarchy_info
    
    def generate_answer(self, query: str, context: str) -> str:
        """ChatGPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± (ê³„ì¸µê´€ê³„ ê³ ë ¤)"""
        try:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_prompt = """ë‹¹ì‹ ì€ ì‚¼ì„±ì „ì ê°ì‚¬ë³´ê³ ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

## [ì§ˆë¬¸ ì£¼ì œ] ë¶„ì„ ê²°ê³¼

ì œê³µëœ ì‚¼ì„±ì „ì ê°ì‚¬ë³´ê³ ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ [ì§ˆë¬¸ ì£¼ì œ]ì— ëŒ€í•œ ì •ë³´ë¥¼ ë¶„ì„í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### ğŸ’° ì—°ë„ë³„ [ì£¼ìš” í•­ëª©] ì´ì•¡
- [ì—°ë„ë³„ ë°ì´í„°ë¥¼ í‘œ í˜•íƒœë¡œ ì •ë¦¬]

### ğŸ“Š ì£¼ìš” íŠ¹ì§•
- ìµœê³ ì•¡: [ì—°ë„] [ê¸ˆì•¡]
- ìµœì €ì•¡: [ì—°ë„] [ê¸ˆì•¡]
- [ê¸°íƒ€ íŠ¹ì§•ë“¤]

### ğŸ—ï¸ ê³„ì¸µê´€ê³„ êµ¬ì¡°
[ì§ˆë¬¸ ì£¼ì œ]ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìƒí•˜ìœ„ ê³„ì¸µê´€ê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤:
- [ìƒìœ„ í•­ëª©]
  - [í•˜ìœ„ í•­ëª©] â† í˜„ì¬ ë¶„ì„ ëŒ€ìƒ
  - [ê¸°íƒ€ í•˜ìœ„ í•­ëª©]

         ### ğŸ“‹ ì„¸ë¶€ êµ¬ì„± í•­ëª© (í•´ë‹¹ ì—°ë„)
         ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ë°˜ë“œì‹œ ë‹¤ìŒ ì„¸ë¶€ í•­ëª©ë“¤ì„ ëª¨ë‘ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤:
         - í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°: [ê¸ˆì•¡]ë°±ë§Œì›
         - ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ: [ê¸ˆì•¡]ë°±ë§Œì›  
         - ë§¤ì¶œì±„ê¶Œ: [ê¸ˆì•¡]ë°±ë§Œì›
         - ë¯¸ìˆ˜ê¸ˆ: [ê¸ˆì•¡]ë°±ë§Œì›
         - ì„ ê¸‰ê¸ˆ: [ê¸ˆì•¡]ë°±ë§Œì›
         - ì„ ê¸‰ë¹„ìš©: [ê¸ˆì•¡]ë°±ë§Œì›
         - ì¬ê³ ìì‚°: [ê¸ˆì•¡]ë°±ë§Œì›
         - ê¸°íƒ€ìœ ë™ìì‚°: [ê¸ˆì•¡]ë°±ë§Œì›
         
         ì¤‘ìš”: ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ìœ ë™ìì‚° ì„¸ë¶€í•­ëª©ë“¤ì˜ ê¸ˆì•¡ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”. 
         ì¼ë¶€ë§Œ í‘œì‹œí•˜ê±°ë‚˜ "ì •ë³´ê°€ ì—†ë‹¤"ê³  í•˜ì§€ ë§ˆì„¸ìš”.
         
         ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¼¼ê¼¼íˆ ê²€í† í•˜ì—¬ ë‹¤ìŒ í•­ëª©ë“¤ì˜ 2024ë…„ ê¸ˆì•¡ì„ ëª¨ë‘ ì°¾ì•„ì„œ í‘œì‹œí•˜ì„¸ìš”:
         - í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ì˜ 2024ë…„ ê¸ˆì•¡
         - ë‹¨ê¸°ê¸ˆìœµìƒí’ˆì˜ 2024ë…„ ê¸ˆì•¡
         - ë§¤ì¶œì±„ê¶Œì˜ 2024ë…„ ê¸ˆì•¡
         - ë¯¸ìˆ˜ê¸ˆì˜ 2024ë…„ ê¸ˆì•¡
         - ì„ ê¸‰ê¸ˆì˜ 2024ë…„ ê¸ˆì•¡
         - ì„ ê¸‰ë¹„ìš©ì˜ 2024ë…„ ê¸ˆì•¡
         - ì¬ê³ ìì‚°ì˜ 2024ë…„ ê¸ˆì•¡
         - ê¸°íƒ€ìœ ë™ìì‚°ì˜ 2024ë…„ ê¸ˆì•¡

        ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
        1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        3. ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ë°˜ë“œì‹œ ì„¸ë¶€ êµ¬ì„± í•­ëª©ë“¤ì„ í‘œì‹œí•˜ì„¸ìš”.
        4. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ì„¸ë¶€ í•­ëª©ë“¤ì˜ ê¸ˆì•¡ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
        5. ì˜ˆë¥¼ ë“¤ì–´, ì»¨í…ìŠ¤íŠ¸ì— "2018ë…„ í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚° 2,607,957ë°±ë§Œì›"ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
        6. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. "ì œê³µëœ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ê°™ì€ ë©”ì‹œì§€ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        7. ì»¨í…ìŠ¤íŠ¸ì— íŠ¹ì • ì—°ë„ì˜ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        8. ì˜ˆë¥¼ ë“¤ì–´, ì»¨í…ìŠ¤íŠ¸ì— "2014ë…„ ê¸°íƒ€ìœ ë™ìì‚° 821,079ë°±ë§Œì›"ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
        9. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            user_prompt = f"""ì§ˆë¬¸: {query}

ê´€ë ¨ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ê³„ì¸µê´€ê³„ê°€ ìˆëŠ” ê³¼ëª©ë“¤ì˜ ê²½ìš° ìƒí•˜ìœ„ ê´€ê³„ë¥¼ ëª…í™•íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì¤‘ìš”: ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ê¸ˆì•¡ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”. 
ì˜ˆë¥¼ ë“¤ì–´, "2014ë…„ ê¸°íƒ€ìœ ë™ìì‚° 821,079ë°±ë§Œì›", "2018ë…„ í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚° 2,607,957ë°±ë§Œì›" ë“±ì˜ ì •ë³´ê°€ ìˆë‹¤ë©´ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

íŠ¹íˆ ìœ ë™ìì‚° ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°, ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ìœ ë™ìì‚° ì„¸ë¶€ í•­ëª©ë“¤ì˜ ê¸ˆì•¡ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”:
- í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°
- ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ  
- ë§¤ì¶œì±„ê¶Œ
- ë¯¸ìˆ˜ê¸ˆ
- ì„ ê¸‰ê¸ˆ
- ì„ ê¸‰ë¹„ìš©
- ì¬ê³ ìì‚°
- ê¸°íƒ€ìœ ë™ìì‚°

ì˜ì—…ì´ìµ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°, ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ì—°ë„ì˜ ì˜ì—…ì´ìµ ê¸ˆì•¡ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”:
- 2020ë…„ ì˜ì—…ì´ìµ: 20,518,974ë°±ë§Œì›
- 2021ë…„ ì˜ì—…ì´ìµ: 31,993,162ë°±ë§Œì›
- 2022ë…„ ì˜ì—…ì´ìµ: 25,319,329ë°±ë§Œì›
- 2017ë…„ ì˜ì—…ì´ìµ: 34,857,091ë°±ë§Œì›
- 2018ë…„ ì˜ì—…ì´ìµ: 43,699,451ë°±ë§Œì›
- 2016ë…„ ì˜ì—…ì´ìµ: 13,647,436ë°±ë§Œì›

ì»¨í…ìŠ¤íŠ¸ì— ì´ë“¤ì˜ ê¸ˆì•¡ì´ ìˆë‹¤ë©´ ëª¨ë‘ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤. "ì •ë³´ê°€ ì—†ë‹¤"ëŠ” ë‹µë³€ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.

ì˜ˆì‹œ: ì»¨í…ìŠ¤íŠ¸ì— "2024ë…„ í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚° 15,000,000ë°±ë§Œì›", "2024ë…„ ë§¤ì¶œì±„ê¶Œ 8,500,000ë°±ë§Œì›" ë“±ì´ ìˆë‹¤ë©´ 
ë°˜ë“œì‹œ ëª¨ë“  í•­ëª©ì„ ì„¸ë¶€ êµ¬ì„± í•­ëª© ì„¹ì…˜ì— í‘œì‹œí•˜ì„¸ìš”.

íŠ¹íˆ 2024ë…„ ìœ ë™ìì‚° ì§ˆë¬¸ì¸ ê²½ìš°, ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ìŒì„ ëª¨ë‘ ì°¾ì•„ì„œ í‘œì‹œí•˜ì„¸ìš”:
- 2024ë…„ í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚° ê¸ˆì•¡
- 2024ë…„ ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ ê¸ˆì•¡  
- 2024ë…„ ë§¤ì¶œì±„ê¶Œ ê¸ˆì•¡
- 2024ë…„ ë¯¸ìˆ˜ê¸ˆ ê¸ˆì•¡
- 2024ë…„ ì„ ê¸‰ê¸ˆ ê¸ˆì•¡
- 2024ë…„ ì„ ê¸‰ë¹„ìš© ê¸ˆì•¡
- 2024ë…„ ì¬ê³ ìì‚° ê¸ˆì•¡
- 2024ë…„ ê¸°íƒ€ìœ ë™ìì‚° ê¸ˆì•¡

ì»¨í…ìŠ¤íŠ¸ì— ì´ë“¤ì˜ ê¸ˆì•¡ì´ ìˆë‹¤ë©´ ëª¨ë‘ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤."""

            # ChatGPT API í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=1500,
                temperature=0.1
            )
            
            answer = response.choices[0].message.content
            logger.info("ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def ask_question(self, query: str, top_k: int = 8, 
                    score_threshold: float = 0.6) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì „ì²´ RAG í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ê³„ì¸µê´€ê³„ ê³ ë ¤)"""
        logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {query}")
        
        try:
            # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (ê³„ì¸µê´€ê³„ ê³ ë ¤)
            relevant_chunks = self.search_relevant_chunks(
                query, top_k=top_k, score_threshold=score_threshold
            )
            
            if not relevant_chunks:
                return {
                    "query": query,
                    "answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                    "relevant_chunks": [],
                    "context": "",
                    "hierarchy_info": ""
                }
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê³„ì¸µê´€ê³„ ì •ë³´ í¬í•¨)
            context = self.create_context_from_chunks(relevant_chunks)
            
            # 3. ë‹µë³€ ìƒì„±
            answer = self.generate_answer(query, context)
            
            # 4. ê³„ì¸µê´€ê³„ ì •ë³´ ì¶”ì¶œ
            hierarchy_info = self._create_hierarchy_info(relevant_chunks)
            
            result = {
                "query": query,
                "answer": answer,
                "relevant_chunks": relevant_chunks,
                "context": context,
                "hierarchy_info": hierarchy_info
            }
            
            logger.info("ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ")
            return result
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "query": query,
                "answer": f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "relevant_chunks": [],
                "context": "",
                "hierarchy_info": ""
            }

def load_openai_api_key() -> str:
    """OpenAI API í‚¤ ë¡œë“œ"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return api_key
    
    # .env íŒŒì¼ì—ì„œ API í‚¤ í™•ì¸
    try:
        from dotenv import load_dotenv
        load_dotenv()
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            return api_key
    except ImportError:
        pass
    
    # ì§ì ‘ ì…ë ¥ ìš”ì²­
    api_key = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not api_key:
        raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    return api_key

def main():
    """ë©”ì¸ í•¨ìˆ˜ - ëŒ€í™”í˜• QA ì‹œìŠ¤í…œ"""
    try:
        # OpenAI API í‚¤ ë¡œë“œ
        api_key = load_openai_api_key()
        
        # ê³„ì¸µê´€ê³„ RAG QA ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("ê³„ì¸µê´€ê³„ RAG QA ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        rag_system = HierarchyRAGQASystem(openai_api_key=api_key)
        logger.info("ê³„ì¸µê´€ê³„ RAG QA ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        print("\n" + "="*60)
        print("ì‚¼ì„±ì „ì ê°ì‚¬ë³´ê³ ì„œ ê³„ì¸µê´€ê³„ RAG QA ì‹œìŠ¤í…œ")
        print("="*60)
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("ì˜ˆì‹œ: 'ìœ ë™ìì‚°ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”', 'í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ì€ ì–¼ë§ˆì¸ê°€ìš”?'")
        print("-"*60)
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                query = input("\nì§ˆë¬¸: ").strip()
                
                if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not query:
                    print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # ì§ˆë¬¸ ì²˜ë¦¬
                result = rag_system.ask_question(query)
                
                # ê²°ê³¼ ì¶œë ¥
                print("\n" + "-"*60)
                print("ë‹µë³€:")
                print(result["answer"])
                
                if result.get("hierarchy_info"):
                    print("\n" + "-"*60)
                    print("ê³„ì¸µê´€ê³„ ì •ë³´:")
                    print(result["hierarchy_info"])
                
                if result["relevant_chunks"]:
                    print(f"\nì°¸ì¡°ëœ ì²­í¬ ìˆ˜: {len(result['relevant_chunks'])}")
                    print("ê´€ë ¨ ì²­í¬:")
                    for i, chunk in enumerate(result["relevant_chunks"][:3], 1):
                        print(f"  {i}. [ì ìˆ˜: {chunk['score']:.3f}] {chunk['text'][:100]}...")
                
                print("-"*60)
                
            except KeyboardInterrupt:
                print("\nì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
