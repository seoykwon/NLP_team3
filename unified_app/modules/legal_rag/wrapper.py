#!/usr/bin/env python3
"""
ë²•ë¥  ë¬¸ì„œ RAG ì‹œìŠ¤í…œ ë˜í¼
í†µí•© ì•±ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤
"""

import streamlit as st
import os
import sys
from pathlib import Path
import logging
from typing import Optional
import json
import numpy as np
import faiss
import re
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from openai import OpenAI

logger = logging.getLogger(__name__)

class SimpleCommercialLawRAG:
    """ìƒë²• RAG ì‹œìŠ¤í…œ (ê°„ì†Œí™” ë²„ì „)"""
    
    def __init__(self, openai_api_key: str):
        self.openai_api_key = openai_api_key
        self.openai_client = OpenAI(api_key=openai_api_key)
        
        # ë°ì´í„° ê²½ë¡œ ì„¤ì • (extra í´ë” ê¸°ì¤€)
        project_root = Path(__file__).parent.parent.parent.parent
        self.base_path = project_root / "extra" / "ìƒë²• 2"
        self.index_path = self.base_path / "kcc_index_json" / "kcc.index"
        self.ids_path = self.base_path / "kcc_index_json" / "ids.npy"
        self.metas_path = self.base_path / "kcc_index_json" / "metas.json"
        self.model_name = "intfloat/multilingual-e5-base"
        
        self.index = None
        self.ids = None
        self.metas = None
        self.model = None
        self.tfidf = None
        self.X_tfidf = None
        self._initialized = False
    
    def initialize(self):
        """ë¦¬ì†ŒìŠ¤ ë¡œë“œ"""
        if self._initialized:
            return True
        
        try:
            with st.spinner("ìƒë²• ì‹œìŠ¤í…œ ë¡œë”© ì¤‘..."):
                # ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not all([self.index_path.exists(), self.ids_path.exists(), self.metas_path.exists()]):
                    st.error("ìƒë²• ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                # ì¸ë±ìŠ¤/ë©”íƒ€ ë¡œë“œ
                self.index = faiss.read_index(str(self.index_path))
                self.ids = np.load(self.ids_path, allow_pickle=True).tolist()
                with open(self.metas_path, "r", encoding="utf-8") as f:
                    self.metas = json.load(f)
                
                # ëª¨ë¸ ë¡œë“œ
                self.model = SentenceTransformer(self.model_name)
                
                # TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•
                self._build_tfidf()
                
                self._initialized = True
                st.success("âœ… ìƒë²• ì‹œìŠ¤í…œ ë¡œë”© ì™„ë£Œ")
                return True
                
        except Exception as e:
            st.error(f"ìƒë²• ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"Commercial law system load failed: {e}")
            return False
    
    def _build_tfidf(self):
        """TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•"""
        texts = []
        for m in self.metas:
            alias = " ".join(m.get("aliases", []))
            raw = m.get("raw_text", "")
            texts.append((alias + " " + raw).strip())
        
        self.tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95)
        self.X_tfidf = self.tfidf.fit_transform(texts)
    
    def normalize_query(self, q: str):
        """ì§ˆì˜ ì •ê·œí™”"""
        qn = q.strip()
        # ì¡°ë¬¸ ë²ˆí˜¸ ì •ê·œí™”
        m = re.search(r"(\d+)\s*ì˜\s*(\d+)", qn)
        if m:
            qn += f" {m.group(1)}-{m.group(2)} ì œ{m.group(1)}ì¡°ì˜{m.group(2)}"
        return qn
    
    def search_and_answer(self, query: str, topk: int = 5):
        """ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±"""
        if not self._initialized:
            if not self.initialize():
                return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        if not all([self.index, self.model, self.tfidf]):
            return "ì‹œìŠ¤í…œì´ ì™„ì „íˆ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        try:
            # ì§ˆì˜ ì •ê·œí™” ë° ì„ë² ë”©
            qn = self.normalize_query(query)
            qv = self.model.encode([qn], convert_to_numpy=True, normalize_embeddings=True)
            
            # FAISS ê²€ìƒ‰
            D, I = self.index.search(qv, topk*3)
            I, D = I[0], D[0]
            
            # TF-IDF ê²€ìƒ‰
            qv_t = self.tfidf.transform([qn])
            S_t = linear_kernel(qv_t, self.X_tfidf).ravel()
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²°í•©
            results = []
            seen = set()
            
            # ì„ë² ë”© ê²°ê³¼ ì¶”ê°€
            for i, score in zip(I, D):
                if i >= 0 and i not in seen:
                    m = self.metas[int(i)]
                    results.append({
                        "id": self.ids[int(i)],
                        "article_id": m.get("article_id"),
                        "title": m.get("title"),
                        "content": m.get("raw_text", ""),
                        "score": float(score)
                    })
                    seen.add(i)
            
            # TF-IDF ìƒìœ„ ê²°ê³¼ ë³´ì™„
            tfidf_top = S_t.argsort()[::-1][:topk]
            for i in tfidf_top:
                if i not in seen and len(results) < topk:
                    m = self.metas[i]
                    results.append({
                        "id": self.ids[i],
                        "article_id": m.get("article_id"),
                        "title": m.get("title"),
                        "content": m.get("raw_text", ""),
                        "score": float(S_t[i])
                    })
                    seen.add(i)
            
            # ìƒìœ„ ê²°ê³¼ë§Œ ì„ íƒ
            results = results[:topk]
            
            # GPT ë‹µë³€ ìƒì„±
            answer = self._generate_answer(query, results)
            
            return answer, results
        
        except Exception as e:
            logger.error(f"Search and answer failed: {e}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", []
    
    def _generate_answer(self, query, results):
        """GPT ë‹µë³€ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for r in results:
            article_id = str(r["article_id"])
            header = f"ìƒë²• ì œ{article_id}ì¡°"
            if "ì˜" in article_id:
                header = f"ìƒë²• ì œ{article_id.replace('ì˜', 'ì¡°ì˜')}"
            
            title = r.get("title", "")
            if title:
                header += f"({title})"
            
            content = r.get("content", "")[:500]  # ê¸¸ì´ ì œí•œ
            context_parts.append(f"### {header}\n{content}")
        
        context = "\n\n".join(context_parts)
        
        system_msg = (
            "ë„ˆëŠ” í•œêµ­ ìƒë²• ì „ë¬¸ê°€ì•¼. ì œê³µëœ ì¡°ë¬¸ë“¤ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´. "
            "ë‹µë³€ ëì— ì°¸ì¡°í•œ ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•´."
        )
        
        user_msg = f"ì§ˆë¬¸: {query}\n\nê·¼ê±° ì¡°ë¬¸:\n{context}"
        
        try:
            resp = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                temperature=0.2,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ]
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"GPT answer generation failed: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

class SimpleKIFRSRAG:
    """K-IFRS RAG ì‹œìŠ¤í…œ (ê°„ì†Œí™” ë²„ì „)"""
    
    def __init__(self, openai_api_key: str):
        self.openai_api_key = openai_api_key
        self.openai_client = OpenAI(api_key=openai_api_key)
        
        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        project_root = Path(__file__).parent.parent.parent.parent
        self.base_path = project_root / "extra" / "ê¸°ì¤€ì„œ 2"
        self.json_path = self.base_path / "ê¸°ì¤€ì„œ íŒŒì‹±.json"
        self.cache_dir = self.base_path / "hf_cache"
        self.title_emb_path = self.cache_dir / "title_emb_intfloat_multilingual-e5-large.npy"
        self.para_emb_path = self.cache_dir / "para_emb_intfloat_multilingual-e5-large.npy"
        self.model_name = "intfloat/multilingual-e5-large"
        
        self.docs = []
        self.paragraphs = []
        self.title_vecs = None
        self.para_vecs = None
        self.model = None
        self._initialized = False
    
    def initialize(self):
        """ë¦¬ì†ŒìŠ¤ ë¡œë“œ"""
        if self._initialized:
            return True
        
        try:
            with st.spinner("K-IFRS ì‹œìŠ¤í…œ ë¡œë”© ì¤‘..."):
                # ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not self.json_path.exists():
                    st.error("K-IFRS ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                # JSON ë°ì´í„° ë¡œë“œ
                with self.json_path.open(encoding='utf-8') as f:
                    data = json.load(f)
                
                self.docs = data.get('documents', [])
                
                # ë¬¸ë‹¨ ë°ì´í„° êµ¬ì„±
                for d in self.docs:
                    std = d.get('standard_no')
                    title = d.get('title', '')
                    for p in d.get('paragraphs', []):
                        self.paragraphs.append({
                            "std": std,
                            "title": title,
                            "para_id": p.get('para_id'),
                            "text": p.get('text', ''),
                            "page": p.get('page')
                        })
                
                # ì„ë² ë”© ë¡œë“œ (ìˆëŠ” ê²½ìš°)
                if self.para_emb_path.exists():
                    self.para_vecs = np.load(self.para_emb_path)
                
                # ëª¨ë¸ ë¡œë“œ
                self.model = SentenceTransformer(self.model_name)
                
                self._initialized = True
                st.success("âœ… K-IFRS ì‹œìŠ¤í…œ ë¡œë”© ì™„ë£Œ")
                return True
                
        except Exception as e:
            st.error(f"K-IFRS ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"K-IFRS system load failed: {e}")
            return False
    
    def search_and_answer(self, query: str, topk: int = 5):
        """ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±"""
        if not self._initialized:
            if not self.initialize():
                return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        try:
            if self.para_vecs is not None:
                # ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©
                query_emb = self.model.encode([f"query: {query}"], normalize_embeddings=True)[0]
                similarities = np.dot(self.para_vecs, query_emb)
                top_indices = np.argpartition(similarities, -topk*3)[-topk*3:]
                top_indices = top_indices[np.argsort(similarities[top_indices])[::-1][:topk]]
            else:
                # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
                query_emb = self.model.encode([query], normalize_embeddings=True)[0]
                similarities = []
                for para in self.paragraphs:
                    para_emb = self.model.encode([para["text"]], normalize_embeddings=True)[0]
                    sim = np.dot(query_emb, para_emb)
                    similarities.append(sim)
                
                similarities = np.array(similarities)
                top_indices = similarities.argsort()[::-1][:topk]
            
            results = []
            for idx in top_indices:
                para = self.paragraphs[idx]
                results.append({
                    "std": para["std"],
                    "title": para["title"],
                    "para_id": para["para_id"],
                    "text": para["text"],
                    "page": para.get("page"),
                    "score": float(similarities[idx]) if isinstance(similarities, np.ndarray) else 0.0
                })
            
            # GPT ë‹µë³€ ìƒì„±
            answer = self._generate_answer(query, results)
            
            return answer, results
        
        except Exception as e:
            logger.error(f"K-IFRS search and answer failed: {e}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", []
    
    def _generate_answer(self, query, results):
        """GPT ë‹µë³€ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for r in results:
            header = f"[{r['std']}:{r['para_id']}] {r['title']} (p.{r.get('page', '?')})"
            text = r['text'][:500]  # ê¸¸ì´ ì œí•œ
            context_parts.append(f"{header}\n{text}")
        
        context = "\n\n".join(context_parts)
        
        system_msg = (
            "ë„ˆëŠ” K-IFRS íšŒê³„ê¸°ì¤€ ì „ë¬¸ê°€ì•¼. ì œê³µëœ ë¬¸ë‹¨ë“¤ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´. "
            "ë‹µë³€ ëì— ì°¸ì¡°í•œ ê¸°ì¤€ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•´."
        )
        
        user_msg = f"ì§ˆë¬¸: {query}\n\nê·¼ê±° ë¬¸ë‹¨:\n{context}"
        
        try:
            resp = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                temperature=0.2,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ]
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"K-IFRS GPT answer generation failed: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

def show_legal_rag_interface():
    """ë²•ë¥  ë¬¸ì„œ RAG ì¸í„°í˜ì´ìŠ¤ í‘œì‹œ"""
    st.markdown("# âš–ï¸ ë²•ë¥  ë¬¸ì„œ RAG ì‹œìŠ¤í…œ")
    
    # API í‚¤ í™•ì¸
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from utils import load_openai_api_key
    api_key = load_openai_api_key()
    
    if not api_key:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.info("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì— ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    # ì‹œìŠ¤í…œ ì„ íƒ
    system_choice = st.selectbox(
        "ì‹œìŠ¤í…œ ì„ íƒ",
        ["ìƒë²• LLM", "K-IFRS RAG"],
        index=0
    )
    
    if system_choice == "ìƒë²• LLM":
        st.header("ğŸ›ï¸ ìƒë²• LLM ì‹œìŠ¤í…œ")
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if 'commercial_law_rag' not in st.session_state:
            st.session_state.commercial_law_rag = SimpleCommercialLawRAG(api_key)
        
        commercial_rag = st.session_state.commercial_law_rag
        
        # ì§ˆë¬¸ ì…ë ¥
        query = st.text_input(
            "ìƒë²• ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ì¤€ë¹„ê¸ˆì˜ ìë³¸ì „ì…ì€ ì´ì‚¬íšŒì—ì„œ ê²°ì •í•  ìˆ˜ ìˆëŠ”ê°€?"
        )
        
        col1, col2 = st.columns([1, 4])
        with col1:
            topk = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜", 3, 10, 5)
        
        if query and st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
            with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                answer, results = commercial_rag.search_and_answer(query, topk)
                
                st.markdown("### ğŸ“ ë‹µë³€")
                st.markdown(answer)
                
                if results:
                    st.markdown("### ğŸ” ì°¸ì¡° ì¡°ë¬¸")
                    for i, result in enumerate(results, 1):
                        with st.expander(f"{i}. ìƒë²• ì œ{result['article_id']}ì¡° - {result.get('title', '')} (ì ìˆ˜: {result['score']:.3f})"):
                            content = result['content']
                            st.write(content[:800] + "..." if len(content) > 800 else content)
    
    else:  # K-IFRS RAG
        st.header("ğŸ“Š K-IFRS RAG ì‹œìŠ¤í…œ")
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if 'kifrs_rag' not in st.session_state:
            st.session_state.kifrs_rag = SimpleKIFRSRAG(api_key)
        
        kifrs_rag = st.session_state.kifrs_rag
        
        # ì§ˆë¬¸ ì…ë ¥
        query = st.text_input(
            "K-IFRS ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ê°œë°œë¹„ ìì‚° ì¸ì‹ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        )
        
        col1, col2 = st.columns([1, 4])
        with col1:
            topk = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜", 3, 10, 5)
        
        if query and st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
            with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                answer, results = kifrs_rag.search_and_answer(query, topk)
                
                st.markdown("### ğŸ“ ë‹µë³€")
                st.markdown(answer)
                
                if results:
                    st.markdown("### ğŸ” ì°¸ì¡° ë¬¸ë‹¨")
                    for i, result in enumerate(results, 1):
                        with st.expander(f"{i}. [{result['std']}:{result['para_id']}] {result['title']} (p.{result.get('page', '?')}) - ì ìˆ˜: {result['score']:.3f}"):
                            st.write(result['text'])
