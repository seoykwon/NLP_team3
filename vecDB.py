import os
import json
import hashlib
import re
import time
import logging
from typing import List, Dict, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from abc import ABC, abstractmethod
import pandas as pd
import numpy as np
from tqdm import tqdm

from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import tiktoken

# ìµœì í™”ëœ ì²­í¬ í¬ê¸° ì„¤ì •
OPTIMAL_CHUNK_SIZES = {
    'annotation': 400,      # ê¸°ì¡´ 1200 â†’ 400 (ì£¼ì„ ë°ì´í„°)
    'financial_table': 600, # ê¸°ì¡´ 2000 â†’ 600 (ì¬ë¬´ì œí‘œ)  
    'accounting_standard': 350  # ê¸°ì¡´ 1000 â†’ 350 (íšŒê³„ê¸°ì¤€ì„œ)
}

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EnhancedChunkMetadata:
    """RAG ìµœì í™”ëœ ë©”íƒ€ë°ì´í„°"""
    document_type: str
    year: int
    section: str
    chunk_index: int
    source_file: str
    char_start: int
    char_end: int
    word_count: int
    content_type: str
    
    # ì¬ë¬´ì œí‘œ íŠ¹í™” í•„ë“œ
    financial_items: Optional[List[str]] = None
    account_codes: Optional[List[str]] = None
    amounts: Optional[Dict[str, str]] = None  # ë¬¸ìì—´ë¡œ ì €ì¥ (í¬ë§· ë³´ì¡´)
    table_type: Optional[str] = None  # ì¬ë¬´ìƒíƒœí‘œ, ì†ìµê³„ì‚°ì„œ ë“±
    
    # ì£¼ì„ íŠ¹í™” í•„ë“œ
    note_number: Optional[str] = None
    related_financial_items: Optional[List[str]] = None
    risk_keywords: Optional[List[str]] = None
    
    # íšŒê³„ê¸°ì¤€ íŠ¹í™” í•„ë“œ
    standard_number: Optional[str] = None
    paragraph_id: Optional[str] = None
    regulation_type: Optional[str] = None
    
    # êµì°¨ ì°¸ì¡° í•„ë“œ (ê°œì„ ë¨)
    cross_references: Optional[List[str]] = None
    entity_mentions: Optional[List[str]] = None
    temporal_references: Optional[List[int]] = None
    confidence_score: float = 1.0
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        # ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        for key, value in result.items():
            if isinstance(value, list) and value:
                result[key] = ', '.join(str(v) for v in value)
            elif isinstance(value, list):
                result[key] = None
            elif isinstance(value, dict) and value:
                # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                result[key] = ', '.join(f"{k}: {v}" for k, v in value.items())
            elif isinstance(value, dict):
                result[key] = None
        return {k: v for k, v in result.items() if v is not None}

class BaseDocumentProcessor(ABC):
    """ê°œì„ ëœ ê¸°ë³¸ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, max_tokens: int = 1000, overlap_tokens: int = 100):
        self.max_tokens = max_tokens  # RAGë¥¼ ìœ„í•´ ë” í° ì²­í¬
        self.overlap_tokens = overlap_tokens
        self.encoding = tiktoken.get_encoding("cl100k_base")
    
    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))
    
    @abstractmethod
    def process_file(self, file_path: str, year: int) -> List[Tuple[str, EnhancedChunkMetadata]]:
        pass
    
    def extract_financial_entities(self, text: str) -> Dict[str, List[str]]:
        """ê°•í™”ëœ ê¸ˆìœµ ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = {
            'financial_items': [],
            'amounts': [],
            'companies': [],
            'dates': [],
            'ratios': [],
            'accounting_items': []
        }
        
        # ë” ì •êµí•œ ê¸ˆìœµ í•­ëª© íŒ¨í„´
        financial_patterns = [
            # ì†ìµê³„ì‚°ì„œ í•­ëª©
            r'ë§¤ì¶œì•¡', r'ë§¤ì¶œìˆ˜ìµ', r'ì˜ì—…ìˆ˜ìµ', r'ì˜ì—…ì´ìµ', r'ì˜ì—…ì†ìµ', r'ë‹¹ê¸°ìˆœì´ìµ', r'ë‹¹ê¸°ìˆœì†ìµ',
            r'ë²•ì¸ì„¸ë¹„ìš©', r'ë²•ì¸ì„¸ì°¨ê°ì „ì´ìµ', r'ê¸ˆìœµìˆ˜ìµ', r'ê¸ˆìœµë¹„ìš©', r'ê¸°íƒ€ìˆ˜ìµ', r'ê¸°íƒ€ë¹„ìš©',
            r'ë§¤ì¶œì›ê°€', r'íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„', r'ì—°êµ¬ê°œë°œë¹„', r'ê°ê°€ìƒê°ë¹„', r'ëŒ€ì†ìƒê°ë¹„',
            
            # ì¬ë¬´ìƒíƒœí‘œ í•­ëª©
            r'ì´ìì‚°', r'ìì‚°ì´ê³„', r'ë¶€ì±„ì´ê³„', r'ìë³¸ì´ê³„', r'ìë³¸ê¸ˆ', r'ì´ìµì‰ì—¬ê¸ˆ',
            r'í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°', r'ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ', r'ë§¤ì¶œì±„ê¶Œ', r'ì¬ê³ ìì‚°', r'ìœ í˜•ìì‚°', r'ë¬´í˜•ìì‚°',
            r'íˆ¬ììì‚°', r'ìœ ë™ìì‚°', r'ë¹„ìœ ë™ìì‚°', r'ìœ ë™ë¶€ì±„', r'ë¹„ìœ ë™ë¶€ì±„', r'ì°¨ì…ê¸ˆ', r'ì‚¬ì±„',
            
            # í˜„ê¸ˆíë¦„í‘œ í•­ëª©
            r'ì˜ì—…í™œë™í˜„ê¸ˆíë¦„', r'íˆ¬ìí™œë™í˜„ê¸ˆíë¦„', r'ì¬ë¬´í™œë™í˜„ê¸ˆíë¦„', r'í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ì˜ì¦ê°',
            
            # ê¸°íƒ€ ê¸ˆìœµ í•­ëª©
            r'ìê¸°ìë³¸', r'ì£¼ì£¼ì§€ë¶„', r'ì§€ë°°ê¸°ì—…ì†Œìœ ì£¼ì§€ë¶„', r'ë¹„ì§€ë°°ì§€ë¶„', r'ì‹ ìš©ì†ì‹¤ì¶©ë‹¹ê¸ˆ',
            r'ë¦¬ìŠ¤ìì‚°', r'ë¦¬ìŠ¤ë¶€ì±„', r'í™•ì •ê¸‰ì—¬ë¶€ì±„', r'ì¶©ë‹¹ë¶€ì±„', r'ìš°ë°œë¶€ì±„'
        ]
        
        for pattern in financial_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                entities['financial_items'].append(pattern)
        
        # íšŒê³„ ê¸°ì¤€ ë° ì •ì±… í•­ëª©
        accounting_patterns = [
            r'ìˆ˜ìµì¸ì‹', r'ê°ê°€ìƒê°', r'ì†ìƒì°¨ì†', r'ì¶©ë‹¹ë¶€ì±„', r'ê¸ˆìœµìƒí’ˆ', r'ë¦¬ìŠ¤', r'ì¢…ì—…ì›ê¸‰ì—¬',
            r'ë²•ì¸ì„¸', r'ì™¸í™”í™˜ì‚°', r'íŒŒìƒìƒí’ˆ', r'ê³µì •ê°€ì¹˜', r'ìƒê°í›„ì›ê°€', r'í˜„ì¬ê°€ì¹˜',
            r'K-IFRS', r'IFRS', r'íšŒê³„ì •ì±…', r'íšŒê³„ì¶”ì •', r'íšŒê³„ë³€ê²½'
        ]
        
        for pattern in accounting_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                entities['accounting_items'].append(pattern)
        
        # ê°œì„ ëœ ê¸ˆì•¡ ì¶”ì¶œ (ë°±ë§Œì›, ì²œì› ë‹¨ìœ„ ì¶”ê°€)
        amount_patterns = [
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*ì¡°\s*ì›',        # ì¡°ì›
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*ì–µ\s*ì›',        # ì–µì›
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*ë°±ë§Œ\s*ì›',      # ë°±ë§Œì›
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*ì²œë§Œ\s*ì›',      # ì²œë§Œì›
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*ì²œ\s*ì›',        # ì²œì›
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*ì›',            # ì›
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*[ì¡°ì–µë°±ë§Œì²œ]+',  # ë‹¨ìœ„ë§Œ
            r'\d+(?:,\d{3})*(?:\.\d+)?ë°±ë§Œ',             # ë°±ë§Œ (ì› ìƒëµ)
            r'\d+(?:,\d{3})*(?:\.\d+)?ì²œë§Œ',             # ì²œë§Œ (ì› ìƒëµ)
            r'\d+(?:,\d{3})*(?:\.\d+)?ì²œ',               # ì²œ (ì› ìƒëµ)
        ]
        
        for pattern in amount_patterns:
            matches = re.findall(pattern, text)
            entities['amounts'].extend(matches)
        
        # ë¹„ìœ¨ ë° ì§€í‘œ ì¶”ì¶œ
        ratio_patterns = [
            r'\d+(?:\.\d+)?%',                          # í¼ì„¼íŠ¸
            r'\d+(?:\.\d+)?\s*ë°°',                      # ë°°ìˆ˜
            r'\d+(?:\.\d+)?\s*íšŒ',                      # íšŒì „ìœ¨
            r'ë¶€ì±„ë¹„ìœ¨\s*:?\s*\d+(?:\.\d+)?%?',
            r'ìê¸°ìë³¸ë¹„ìœ¨\s*:?\s*\d+(?:\.\d+)?%?',
            r'ìœ ë™ë¹„ìœ¨\s*:?\s*\d+(?:\.\d+)?%?',
            r'ROE\s*:?\s*\d+(?:\.\d+)?%?',
            r'ROA\s*:?\s*\d+(?:\.\d+)?%?',
        ]
        
        for pattern in ratio_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            entities['ratios'].extend(matches)
        
        # íšŒì‚¬ëª… ì¶”ì¶œ (ë” ì •êµí•œ íŒ¨í„´)
        company_patterns = [
            r'ì‚¼ì„±ì „ì(?:ì£¼ì‹íšŒì‚¬)?',
            r'(?:ì£¼ì‹íšŒì‚¬\s*)?[ê°€-í£]{2,10}(?:ì „ì|ì „ê¸°|í™”í•™|ê±´ì„¤|ê¸ˆìœµ|ë³´í—˜|ì¦ê¶Œ|ì€í–‰)',
            r'[A-Z][a-zA-Z\s]{3,20}(?:Corp|Inc|Ltd|Co)',
        ]
        
        for pattern in company_patterns:
            matches = re.findall(pattern, text)
            entities['companies'].extend(matches)
        
        # ì—°ë„ ì¶”ì¶œ íŒ¨í„´ í™•ì¥ (2025ë…„ê¹Œì§€)
        year_patterns = [
            r'20(?:1[0-9]|2[0-5])ë…„?',     # 2010-2025ë…„
            r'(?:19|20)\d{2}ë…„ë„',          # ì—°ë„ í˜•íƒœ
            r'FY\s*20(?:1[0-9]|2[0-5])',   # íšŒê³„ì—°ë„
        ]
        
        all_years = []
        for pattern in year_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # ìˆ«ìë§Œ ì¶”ì¶œ
                year_num = re.findall(r'20(?:1[0-9]|2[0-5])', match)
                if year_num:
                    all_years.extend([int(y) for y in year_num])
        
        entities['dates'] = list(set(all_years))  # ì¤‘ë³µ ì œê±°
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        for key in entities:
            if isinstance(entities[key], list):
                entities[key] = list(set(entities[key]))  # ì¤‘ë³µ ì œê±°
                entities[key] = [item for item in entities[key] if str(item).strip()]  # ë¹ˆ ë¬¸ìì—´ ì œê±°
        
        return entities
    
    def extract_cross_references(self, text: str) -> Dict[str, List[str]]:
        """êµì°¨ ì°¸ì¡° ì¶”ì¶œ (ìƒˆë¡œìš´ ë©”ì„œë“œ)"""
        cross_refs = {
            'note_references': [],      # ì£¼ì„ ë²ˆí˜¸ ì°¸ì¡°
            'account_references': [],   # ê³„ì •ê³¼ëª© ì°¸ì¡°  
            'table_references': [],     # í‘œ ì°¸ì¡°
            'page_references': [],      # í˜ì´ì§€ ì°¸ì¡°
            'section_references': []    # ì„¹ì…˜ ì°¸ì¡°
        }
        
        # ì£¼ì„ ë²ˆí˜¸ ì°¸ì¡° ì¶”ì¶œ
        note_patterns = [
            r'ì£¼ì„\s*(\d+)',
            r'ê°ì£¼\s*(\d+)',
            r'Note\s*(\d+)',
            r'\(ì£¼\s*(\d+)\)',
            r'ì£¼\s*(\d+)\s*ì°¸ì¡°'
        ]
        
        for pattern in note_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            cross_refs['note_references'].extend(matches)
        
        # ê³„ì •ê³¼ëª© ì°¸ì¡° ì¶”ì¶œ (ë” í¬ê´„ì )
        account_patterns = [
            r'(ë§¤ì¶œì•¡|ì˜ì—…ì´ìµ|ë‹¹ê¸°ìˆœì´ìµ|ë²•ì¸ì„¸ë¹„ìš©|ê°ê°€ìƒê°ë¹„)',
            r'(ì´ìì‚°|ìœ ë™ìì‚°|ë¹„ìœ ë™ìì‚°|ìœ í˜•ìì‚°|ë¬´í˜•ìì‚°)',
            r'(ë¶€ì±„ì´ê³„|ìœ ë™ë¶€ì±„|ë¹„ìœ ë™ë¶€ì±„|ì°¨ì…ê¸ˆ|ì‚¬ì±„)',
            r'(ìë³¸ì´ê³„|ìë³¸ê¸ˆ|ì´ìµì‰ì—¬ê¸ˆ|ê¸°íƒ€í¬ê´„ì†ìµëˆ„ê³„ì•¡)',
            r'(í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°|ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ|ë§¤ì¶œì±„ê¶Œ|ì¬ê³ ìì‚°)',
            r'(ì˜ì—…í™œë™í˜„ê¸ˆíë¦„|íˆ¬ìí™œë™í˜„ê¸ˆíë¦„|ì¬ë¬´í™œë™í˜„ê¸ˆíë¦„)'
        ]
        
        for pattern in account_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            cross_refs['account_references'].extend([match for match in matches if match])
        
        # í‘œ ì°¸ì¡° ì¶”ì¶œ
        table_patterns = [
            r'í‘œ\s*(\d+)',
            r'Table\s*(\d+)',
            r'<í‘œ\s*(\d+)>',
            r'\[í‘œ\s*(\d+)\]',
            r'ìƒê¸°\s*í‘œ',
            r'ì•„ë˜\s*í‘œ',
            r'ìœ„\s*í‘œ'
        ]
        
        for pattern in table_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if isinstance(matches[0] if matches else None, tuple):
                cross_refs['table_references'].extend([match[0] for match in matches])
            else:
                cross_refs['table_references'].extend(matches)
        
        # í˜ì´ì§€ ì°¸ì¡° ì¶”ì¶œ
        page_patterns = [
            r'(\d+)\s*í˜ì´ì§€',
            r'p\.\s*(\d+)',
            r'page\s*(\d+)',
            r'(\d+)\s*ìª½'
        ]
        
        for pattern in page_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            cross_refs['page_references'].extend(matches)
        
        # ì„¹ì…˜ ì°¸ì¡° ì¶”ì¶œ
        section_patterns = [
            r'(\d+)\.\s*([ê°€-í£\w\s]{2,20})',  # "1. ì¼ë°˜ì‚¬í•­" í˜•íƒœ
            r'\(([ê°€-í£])\)\s*([ê°€-í£\w\s]{2,20})',  # "(ê°€) ê°œìš”" í˜•íƒœ
            r'ì œ\s*(\d+)\s*ì ˆ',                # "ì œ1ì ˆ" í˜•íƒœ
            r'([ê°€-í£]{2,10})\s*ê´€ë ¨',          # "íšŒê³„ì •ì±… ê´€ë ¨" í˜•íƒœ
        ]
        
        for pattern in section_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    cross_refs['section_references'].append(' '.join(match))
                else:
                    cross_refs['section_references'].append(match)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        for key in cross_refs:
            cross_refs[key] = list(set(cross_refs[key]))  # ì¤‘ë³µ ì œê±°
            cross_refs[key] = [item for item in cross_refs[key] if item and str(item).strip()]  # ë¹ˆ ê°’ ì œê±°
        
        return cross_refs

class AnnotationProcessor(BaseDocumentProcessor):
    """ì£¼ì„ ì²˜ë¦¬ê¸° - ê°œì„ ë¨"""
    
    def __init__(self):
        super().__init__(max_tokens=400, overlap_tokens=150)
    
    def process_file(self, file_path: str, year: int) -> List[Tuple[str, EnhancedChunkMetadata]]:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        chunks = []
        
        if isinstance(data, list):
            for item in data:
                if 'content' in item and str(item['content']).strip():
                    note_chunks = self._process_note_item(item, year, file_path)
                    chunks.extend(note_chunks)
        
        return chunks
    
    def _process_note_item(self, item: Dict, year: int, file_path: str) -> List[Tuple[str, EnhancedChunkMetadata]]:
        """ê¸°ì¡´ íŒŒì‹± ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•œ ì£¼ì„ ì²˜ë¦¬"""
        content = str(item['content'])  # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
        
        # ê¸°ì¡´ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„° í™œìš© (ChatGPTê°€ ë†“ì¹œ ë¶€ë¶„)
        note_number = item.get('note_number', 'unknown')
        title = item.get('title', f'ì£¼ì„ {note_number}')
        
        # ì „ì²´ íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ëŠ” ë³„ë„ë¡œ ë°›ì•„ì•¼ í•¨
        file_metadata = item.get('metadata', {})  # íŒŒì¼ ë ˆë²¨ ë©”íƒ€ë°ì´í„°
        
        # ì£¼ì œë³„ ë¶„í•  (ë” í° ì˜ë¯¸ ë‹¨ìœ„ë¡œ)
        sections = self._split_by_semantic_units(content)
        chunks = []
        
        for i, section in enumerate(sections):
            section = str(section)  # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
            if not section.strip() or self.count_tokens(section) < 100:
                continue
            
            # ì œëª©ì„ í¬í•¨í•œ ì™„ì „í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            full_content = f"[ì£¼ì„ {note_number}: {title}]\n\n{section}"
            
            # í† í° ìˆ˜ê°€ ê³¼ë„í•˜ê²Œ í´ ë•Œë§Œ ë¶„í• 
            if self.count_tokens(full_content) > self.max_tokens:
                sub_chunks = self._smart_split(full_content, title, note_number)
                for sub_chunk in sub_chunks:
                    metadata = self._create_note_metadata(
                        sub_chunk, 
                        file_metadata,  # íŒŒì¼ ë ˆë²¨ ë©”íƒ€ë°ì´í„°
                        item,          # ì•„ì´í…œ ë ˆë²¨ ë©”íƒ€ë°ì´í„° (note_number, title í¬í•¨)
                        len(chunks)
                    )
                    chunks.append((sub_chunk, metadata))
            else:
                metadata = self._create_note_metadata(
                    full_content, 
                    file_metadata,  # íŒŒì¼ ë ˆë²¨ ë©”íƒ€ë°ì´í„°
                    item,          # ì•„ì´í…œ ë ˆë²¨ ë©”íƒ€ë°ì´í„° (note_number, title í¬í•¨)
                    len(chunks)
                )
                chunks.append((full_content, metadata))
        
        return chunks
    
    def _split_by_semantic_units(self, content: str) -> List[str]:
        """ì˜ë¯¸ ë‹¨ìœ„ ê¸°ë°˜ ë¶„í• """
        # ë” í° ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
        patterns = [
            r'(?=\d+\.\s*[ê°€-í£]+)',  # "1. ì¼ë°˜ì‚¬í•­"
            r'(?=\([ê°€-í£]\)\s*[ê°€-í£]+)',  # "(ê°€) ì‹ ìš©ìœ„í—˜"
        ]
        
        sections = [content]
        for pattern in patterns[:1]:  # ì²« ë²ˆì§¸ íŒ¨í„´ë§Œ ì‚¬ìš©í•´ ë” í° ì²­í¬ ìœ ì§€
            new_sections = []
            for section in sections:
                splits = re.split(pattern, section)
                new_sections.extend([str(s).strip() for s in splits if str(s).strip()])
            sections = new_sections
        
        return sections
    
    def _smart_split(self, content: str, title: str, note_number: str) -> List[str]:
        """ìŠ¤ë§ˆíŠ¸ ë¶„í•  - ë¬¸ë§¥ ë³´ì¡´ (ê°œì„ ëœ í•œêµ­ì–´ ë¬¸ì¥ ë¶„í• )"""
        # ê°œì„ ëœ í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©
        korean_sentence_patterns = [
            r'(?<=[ë‹¤ìŒìš”ìŒìŠµë‹ˆë‹¤ë©ë‹ˆë‹¤])\.(?=\s)',  # ê¸°ë³¸ ì¢…ê²°ì–´ë¯¸ + ìŠµë‹ˆë‹¤, ë©ë‹ˆë‹¤
            r'(?<=ë‹ˆë‹¤)\.(?=\s)',                    # ~ë‹ˆë‹¤
            r'(?<=ì…ë‹ˆë‹¤)\.(?=\s)',                  # ì…ë‹ˆë‹¤
            r'(?<=ì—ˆìŠµë‹ˆë‹¤)\.(?=\s)',                # ì—ˆìŠµë‹ˆë‹¤
            r'(?<=ì•˜ìŠµë‹ˆë‹¤)\.(?=\s)',                # ì•˜ìŠµë‹ˆë‹¤
            r'(?<=[ë‹¤ìŒìš”ìŒ])\.(?=\s)',              # ê¸°ì¡´ íŒ¨í„´ (ë°±ì—…)
        ]
        
        # ì²« ë²ˆì§¸ íŒ¨í„´ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ë¶„í•  ì‹œë„
        sentences = [content]
        for pattern in korean_sentence_patterns:
            new_sentences = []
            for sentence in sentences:
                split_result = re.split(pattern, sentence)
                if len(split_result) > 1:  # ë¶„í• ì´ ì¼ì–´ë‚¬ë‹¤ë©´
                    new_sentences.extend([str(s) for s in split_result if str(s).strip()])
                else:
                    new_sentences.append(sentence)
            sentences = new_sentences
            
            # ë¶„í• ì´ ì¶©ë¶„íˆ ë˜ì—ˆìœ¼ë©´ ì¤‘ë‹¨ (ë„ˆë¬´ ë§ì€ ë¶„í•  ë°©ì§€)
            if len(sentences) >= 3:
                break
        
        chunks = []
        current_chunk = f"[ì£¼ì„ {note_number}: {title}]\n\n"
        header_length = len(current_chunk)
        
        for sentence in sentences:
            sentence = str(sentence)  # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
            if not sentence.strip():
                continue
                
            test_chunk = current_chunk + sentence + "."
            
            if self.count_tokens(test_chunk) <= self.max_tokens:
                current_chunk = test_chunk
            else:
                if len(current_chunk) > header_length + 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    chunks.append(current_chunk.strip())
                
                # ìƒˆ ì²­í¬ ì‹œì‘ (í—¤ë” + ê²¹ì¹˜ëŠ” ë¶€ë¶„)
                overlap = current_chunk.split('.')[-2:] if '.' in current_chunk else []
                overlap_text = '. '.join(overlap) + '. ' if overlap else ''
                current_chunk = f"[ì£¼ì„ {note_number}: {title}]\n\n{overlap_text}{sentence}."
        
        if len(current_chunk) > header_length + 50:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _create_note_metadata(self, chunk: str, file_metadata: Dict, item_metadata: Dict, chunk_index: int) -> EnhancedChunkMetadata:
        """ê°•í™”ëœ ë©”íƒ€ë°ì´í„° ìƒì„± (êµì°¨ ì°¸ì¡° í¬í•¨)"""
        # ê°•í™”ëœ ê¸ˆìœµ ì—”í‹°í‹° ì¶”ì¶œ
        entities = self.extract_financial_entities(chunk)
        
        # ìƒˆë¡œìš´ êµì°¨ ì°¸ì¡° ì¶”ì¶œ
        cross_references = self.extract_cross_references(chunk)
        
        # í†µí•©ëœ êµì°¨ ì°¸ì¡° ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
        cross_refs = []
        
        # ì¬ë¬´ì œí‘œ í•­ëª© ì–¸ê¸‰
        for item in entities['financial_items']:
            cross_refs.append(f"financial_item:{item}")
        
        # íšŒê³„ í•­ëª© ì–¸ê¸‰
        for item in entities.get('accounting_items', []):
            cross_refs.append(f"accounting_item:{item}")
        
        # ì£¼ì„ ì°¸ì¡°
        for note_ref in cross_references['note_references']:
            cross_refs.append(f"note_ref:{note_ref}")
        
        # í‘œ ì°¸ì¡°
        for table_ref in cross_references['table_references']:
            cross_refs.append(f"table_ref:{table_ref}")
        
        # ê³„ì •ê³¼ëª© ì°¸ì¡°
        for account_ref in cross_references['account_references']:
            cross_refs.append(f"account_ref:{account_ref}")
        
        # ì„¹ì…˜ ì°¸ì¡°
        for section_ref in cross_references['section_references']:
            cross_refs.append(f"section_ref:{section_ref}")
        
        # í˜ì´ì§€ ì°¸ì¡°
        for page_ref in cross_references['page_references']:
            cross_refs.append(f"page_ref:{page_ref}")
        
        # ìœ„í—˜ í‚¤ì›Œë“œ
        risk_keywords = []
        risk_terms = ['ìœ„í—˜', 'ë¶ˆí™•ì‹¤ì„±', 'ì†ìƒ', 'ì—°ì²´', 'ì†Œì†¡', 'ìš°ë°œ', 'ì½”ë¡œë‚˜', 'COVID']
        for term in risk_terms:
            if term in chunk:
                risk_keywords.append(term)
        
        return EnhancedChunkMetadata(
            document_type='annotation',
            year=file_metadata.get('file_year'),  # íŒŒì¼ ë©”íƒ€ë°ì´í„°ì—ì„œ
            section=item_metadata.get('title', 'Unknown'),  # ì•„ì´í…œ ë©”íƒ€ë°ì´í„°ì—ì„œ
            chunk_index=chunk_index,
            source_file=file_metadata.get('source_file', 'unknown'),
            char_start=0,
            char_end=len(chunk),
            word_count=len(chunk.split()),
            content_type='annotation',
            note_number=item_metadata.get('note_number'),  # ì•„ì´í…œ ë©”íƒ€ë°ì´í„°ì—ì„œ
            # ê°•í™”ëœ ê¸ˆìœµ ì—”í‹°í‹° ì •ë³´
            related_financial_items=entities['financial_items'] + entities.get('accounting_items', []),
            risk_keywords=risk_keywords if risk_keywords else None,
            cross_references=cross_refs if cross_refs else None,
            entity_mentions=entities['amounts'] + entities.get('ratios', []),  # ê¸ˆì•¡ê³¼ ë¹„ìœ¨ í¬í•¨
            temporal_references=entities['dates'] if entities['dates'] else None
        )

class FinancialTableProcessor(BaseDocumentProcessor):
    """ì¬ë¬´ì œí‘œ ì²˜ë¦¬ê¸° - í‘œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬"""
    
    def __init__(self):
        super().__init__(max_tokens=600, overlap_tokens=0)  # í‘œëŠ” ê²¹ì¹˜ì§€ ì•ŠìŒ
    
    def process_file(self, file_path: str, year: int) -> List[Tuple[str, EnhancedChunkMetadata]]:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        chunks = []
        
        # ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •: dict í˜•íƒœì—ì„œ 'tables' í‚¤ í™•ì¸
        if isinstance(data, dict) and 'tables' in data:
            tables = data['tables']
            for item in tables:
                if item.get('role') == 'data' and 'rows' in item:
                    table_chunk = self._process_complete_table(item, year, file_path, len(chunks))
                    if table_chunk:
                        chunks.append(table_chunk)
        elif isinstance(data, list):
            # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë„ ì§€ì›
            for item in data:
                if item.get('role') == 'data' and 'rows' in item:
                    table_chunk = self._process_complete_table(item, year, file_path, len(chunks))
                    if table_chunk:
                        chunks.append(table_chunk)
        
        return chunks
    
    def _process_complete_table(self, table_data: Dict, year: int, file_path: str, 
                               table_index: int) -> Optional[Tuple[str, EnhancedChunkMetadata]]:
        """í‘œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬"""
        title_lines = table_data.get('title_lines', [])
        columns = table_data.get('columns', [])
        rows = table_data.get('rows', [])
        
        if not rows:
            return None
        
        # í‘œ ì œëª© ê²°ì •
        table_title = ' | '.join(title_lines) if title_lines else 'ì¬ë¬´ì œí‘œ'
        table_type = self._identify_table_type(table_title, rows)
        
        # í‘œ ì „ì²´ë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        table_text = self._create_complete_table_text(table_title, columns, rows, year)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = self._create_table_metadata(
            table_text, table_data, year, file_path, table_title, table_type, table_index
        )
        
        return (table_text, metadata)
    
    def _identify_table_type(self, title: str, rows: List[Dict]) -> str:
        """í‘œ ìœ í˜• ì‹ë³„"""
        title_lower = title.lower()
        
        if 'ì¬ë¬´ìƒíƒœí‘œ' in title or 'ëŒ€ì°¨ëŒ€ì¡°í‘œ' in title:
            return 'ì¬ë¬´ìƒíƒœí‘œ'
        elif 'ì†ìµê³„ì‚°ì„œ' in title or 'í¬ê´„ì†ìµê³„ì‚°ì„œ' in title:
            return 'ì†ìµê³„ì‚°ì„œ'
        elif 'í˜„ê¸ˆíë¦„í‘œ' in title:
            return 'í˜„ê¸ˆíë¦„í‘œ'
        elif 'ìë³¸ë³€ë™í‘œ' in title:
            return 'ìë³¸ë³€ë™í‘œ'
        else:
            # ê³„ì •ê³¼ëª©ìœ¼ë¡œ ì¶”ì •
            account_names = [row.get('ê³¼ëª©', '') for row in rows[:5]]
            account_text = ' '.join(account_names)
            
            if any(keyword in account_text for keyword in ['ìì‚°', 'ë¶€ì±„', 'ìë³¸']):
                return 'ì¬ë¬´ìƒíƒœí‘œ'
            elif any(keyword in account_text for keyword in ['ë§¤ì¶œ', 'ë¹„ìš©', 'ì´ìµ']):
                return 'ì†ìµê³„ì‚°ì„œ'
            else:
                return 'ê¸°íƒ€ì¬ë¬´ì œí‘œ'
    
    def _create_complete_table_text(self, title: str, columns: List[str], 
                                   rows: List[Dict], year: int) -> str:
        """í‘œ ì „ì²´ë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        parts.append(f"[{year}ë…„ë„ {title}]")
        parts.append("")
        
        # ì»¬ëŸ¼ í—¤ë”
        if columns:
            header = " | ".join(columns)
            parts.append(f"êµ¬ì¡°: {header}")
            parts.append("")
        
        # ì£¼ìš” ê³„ì •ê³¼ëª©ê³¼ ê¸ˆì•¡
        parts.append("ì£¼ìš” í•­ëª©:")
        for row in rows:
            account = row.get('ê³¼ëª©', '').strip()
            if not account:
                continue
            
            # ê¸ˆì•¡ ì •ë³´ ì¶”ì¶œ
            amounts = []
            for col in columns:
                if col in ['ê³¼ëª©', 'ì£¼ì„']:
                    continue
                value = row.get(col, '').strip()
                if value and value != '-':
                    amounts.append(f"{col}: {value}")
            
            if amounts:
                amount_text = " | ".join(amounts)
                parts.append(f"â€¢ {account}: {amount_text}")
        
        # í…Œì´ë¸” ìš”ì•½
        parts.append("")
        parts.append("í…Œì´ë¸” íŠ¹ì„±:")
        parts.append(f"â€¢ ì´ {len(rows)}ê°œ ê³„ì •ê³¼ëª©")
        parts.append(f"â€¢ ê¸°ì¤€ì—°ë„: {year}")
        
        # ì£¼ìš” ê¸ˆìœµì§€í‘œ ìš”ì•½
        major_items = self._extract_major_financial_items(rows)
        if major_items:
            parts.append("â€¢ ì£¼ìš” ì§€í‘œ: " + ", ".join(major_items))
        
        return "\n".join(parts)
    
    def _extract_major_financial_items(self, rows: List[Dict]) -> List[str]:
        """ì£¼ìš” ê¸ˆìœµ ì§€í‘œ ì¶”ì¶œ"""
        major_items = []
        key_accounts = ['ë§¤ì¶œì•¡', 'ì˜ì—…ì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ', 'ì´ìì‚°', 'ë¶€ì±„ì´ê³„', 'ìë³¸ì´ê³„']
        
        for row in rows:
            account = row.get('ê³¼ëª©', '')
            for key_account in key_accounts:
                if key_account in account:
                    major_items.append(account)
                    break
        
        return major_items[:10]  # ìµœëŒ€ 10ê°œ
    
    def _create_table_metadata(self, text: str, table_data: Dict, year: int, 
                              file_path: str, table_title: str, table_type: str, 
                              table_index: int) -> EnhancedChunkMetadata:
        """í‘œ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        rows = table_data.get('rows', [])
        
        # ëª¨ë“  ê³„ì •ê³¼ëª© ì¶”ì¶œ
        account_codes = []
        financial_items = []
        amounts = {}
        
        for row in rows:
            account = row.get('ê³¼ëª©', '').strip()
            if account:
                account_codes.append(account)
                financial_items.extend(self.extract_financial_entities(account)['financial_items'])
            
            # ê¸ˆì•¡ ì •ë³´ ì €ì¥ (ë¬¸ìì—´ë¡œ ë³´ì¡´)
            for key, value in row.items():
                if key not in ['ê³¼ëª©', 'ì£¼ì„'] and value and str(value).strip() and str(value) != '-':
                    amounts[f"{account}_{key}"] = str(value).strip()
        
        # êµì°¨ ì°¸ì¡° ìƒì„±
        cross_refs = []
        for item in financial_items:
            cross_refs.append(f"annotation:{item}")
        
        return EnhancedChunkMetadata(
            document_type='financial_table',
            year=year,
            section=table_title,
            chunk_index=table_index,
            source_file=file_path,
            char_start=0,
            char_end=len(text),
            word_count=len(text.split()),
            content_type='complete_table',
            financial_items=list(set(financial_items)) if financial_items else None,
            account_codes=account_codes if account_codes else None,
            amounts=amounts if amounts else None,
            table_type=table_type,
            cross_references=cross_refs if cross_refs else None
        )

class AccountingStandardProcessor(BaseDocumentProcessor):
    """íšŒê³„ê¸°ì¤€ì„œ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        super().__init__(max_tokens=350, overlap_tokens=100)
    
    def process_file(self, file_path: str, year: int) -> List[Tuple[str, EnhancedChunkMetadata]]:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        chunks = []
        
        if 'documents' in data:
            for doc in data['documents']:
                doc_chunks = self._process_standard_document(doc, year, file_path)
                chunks.extend(doc_chunks)
        
        return chunks
    
    def _process_standard_document(self, doc: Dict, year: int, file_path: str) -> List[Tuple[str, EnhancedChunkMetadata]]:
        standard_no = doc.get('standard_no', 'unknown')
        title = doc.get('title', 'íšŒê³„ê¸°ì¤€')
        paragraphs = doc.get('paragraphs', [])
        
        chunks = []
        
        # ë¬¸ë‹¨ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘
        grouped_paragraphs = self._group_related_paragraphs(paragraphs)
        
        for group in grouped_paragraphs:
            group_text = self._create_group_text(group, standard_no, title)
            
            if self.count_tokens(group_text) > self.max_tokens:
                sub_chunks = self._split_group(group_text, standard_no, title)
                for sub_chunk in sub_chunks:
                    metadata = self._create_standard_metadata(
                        sub_chunk, doc, group[0], year, file_path, len(chunks)
                    )
                    chunks.append((sub_chunk, metadata))
            else:
                metadata = self._create_standard_metadata(
                    group_text, doc, group[0], year, file_path, len(chunks)
                )
                chunks.append((group_text, metadata))
        
        return chunks
    
    def _group_related_paragraphs(self, paragraphs: List[Dict]) -> List[List[Dict]]:
        """ê´€ë ¨ ë¬¸ë‹¨ ê·¸ë£¹í•‘"""
        if not paragraphs:
            return []
        
        groups = []
        current_group = [paragraphs[0]]
        
        for para in paragraphs[1:]:
            current_text = ' '.join([p.get('text', '') for p in current_group])
            new_text = para.get('text', '')
            
            if self.count_tokens(current_text + new_text) <= self.max_tokens:
                current_group.append(para)
            else:
                groups.append(current_group)
                current_group = [para]
        
        if current_group:
            groups.append(current_group)
        
        return groups
    
    def _create_group_text(self, group: List[Dict], standard_no: str, title: str) -> str:
        """ê·¸ë£¹ í…ìŠ¤íŠ¸ ìƒì„±"""
        parts = [f"[K-IFRS {standard_no}: {title}]", ""]
        
        for para in group:
            para_id = para.get('para_id', '')
            text = para.get('text', '').strip()
            if text:
                if para_id:
                    parts.append(f"ë¬¸ë‹¨ {para_id}: {text}")
                else:
                    parts.append(text)
                parts.append("")
        
        return "\n".join(parts).strip()
    
    def _split_group(self, text: str, standard_no: str, title: str) -> List[str]:
        """ê·¸ë£¹ ë¶„í• """
        sentences = re.split(r'(?<=[ë‹¤ìŒìš”ìŒ])\.(?=\s)', text)
        
        chunks = []
        header = f"[K-IFRS {standard_no}: {title}]\n\n"
        current_chunk = header
        
        for sentence in sentences:
            if not sentence.strip():
                continue
            
            test_chunk = current_chunk + sentence + "."
            
            if self.count_tokens(test_chunk) <= self.max_tokens:
                current_chunk = test_chunk
            else:
                if len(current_chunk) > len(header) + 50:
                    chunks.append(current_chunk.strip())
                
                overlap = current_chunk.split('.')[-1:] if '.' in current_chunk else []
                overlap_text = '. '.join(overlap) + '. ' if overlap else ''
                current_chunk = header + overlap_text + sentence + "."
        
        if len(current_chunk) > len(header) + 50:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _create_standard_metadata(self, text: str, doc: Dict, para: Dict, 
                                 year: int, file_path: str, chunk_index: int) -> EnhancedChunkMetadata:
        standard_no = doc.get('standard_no', 'unknown')
        title = doc.get('title', 'íšŒê³„ê¸°ì¤€')
        para_id = para.get('para_id', 'unknown')
        
        return EnhancedChunkMetadata(
            document_type='accounting_standard',
            year=year,
            section=f"K-IFRS {standard_no}: {title}",
            chunk_index=chunk_index,
            source_file=file_path,
            char_start=0,
            char_end=len(text),
            word_count=len(text.split()),
            content_type='regulation',
            standard_number=standard_no,
            paragraph_id=para_id,
            regulation_type='K-IFRS'
        )

class ImprovedVectorPipeline:
    """ê°œì„ ëœ ë²¡í„° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, db_path: str = "./chroma_samsung_rag", batch_size: int = 30, force_reset: bool = False):
        self.db_path = db_path
        self.batch_size = batch_size
        self.force_reset = force_reset
        self._load_embedding_model()
        self._init_chroma_collections(force_reset)
        
        self.processors = {
            'annotation': AnnotationProcessor(),
            'financial_table': FinancialTableProcessor(),
            'accounting_standard': AccountingStandardProcessor()
        }
        
        self.seen_hashes = set()
        self.stats = {'processed': 0, 'duplicates': 0, 'errors': 0}
    
    def _load_embedding_model(self):
        model_candidates = [
            "jhgan/ko-sroberta-multitask",
            "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
            "jhgan/ko-sbert-multitask"
        ]
        
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        for model_name in model_candidates:
            try:
                self.model = SentenceTransformer(model_name)
                logger.info(f"âœ“ {model_name} ë¡œë”© ì™„ë£Œ")
                break
            except Exception as e:
                logger.warning(f"âœ— {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
        else:
            raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def safe_collection_reset(self, collection_name: str, force: bool = False) -> bool:
        """ì•ˆì „í•œ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            existing_collection = None
            try:
                existing_collection = self.client.get_collection(collection_name)
                collection_count = existing_collection.count()
            except:
                collection_count = 0
                
            if collection_count > 0 and not force:
                logger.warning(f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}'ì— {collection_count}ê°œì˜ ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.")
                
                # ë°±ì—… ìƒì„± ì œì•ˆ
                backup_path = f"{self.db_path}_backup_{collection_name}_{int(time.time())}"
                logger.info(f"ğŸ’¾ ë°±ì—… ê²½ë¡œ: {backup_path}")
                
                # ëŒ€í™”í˜• í™•ì¸ (í”„ë¡œê·¸ë˜ë° í™˜ê²½ì—ì„œëŠ” ìë™ìœ¼ë¡œ ê±´ë„ˆë›°ê¸°)
                try:
                    import sys
                    if sys.stdin.isatty():  # ëŒ€í™”í˜• í„°ë¯¸ë„ì¸ ê²½ìš°ë§Œ
                        response = input(f"ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip()
                        if response not in ['y', 'yes']:
                            logger.info(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                            return False
                    else:
                        # ë¹„ëŒ€í™”í˜• í™˜ê²½ì—ì„œëŠ” ê¸°ì¡´ ë°ì´í„° ë³´ì¡´
                        logger.info(f"âœ… ë¹„ëŒ€í™”í˜• í™˜ê²½: ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ë°ì´í„°ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.")
                        return False
                except:
                    # ì…ë ¥ ë¶ˆê°€ëŠ¥í•œ í™˜ê²½ì—ì„œëŠ” ê¸°ì¡´ ë°ì´í„° ë³´ì¡´
                    logger.info(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ë°ì´í„°ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.")
                    return False
            
            # ì»¬ë ‰ì…˜ ì‚­ì œ ë° ì¬ìƒì„±
            if existing_collection:
                logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œ ì¤‘...")
                self.client.delete_collection(collection_name)
                
            logger.info(f"ğŸ”„ ì»¬ë ‰ì…˜ '{collection_name}' ì¬ìƒì„± ì¤‘...")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ '{collection_name}' ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _create_backup(self, collection_name: str, backup_path: str) -> bool:
        """ì»¬ë ‰ì…˜ ë°±ì—… ìƒì„±"""
        try:
            import shutil
            if os.path.exists(self.db_path):
                # ì „ì²´ DB í´ë” ë°±ì—… (íŠ¹ì • ì»¬ë ‰ì…˜ë§Œ ë°±ì—…í•˜ëŠ” ê²ƒì€ ChromaDBì—ì„œ ë³µì¡í•¨)
                shutil.copytree(self.db_path, backup_path)
                logger.info(f"âœ… ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_path}")
                return True
        except Exception as e:
            logger.error(f"âŒ ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def _init_chroma_collections(self, force_reset: bool = False):
        """ì•ˆì „í•œ ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        import shutil
        
        # ì „ì²´ DB í´ë” ì‚­ì œëŠ” force_resetì´ Trueì¼ ë•Œë§Œ
        if force_reset and os.path.exists(self.db_path):
            logger.warning(f"ğŸ—‘ï¸ force_reset=True: ì „ì²´ DB í´ë” ì‚­ì œ ì¤‘... ({self.db_path})")
            try:
                shutil.rmtree(self.db_path)
                logger.info(f"âœ… DB í´ë” ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ DB í´ë” ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path=self.db_path,
            settings=Settings(anonymized_telemetry=False, allow_reset=True)
        )
        
        self.collections = {}
        configs = {
            'annotation': 'samsung_annotations_rag',
            'financial_table': 'samsung_financial_rag', 
            'accounting_standard': 'samsung_accounting_rag'
        }
        
        # ê° ì»¬ë ‰ì…˜ì„ ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”
        for doc_type, collection_name in configs.items():
            try:
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ ë° ì•ˆì „í•œ ì²˜ë¦¬
                existing_collection = None
                try:
                    existing_collection = self.client.get_collection(collection_name)
                    collection_count = existing_collection.count()
                    
                    if collection_count > 0 and not force_reset:
                        logger.info(f"ğŸ“‹ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ë°œê²¬ ({collection_count}ê°œ ë°ì´í„°)")
                        logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³´ì¡´í•˜ê³  ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        self.collections[doc_type] = existing_collection
                        continue
                    elif force_reset:
                        logger.info(f"ğŸ”„ force_reset=True: ì»¬ë ‰ì…˜ '{collection_name}' ì¬ìƒì„±")
                        self.client.delete_collection(collection_name)
                        
                except Exception:
                    # ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                    logger.info(f"ğŸ†• ìƒˆ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„±")
                
                # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ì¬ìƒì„±
                self.collections[doc_type] = self.client.create_collection(
                    name=collection_name,
                    metadata={"hnsw:space": "cosine"},
                    embedding_function=None  # ìˆ˜ë™ìœ¼ë¡œ ì„ë² ë”© ì œê³µ
                )
                logger.info(f"âœ“ {collection_name} ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ ì»¬ë ‰ì…˜ '{collection_name}' ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ì»¬ë ‰ì…˜ì´ë¼ë„ ìƒì„± ì‹œë„
                try:
                    self.collections[doc_type] = self.client.create_collection(
                        name=f"{collection_name}_fallback",
                        metadata={"hnsw:space": "cosine"},
                        embedding_function=None
                    )
                    logger.warning(f"âš ï¸ ëŒ€ì²´ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}_fallback")
                except Exception as fallback_e:
                    logger.error(f"âŒ ëŒ€ì²´ ì»¬ë ‰ì…˜ ìƒì„±ë„ ì‹¤íŒ¨: {fallback_e}")
                    raise
    
    def is_duplicate(self, text: str) -> bool:
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        if text_hash in self.seen_hashes:
            self.stats['duplicates'] += 1
            return True
        self.seen_hashes.add(text_hash)
        return False
    
    def _process_batch(self, chunks_batch: List[Tuple[str, EnhancedChunkMetadata]], collection):
        """ë°°ì¹˜ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ ìµœì í™”"""
        if not chunks_batch:
            return
        
        try:
            texts = [chunk[0] for chunk in chunks_batch]
            metadatas = [chunk[1] for chunk in chunks_batch]
            
            # ë°°ì¹˜ ì„ë² ë”© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            embeddings = self.model.encode(
                texts, 
                batch_size=32,  # 16ì—ì„œ 32ë¡œ ì¦ê°€
                show_progress_bar=True,  # ì§„í–‰ë¥  í‘œì‹œ í™œì„±í™”
                convert_to_numpy=True,
                normalize_embeddings=True  # ì„ë² ë”© ì •ê·œí™” ì¶”ê°€
            )
            
            # Chroma ì €ì¥
            ids = []
            documents = []
            chroma_embeddings = []
            chroma_metadatas = []
            
            for i, (text, metadata) in enumerate(chunks_batch):
                chunk_id = f"{metadata.document_type}_{metadata.year}_{metadata.chunk_index}_{hashlib.md5(text.encode()).hexdigest()[:8]}"
                
                ids.append(chunk_id)
                documents.append(text)
                chroma_embeddings.append(embeddings[i].tolist())
                chroma_metadatas.append(metadata.to_dict())
            
            collection.add(
                ids=ids,
                documents=documents,
                embeddings=chroma_embeddings,
                metadatas=chroma_metadatas
            )
            
            self.stats['processed'] += len(chunks_batch)
            
        except MemoryError as e:
            logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}")
            logger.info(f"ğŸ’¡ ë°°ì¹˜ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì¬ì‹œë„...")
            
            # ë°°ì¹˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¬ì‹œë„
            mid = len(chunks_batch) // 2
            if mid > 0:
                try:
                    # ì²« ë²ˆì§¸ ì ˆë°˜ ì²˜ë¦¬
                    first_half = chunks_batch[:mid]
                    self._process_batch(first_half, collection)
                    
                    # ë‘ ë²ˆì§¸ ì ˆë°˜ ì²˜ë¦¬
                    second_half = chunks_batch[mid:]
                    self._process_batch(second_half, collection)
                    
                    logger.info(f"âœ… ë¶„í•  ì²˜ë¦¬ ì„±ê³µ: {len(chunks_batch)}ê°œ ì²­í¬")
                    
                except Exception as retry_e:
                    logger.error(f"âŒ ë¶„í•  ì²˜ë¦¬ë„ ì‹¤íŒ¨: {retry_e}")
                    self.stats['batch_memory_errors'] = self.stats.get('batch_memory_errors', 0) + 1
                    self.stats['errors'] += len(chunks_batch)
            else:
                logger.error(f"âŒ ë‹¨ì¼ ì²­í¬ë„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ (ë©”ëª¨ë¦¬ ë¶€ì¡±)")
                self.stats['single_chunk_errors'] = self.stats.get('single_chunk_errors', 0) + 1
                self.stats['errors'] += len(chunks_batch)
                
        except ValueError as e:
            logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ê°’ ì—ëŸ¬: {e}")
            logger.info(f"ğŸ’¡ ì„ë² ë”© ì°¨ì› ë˜ëŠ” ë°ì´í„° í˜•ì‹ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            self.stats['batch_value_errors'] = self.stats.get('batch_value_errors', 0) + 1
            self.stats['errors'] += len(chunks_batch)
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {type(e).__name__}: {e}")
            logger.info(f"ğŸ’¡ ë°°ì¹˜ í¬ê¸°: {len(chunks_batch)}ê°œ")
            self.stats['batch_unknown_errors'] = self.stats.get('batch_unknown_errors', 0) + 1
            self.stats['errors'] += len(chunks_batch)
    
    def process_document_type(self, file_mapping: Dict[int, str], doc_type: str):
        """ë¬¸ì„œ íƒ€ì…ë³„ ì²˜ë¦¬"""
        logger.info(f"\nğŸ“ {doc_type.upper()} ì²˜ë¦¬ ì‹œì‘")
        
        processor = self.processors[doc_type]
        collection = self.collections[doc_type]
        all_chunks = []
        
        for year, file_path in file_mapping.items():
            if not os.path.exists(file_path):
                logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}")
                continue
            
            try:
                logger.info(f"  {year}ë…„ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {os.path.basename(file_path)}")
                chunks = processor.process_file(file_path, year)
                
                # ì¤‘ë³µ ì œê±°
                unique_chunks = []
                for chunk_text, chunk_metadata in chunks:
                    if not self.is_duplicate(chunk_text):
                        unique_chunks.append((chunk_text, chunk_metadata))
                
                all_chunks.extend(unique_chunks)
                logger.info(f"    {len(chunks)}ê°œ ì²­í¬ â†’ {len(unique_chunks)}ê°œ ìœ ë‹ˆí¬")
                
            except UnicodeDecodeError as e:
                logger.error(f"âŒ íŒŒì¼ ì¸ì½”ë”© ì—ëŸ¬ ({file_path}): {e}")
                logger.info(f"ğŸ’¡ ë‹¤ë¥¸ ì¸ì½”ë”©ìœ¼ë¡œ ì¬ì‹œë„ ì¤‘...")
                # UTF-8 ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”©ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    # ì„ì‹œë¡œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„ (cp949, euc-kr ë“±)
                    logger.warning(f"âš ï¸ {file_path} ì¸ì½”ë”© ë¬¸ì œë¡œ ê±´ë„ˆëœ€")
                    self.stats['encoding_errors'] = self.stats.get('encoding_errors', 0) + 1
                except:
                    pass
                continue
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON íŒŒì‹± ì—ëŸ¬ ({file_path}): {e}")
                logger.info(f"ğŸ’¡ íŒŒì¼ í˜•ì‹ í™•ì¸ í•„ìš”: {file_path}")
                self.stats['json_errors'] = self.stats.get('json_errors', 0) + 1
                continue
                
            except MemoryError as e:
                logger.error(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡± ({file_path}): {e}")
                logger.info(f"ğŸ’¡ ë°°ì¹˜ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì—¬ì„œ ì¬ì‹œë„...")
                # ë°°ì¹˜ í¬ê¸° ìë™ ê°ì†Œ
                original_batch_size = self.batch_size
                self.batch_size = max(5, self.batch_size // 2)
                logger.info(f"ğŸ“‰ ë°°ì¹˜ í¬ê¸°: {original_batch_size} â†’ {self.batch_size}")
                
                try:
                    # ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œë¡œ ì¬ì‹œë„
                    chunks = processor.process_file(file_path, year)
                    unique_chunks = []
                    for chunk_text, chunk_metadata in chunks:
                        if not self.is_duplicate(chunk_text):
                            unique_chunks.append((chunk_text, chunk_metadata))
                    all_chunks.extend(unique_chunks)
                    logger.info(f"âœ… ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œë¡œ ì„±ê³µ: {len(chunks)}ê°œ ì²­í¬ â†’ {len(unique_chunks)}ê°œ ìœ ë‹ˆí¬")
                except Exception as retry_e:
                    logger.error(f"âŒ ì¬ì‹œë„ë„ ì‹¤íŒ¨ ({file_path}): {retry_e}")
                    self.stats['memory_errors'] = self.stats.get('memory_errors', 0) + 1
                    continue
                finally:
                    # ë°°ì¹˜ í¬ê¸° ë³µì›í•˜ì§€ ì•ŠìŒ (ë©”ëª¨ë¦¬ ì•ˆì •ì„± ìœ ì§€)
                    pass
                    
            except FileNotFoundError as e:
                logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ ({file_path}): {e}")
                self.stats['file_not_found_errors'] = self.stats.get('file_not_found_errors', 0) + 1
                continue
                
            except Exception as e:
                logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ ({file_path}): {type(e).__name__}: {e}")
                logger.info(f"ğŸ’¡ íŒŒì¼ ê±´ë„ˆëœ€: {file_path}")
                self.stats['unknown_errors'] = self.stats.get('unknown_errors', 0) + 1
                continue
        
        # ë°°ì¹˜ ì²˜ë¦¬
        logger.info(f"ì´ {len(all_chunks)}ê°œ ì²­í¬ ë²¡í„°í™” ì¤‘...")
        
        for i in tqdm(range(0, len(all_chunks), self.batch_size), desc=f"{doc_type} ë²¡í„°í™”"):
            batch = all_chunks[i:i + self.batch_size]
            self._process_batch(batch, collection)
        
        count = collection.count()
        logger.info(f"âœ… {doc_type}: {count}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
        return count
    
    def run_pipeline(self, file_mappings: Dict[str, Dict[int, str]], parallel: bool = True):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        logger.info("ğŸš€ ê°œì„ ëœ RAG ë²¡í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # í†µê³„ ì´ˆê¸°í™”
        self.stats = {'processed': 0, 'duplicates': 0, 'errors': 0}
        results = {}
        
        if parallel:
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {
                    executor.submit(self.process_document_type, file_mapping, doc_type): doc_type
                    for doc_type, file_mapping in file_mappings.items()
                }
                
                for future in as_completed(futures):
                    doc_type = futures[future]
                    try:
                        count = future.result()
                        results[doc_type] = count
                    except Exception as e:
                        logger.error(f"âŒ {doc_type} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        results[doc_type] = 0
        else:
            # ìˆœì°¨ ì²˜ë¦¬
            for doc_type, file_mapping in file_mappings.items():
                try:
                    count = self.process_document_type(file_mapping, doc_type)
                    results[doc_type] = count
                except Exception as e:
                    logger.error(f"âŒ {doc_type} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    results[doc_type] = 0
        
        # ê²°ê³¼ ìš”ì•½
        total_time = time.time() - start_time
        total_chunks = sum(results.values())
        
        logger.info(f"\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        logger.info(f"â° ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        logger.info(f"  â€¢ ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
        logger.info(f"  â€¢ ì¤‘ë³µ ì œê±°: {self.stats['duplicates']}ê°œ")
        logger.info(f"  â€¢ ì²˜ë¦¬ ì‹¤íŒ¨: {self.stats['errors']}ê°œ")
        
        # ì„¸ë¶€ ì—ëŸ¬ í†µê³„ ì¶œë ¥
        error_details = []
        if self.stats.get('encoding_errors', 0) > 0:
            error_details.append(f"ì¸ì½”ë”© ì—ëŸ¬: {self.stats['encoding_errors']}ê°œ")
        if self.stats.get('json_errors', 0) > 0:
            error_details.append(f"JSON íŒŒì‹± ì—ëŸ¬: {self.stats['json_errors']}ê°œ")
        if self.stats.get('memory_errors', 0) > 0:
            error_details.append(f"ë©”ëª¨ë¦¬ ì—ëŸ¬: {self.stats['memory_errors']}ê°œ")
        if self.stats.get('file_not_found_errors', 0) > 0:
            error_details.append(f"íŒŒì¼ ì—†ìŒ: {self.stats['file_not_found_errors']}ê°œ")
        if self.stats.get('batch_memory_errors', 0) > 0:
            error_details.append(f"ë°°ì¹˜ ë©”ëª¨ë¦¬ ì—ëŸ¬: {self.stats['batch_memory_errors']}ê°œ")
        if self.stats.get('unknown_errors', 0) > 0:
            error_details.append(f"ê¸°íƒ€ ì—ëŸ¬: {self.stats['unknown_errors']}ê°œ")
            
        if error_details:
            logger.info(f"  ğŸ“‹ ì—ëŸ¬ ì„¸ë¶€ì‚¬í•­:")
            for detail in error_details:
                logger.info(f"    - {detail}")
        
        for doc_type, count in results.items():
            logger.info(f"  â€¢ {doc_type}: {count}ê°œ")
        
        return results
    
    # ========== RAG ìµœì í™”ëœ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ ==========
    
    def _analyze_query_type(self, query: str) -> Dict[str, float]:
        """ì§ˆë¬¸ ë‚´ìš© ë¶„ì„í•˜ì—¬ ë™ì  ê°€ì¤‘ì¹˜ ê²°ì •"""
        query_lower = query.lower()
        
        # íšŒê³„ê¸°ì¤€ ê´€ë ¨ í‚¤ì›Œë“œ
        accounting_keywords = [
            'k-ifrs', 'kifrs', 'ê¸°ì¤€ì„œ', 'íšŒê³„ê¸°ì¤€', 'êµ­ì œíšŒê³„ê¸°ì¤€', 
            'íšŒê³„ì²˜ë¦¬ê¸°ì¤€', 'ì ìš©ê¸°ì¤€', 'ì¸ì‹ê¸°ì¤€', 'ì¸¡ì •ê¸°ì¤€'
        ]
        
        # ì¬ë¬´ìˆ˜ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ
        financial_keywords = [
            'ê¸ˆì•¡', 'ìˆ˜ì¹˜', 'ì›', 'ì–µ', 'ì¡°', 'ë°±ë§Œ', 'ì²œë§Œ', 'ë§¤ì¶œì•¡', 
            'ì˜ì—…ì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ', 'ìì‚°', 'ë¶€ì±„', 'ìë³¸', 'í˜„ê¸ˆ', 
            'ë¹„ìœ¨', 'ì¦ê°', 'ë³€ë™', 'ê·œëª¨', 'í¬ê¸°'
        ]
        
        # ì£¼ì„/ì„¤ëª… ê´€ë ¨ í‚¤ì›Œë“œ
        annotation_keywords = [
            'ì •ì±…', 'ë°©ì¹¨', 'ì ˆì°¨', 'ê¸°ì¤€', 'ë°©ë²•', 'ì²˜ë¦¬', 'ì„¤ëª…', 
            'ë‚´ì—­', 'í˜„í™©', 'ìƒì„¸', 'ì„¸ë¶€', 'êµ¬ì²´ì ', 'ì–´ë–»ê²Œ', 'ì™œ', 'ë¬´ì—‡'
        ]
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        accounting_score = sum(1 for keyword in accounting_keywords if keyword in query_lower)
        financial_score = sum(1 for keyword in financial_keywords if keyword in query_lower)
        annotation_score = sum(1 for keyword in annotation_keywords if keyword in query_lower)
        
        # ë™ì  ê°€ì¤‘ì¹˜ ê²°ì •
        if accounting_score > 0:
            return {
                'accounting_standard': 0.7,
                'annotation': 0.2, 
                'financial_table': 0.1
            }
        elif financial_score > 0:
            return {
                'financial_table': 0.6,
                'annotation': 0.3,
                'accounting_standard': 0.1
            }
        else:
            # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (annotation ìš°ì„ )
            return {
                'annotation': 0.5,
                'financial_table': 0.3,
                'accounting_standard': 0.2
            }
    
    def search_unified(self, query: str, doc_types: List[str] = None, 
                      n_results: int = 5, year_filter: int = None,
                      section_filter: str = None,
                      weights: Dict[str, float] = None) -> List[Dict]:
        """RAG ìµœì í™”ëœ í†µí•© ê²€ìƒ‰ (ì„±ëŠ¥ ê°œì„ )"""
        if doc_types is None:
            doc_types = list(self.collections.keys())
        
        # ë™ì  ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ: ì§ˆë¬¸ ë‚´ìš©ì— ë”°ë¼ ìë™ ê²°ì •
        if weights is None:
            weights = self._analyze_query_type(query)
            logger.info(f"ğŸ¯ ë™ì  ê°€ì¤‘ì¹˜ ì ìš©: {weights}")
        
        all_results = []
        
        for doc_type in doc_types:
            if doc_type not in self.collections:
                continue
            
            collection = self.collections[doc_type]
            weight = weights.get(doc_type, 1.0)
            
            # ê°•í™”ëœ ë©”íƒ€ë°ì´í„° í•„í„°ë§
            where_filter = {}
            if year_filter:
                where_filter["year"] = year_filter
            if section_filter:
                where_filter["section"] = {"$regex": f".*{section_filter}.*"}  # ë¶€ë¶„ ë§¤ì¹­
            
            try:
                # ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
                query_embedding = self.model.encode([query], 
                                                   show_progress_bar=False,
                                                   normalize_embeddings=True)[0]
                
                search_results = collection.query(
                    query_embeddings=[query_embedding.tolist()],
                    n_results=n_results,
                    where=where_filter if where_filter else None,
                    include=["documents", "metadatas", "distances"]
                )
                
                # ê°œì„ ëœ ì ìˆ˜ ê³„ì‚° (ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ë³€í™˜)
                distances = search_results['distances'][0]
                if distances:
                    for i, (doc, metadata, distance) in enumerate(zip(
                        search_results['documents'][0],
                        search_results['metadatas'][0], 
                        distances
                    )):
                        # ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê°œì„ ëœ ìŠ¤ì¼€ì¼ë§)
                        # ê±°ë¦¬ ë²”ìœ„ [0, 2] â†’ ìœ ì‚¬ë„ ë²”ìœ„ [0, 1]
                        base_score = max(0.0, min(1.0, 1.0 - distance))
                        
                        # ìœ ì‚¬ë„ ê³¡ì„  ì¡°ì • (ë‚®ì€ ìœ ì‚¬ë„ëŠ” ë” ë‚®ê²Œ, ë†’ì€ ìœ ì‚¬ë„ëŠ” ë” ë†’ê²Œ)
                        if base_score > 0.7:
                            base_score = base_score ** 0.8  # ë†’ì€ ìœ ì‚¬ë„ ê°•í™”
                        elif base_score < 0.3:
                            base_score = base_score ** 1.2  # ë‚®ì€ ìœ ì‚¬ë„ ì•½í™”
                        
                        # ê°€ì¤‘ì¹˜ ì ìš© (ë™ì  ë¬¸ì„œ íƒ€ì…ë³„ ì¤‘ìš”ë„)
                        weighted_score = base_score * weight
                        
                        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ ë³´ì •
                        content_boost = 1.0
                        if doc_type == 'annotation' and 'note_number' in metadata:
                            content_boost *= 1.1  # ì£¼ì„ ë°ì´í„° ì¶”ê°€ ê°€ì¤‘ì¹˜
                        elif doc_type == 'financial_table':
                            # ì¬ë¬´ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ì •
                            financial_keywords = ['ë§¤ì¶œ', 'ì´ìµ', 'ìì‚°', 'ë¶€ì±„', 'ìë³¸', 'í˜„ê¸ˆ']
                            keyword_matches = sum(1 for keyword in financial_keywords if keyword in doc.lower())
                            if keyword_matches > 0:
                                content_boost *= (1.0 + 0.05 * keyword_matches)  # í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ì— ë”°ë¥¸ ë³´ì •
                        elif doc_type == 'accounting_standard':
                            # íšŒê³„ê¸°ì¤€ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ì •
                            accounting_keywords = ['ê¸°ì¤€ì„œ', 'ifrs', 'íšŒê³„ê¸°ì¤€', 'ì¸ì‹', 'ì¸¡ì •']
                            keyword_matches = sum(1 for keyword in accounting_keywords if keyword in doc.lower())
                            if keyword_matches > 0:
                                content_boost *= (1.0 + 0.08 * keyword_matches)
                        
                        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
                        final_score = weighted_score * content_boost
                        
                        result = {
                            'document': doc,
                            'metadata': metadata,
                            'score': final_score,  # ìµœì¢… ì ìˆ˜ ì‚¬ìš©
                            'original_distance': distance,
                            'normalized_score': base_score,
                            'base_weighted_score': weighted_score,  # ê°€ì¤‘ì¹˜ ì ìš© í›„ ê¸°ë³¸ ì ìˆ˜
                            'content_boost': content_boost,  # ì»¨í…ìŠ¤íŠ¸ ë³´ì •ê°’
                            'doc_type': doc_type,
                            'doc_weight': weight,  # ì ìš©ëœ ë¬¸ì„œ íƒ€ì… ê°€ì¤‘ì¹˜
                            'rank': i + 1,
                            'content_hash': hashlib.md5(doc.encode('utf-8')).hexdigest()[:8]  # ì¤‘ë³µ í™•ì¸ìš©
                        }
                        all_results.append(result)
                    
            except Exception as e:
                logger.error(f"âŒ {doc_type} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ë¬¸ì„œ ì œê±° (ê°•í™”ëœ ë¡œì§)
        seen_hashes = set()
        unique_results = []
        for result in all_results:
            content_hash = result['content_hash']
            # ë‚´ìš©ì´ 70% ì´ìƒ ìœ ì‚¬í•œ ê²½ìš° ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
            is_duplicate = False
            for seen_hash in seen_hashes:
                if self._calculate_similarity(content_hash, seen_hash) > 0.7:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                seen_hashes.add(content_hash)
                unique_results.append(result)
        
        # ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ ë° ìƒìœ„ ê²°ê³¼ ì„ íƒ
        unique_results.sort(key=lambda x: x['score'], reverse=True)
        return unique_results[:n_results]
    
    def _calculate_similarity(self, hash1: str, hash2: str) -> float:
        """ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if hash1 == hash2:
            return 1.0
        # í•´ì‹œê°€ ë‹¤ë¥´ë©´ ë‚´ìš©ì´ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ê°„ì£¼
        return 0.0
    
    def search_with_cross_reference(self, query: str, n_results: int = 5) -> Dict[str, Any]:
        """êµì°¨ ì°¸ì¡° ê¸°ë°˜ í™•ì¥ ê²€ìƒ‰"""
        # 1ì°¨ ê²€ìƒ‰
        primary_results = self.search_unified(query, n_results=n_results)
        
        # êµì°¨ ì°¸ì¡° í‚¤ì›Œë“œ ì¶”ì¶œ
        cross_ref_queries = set()
        for result in primary_results:
            metadata = result['metadata']
            
            # êµì°¨ ì°¸ì¡°ì—ì„œ ì¶”ê°€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
            if metadata.get('cross_references'):
                for ref in metadata['cross_references']:
                    if ':' in ref:
                        ref_type, ref_term = ref.split(':', 1)
                        cross_ref_queries.add(ref_term)
            
            # ê¸ˆìœµ í•­ëª©ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ
            if metadata.get('financial_items'):
                cross_ref_queries.update(metadata['financial_items'])
        
        # êµì°¨ ì°¸ì¡° ê¸°ë°˜ 2ì°¨ ê²€ìƒ‰
        secondary_results = []
        for ref_query in list(cross_ref_queries)[:3]:  # ìµœëŒ€ 3ê°œ ì¶”ê°€ ê²€ìƒ‰
            ref_results = self.search_unified(ref_query, n_results=2)
            secondary_results.extend(ref_results)
        
        # ì¤‘ë³µ ì œê±° (ë¬¸ì„œ ID ê¸°ë°˜)
        seen_docs = set()
        final_results = []
        
        for result in primary_results + secondary_results:
            doc_hash = hashlib.md5(result['document'].encode()).hexdigest()[:16]
            if doc_hash not in seen_docs:
                seen_docs.add(doc_hash)
                final_results.append(result)
        
        return {
            'primary_results': primary_results,
            'cross_reference_results': secondary_results,
            'unified_results': final_results[:n_results * 2],
            'cross_ref_queries': list(cross_ref_queries)
        }
    
    def analyze_financial_trend(self, financial_item: str, years: List[int] = None) -> Dict[str, Any]:
        """ê¸ˆìœµ ì§€í‘œ íŠ¸ë Œë“œ ë¶„ì„"""
        if years is None:
            years = list(range(2020, 2025))  # ìµœê·¼ 5ë…„
        
        trend_data = {}
        
        for year in years:
            # í•´ë‹¹ ì—°ë„ ì¬ë¬´ì œí‘œì—ì„œ ê²€ìƒ‰
            results = self.search_unified(
                query=financial_item,
                doc_types=['financial_table'],
                year_filter=year,
                n_results=3
            )
            
            year_data = {
                'year': year,
                'found_documents': len(results),
                'key_findings': []
            }
            
            for result in results:
                metadata = result['metadata']
                if metadata.get('amounts'):
                    # ê´€ë ¨ ê¸ˆì•¡ ì •ë³´ ì¶”ì¶œ
                    relevant_amounts = {
                        k: v for k, v in metadata['amounts'].items() 
                        if financial_item in k
                    }
                    if relevant_amounts:
                        year_data['key_findings'].append({
                            'source': metadata.get('section', 'Unknown'),
                            'amounts': relevant_amounts,
                            'table_type': metadata.get('table_type', 'Unknown')
                        })
            
            trend_data[year] = year_data
        
        return {
            'financial_item': financial_item,
            'analysis_period': years,
            'yearly_data': trend_data,
            'total_documents_found': sum(data['found_documents'] for data in trend_data.values())
        }
    
    def get_context_for_rag(self, query: str, max_context_length: int = 2000) -> str:
        """RAGë¥¼ ìœ„í•œ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        # êµì°¨ ì°¸ì¡° ê²€ìƒ‰ìœ¼ë¡œ í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        search_results = self.search_with_cross_reference(query, n_results=10)
        unified_results = search_results['unified_results']
        
        context_parts = []
        current_length = 0
        
        # ê°œì„ ëœ ë¬¸ì„œ íƒ€ì…ë³„ ê· í˜• ì¡íŒ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        type_counts = {'annotation': 0, 'financial_table': 0, 'accounting_standard': 0}
        # ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ìµœëŒ€ ê°œìˆ˜ ì¡°ì •
        max_per_type = {
            'annotation': 4,        # ì£¼ì„ ë°ì´í„° ìš°ì„ ìˆœìœ„ ë†’ìŒ
            'financial_table': 3,   # ì¬ë¬´ì œí‘œ ì¤‘ê°„
            'accounting_standard': 2 # íšŒê³„ê¸°ì¤€ì„œ ìµœì†Œ
        }
        
        for result in unified_results:
            doc_type = result['doc_type']
            
            # ê°œì„ ëœ íƒ€ì…ë³„ ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            if type_counts[doc_type] >= max_per_type[doc_type]:
                continue
            
            document = result['document']
            metadata = result['metadata']
            score = result.get('score', 0.0)
            
            # ì ìˆ˜ ê¸°ë°˜ ë¬¸ì„œ í’ˆì§ˆ í•„í„°ë§ (ë‚®ì€ ì ìˆ˜ ë¬¸ì„œ ì œì™¸)
            if score < 0.3:
                continue
            
            # ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì²´í¬
            estimated_part_length = len(document) + 150  # ë©”íƒ€ë°ì´í„° í¬ë§·íŒ… ê¸¸ì´ í¬í•¨
            if current_length + estimated_part_length > max_context_length:
                # ë¬¸ì„œë¥¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì˜ë¼ì„œ í¬í•¨
                remaining_length = max_context_length - current_length - 150
                if remaining_length > 300:  # ìµœì†Œ ê¸¸ì´ í™•ë³´
                    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸° ì‹œë„
                    sentences = document.split('.')
                    truncated = ""
                    for sentence in sentences:
                        if len(truncated) + len(sentence) + 1 <= remaining_length:
                            truncated += sentence + "."
                        else:
                            break
                    document = truncated + "..." if truncated else document[:remaining_length] + "..."
                else:
                    break
            
            # ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ í˜•ì‹í™” (ë” ê°„ê²°í•˜ê³  êµ¬ì¡°í™”ë¨)
            context_part = f"""
ğŸ“„ {doc_type.upper()} ({metadata.get('year', 'N/A')}ë…„) | ì ìˆ˜: {score:.2f}
ğŸ“ ì¶œì²˜: {metadata.get('section', 'Unknown')}
ğŸ“ {document.strip()}
"""
            
            context_parts.append(context_part)
            current_length += len(context_part)
            type_counts[doc_type] += 1
        
        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = f"""
ë‹¤ìŒì€ '{query}' ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤:

{''.join(context_parts)}

--- ê²€ìƒ‰ ë©”íƒ€ì •ë³´ ---
â€¢ ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(unified_results)}ê°œ
â€¢ êµì°¨ ì°¸ì¡° í‚¤ì›Œë“œ: {', '.join(search_results['cross_ref_queries'][:5])}
â€¢ ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬: {type_counts}
"""
        
        return context.strip()
    
    def print_search_results(self, query: str, results: List[Dict]):
        """ê°œì„ ëœ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥ (ìƒì„¸ ì ìˆ˜ ë¶„ì„ í¬í•¨)"""
        print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
        print("=" * 70)
        
        for i, result in enumerate(results, 1):
            metadata = result['metadata']
            score = result['score']
            doc_type = result['doc_type']
            
            # ê¸°ë³¸ ì •ë³´
            print(f"\n{i}. [{doc_type.upper()}] ìµœì¢…ì ìˆ˜: {score:.6f}")
            print(f"   ì—°ë„: {metadata.get('year', 'N/A')}")
            print(f"   ì„¹ì…˜: {metadata.get('section', 'N/A')}")
            
            # ì ìˆ˜ ìƒì„¸ ë¶„ì„ (ê°œì„ ëœ ì •ë³´)
            if 'original_distance' in result:
                distance = result['original_distance']
                base_score = result.get('normalized_score', 0)
                doc_weight = result.get('doc_weight', 1.0)
                content_boost = result.get('content_boost', 1.0)
                
                print(f"   ğŸ“Š ì ìˆ˜ë¶„ì„: ê±°ë¦¬({distance:.4f}) â†’ ê¸°ë³¸({base_score:.4f}) Ã— ê°€ì¤‘ì¹˜({doc_weight:.2f}) Ã— ë³´ì •({content_boost:.3f})")
            
            # ë¬¸ì„œ íƒ€ì…ë³„ ì¶”ê°€ ì •ë³´
            if metadata.get('table_type'):
                print(f"   í‘œ ìœ í˜•: {metadata['table_type']}")
            if metadata.get('financial_items'):
                print(f"   ê¸ˆìœµ í•­ëª©: {', '.join(metadata['financial_items'][:3])}")
            if metadata.get('note_number'):
                print(f"   ì£¼ì„ë²ˆí˜¸: {metadata['note_number']}")
            
            print(f"   ë‚´ìš©: {result['document'][:200]}...")
    
    def get_statistics(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ í†µê³„ ì •ë³´ ë°˜í™˜"""
        stats = {
            'total_documents': 0,
            'total_chunks': 0,
            'collections': {},
            'processing_stats': getattr(self, 'stats', {})
        }
        
        # ê° ì»¬ë ‰ì…˜ë³„ í†µê³„
        for doc_type, collection in self.collections.items():
            try:
                count = collection.count()
                stats['collections'][doc_type] = count
                stats['total_chunks'] += count
            except Exception as e:
                stats['collections'][doc_type] = f"Error: {e}"
        
        # ì „ì²´ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
        stats['total_documents'] = len([f for f in stats['collections'].values() if isinstance(f, int)])
        
        return stats

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    # íŒŒì¼ ë§¤í•‘ ì„¤ì •
    file_mappings = {
        'annotation': {
            2014: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2014_parsed.json",
            2015: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2015_parsed.json",
            2016: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2016_parsed.json",
            2017: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2017_parsed.json",
            2018: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2018_parsed.json",
            2019: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2019_parsed.json",
            2020: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2020_parsed.json",
            2021: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2021_parsed.json",
            2022: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2022_parsed.json",
            2023: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2023_parsed.json",
            2024: "ì•„ì¹´ì´ë¸Œ/ê°ì‚¬ë³´ê³ ì„œ_2024_parsed.json"
        },
        'financial_table': {
            2014: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2014_parsed.json",
            2015: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2015_parsed.json",
            2016: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2016_parsed.json",
            2017: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2017_parsed.json",
            2018: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2018_parsed.json",
            2019: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2019_parsed.json",
            2020: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2020_parsed.json",
            2021: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2021_parsed.json",
            2022: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2022_parsed.json",
            2023: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2023_parsed.json",
            2024: "table_parsing/ê°ì‚¬ë³´ê³ ì„œ_2024_parsed.json"
        },
        'accounting_standard': {
            2024: "kifrs_combined_2.json"
        }
    }
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    # force_reset=Trueë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì™„ì „íˆ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì‹œì‘
    # force_reset=False (ê¸°ë³¸ê°’)ë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³´ì¡´í•˜ê³  ì¬ì‚¬ìš©
    pipeline = ImprovedVectorPipeline(
        db_path="./chroma_samsung_rag_optimized",
        batch_size=25,
        force_reset=False  # ê¸°ì¡´ ë°ì´í„° ë³´ì¡´ (Trueë¡œ ë³€ê²½í•˜ë©´ ì™„ì „ ì´ˆê¸°í™”)
    )
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    results = pipeline.run_pipeline(file_mappings, parallel=True)
    
    # ========== RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ==========
    print("\n" + "="*70)
    print("ğŸ” RAG ìµœì í™”ëœ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*70)
    
    # 1. í†µí•© ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # ê°œì„ ëœ RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        # ì¬ë¬´ì œí‘œ ê´€ë ¨ ì§ˆë¬¸
        "2024ë…„ ë§¤ì¶œì•¡ê³¼ ì˜ì—…ì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "ìœ í˜•ìì‚°ì˜ ê°ê°€ìƒê° ì •ì±…ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ì˜ êµ¬ì„± ë‚´ì—­ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ì¬ê³ ìì‚°ì˜ í‰ê°€ë°©ë²•ê³¼ ì¥ë¶€ê¸ˆì•¡ì€?",
        
        # ì£¼ì„ ê´€ë ¨ ì§ˆë¬¸  
        "ê¸ˆìœµë¦¬ìŠ¤ì™€ ìš´ìš©ë¦¬ìŠ¤ì˜ íšŒê³„ì²˜ë¦¬ ë°©ë²•ì€?",
        "ì¶©ë‹¹ë¶€ì±„ì˜ ì„¤ì • ê¸°ì¤€ê³¼ ê¸ˆì•¡ì€?",
        "ê´€ê³„ê¸°ì—… íˆ¬ì ë‚´ì—­ê³¼ ì§€ë¶„ë²• ì ìš© í˜„í™©ì€?",
        "ì™¸í™”ìì‚° ë° ë¶€ì±„ì˜ í™˜ì‚° ê¸°ì¤€ì€?",
        
        # íšŒê³„ê¸°ì¤€ ê´€ë ¨ ì§ˆë¬¸
        "K-IFRS 1116 ë¦¬ìŠ¤ ê¸°ì¤€ì„œ ì ìš© í˜„í™©ì€?",
        "K-IFRS 1109 ê¸ˆìœµìƒí’ˆ ë¶„ë¥˜ ê¸°ì¤€ì€?",
        "ìˆ˜ìµì¸ì‹ ì‹œì ê³¼ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì†ìƒì°¨ì† ì¸ì‹ ë° í™˜ì… ì •ì±…ì€?",
        
        # ê°ì‚¬ì˜ê²¬ ê´€ë ¨ ì§ˆë¬¸
        "ê°ì‚¬ì¸ì˜ ì˜ê²¬ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•µì‹¬ê°ì‚¬ì‚¬í•­(KAM)ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ê²½ì˜ì§„ì˜ ì¬ë¬´ì œí‘œ ì‘ì„± ì±…ì„ì€?",
        "ë‚´ë¶€íšŒê³„ê´€ë¦¬ì œë„ ìš´ì˜ì‹¤íƒœëŠ”?",
        
        # ì—°ë„ë³„ ë¹„êµ ì§ˆë¬¸
        "2023ë…„ ëŒ€ë¹„ 2024ë…„ ì´ìì‚° ë³€ë™ì‚¬í•­ì€?",
        "ì „ë…„ë„ì™€ ë¹„êµí•œ ë¶€ì±„ë¹„ìœ¨ ë³€í™”ëŠ”?",
        "ì—°ê²°ëŒ€ìƒ ì¢…ì†ê¸°ì—…ì˜ ë³€ë™ì‚¬í•­ì€?"
    ]
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"ì§ˆë¬¸: {query}")
        print('='*50)
        
        # í†µí•© ê²€ìƒ‰
        results = pipeline.search_unified(query, n_results=5)
        pipeline.print_search_results(query, results)
        
        # RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print(f"\n--- RAG ì»¨í…ìŠ¤íŠ¸ (ê¸¸ì´ ì œí•œ: 1000ì) ---")
        context = pipeline.get_context_for_rag(query, max_context_length=1000)
        print(context[:1000] + "..." if len(context) > 1000 else context)
    
    # 2. êµì°¨ ì°¸ì¡° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸  
    print(f"\n{'='*50}")
    print("êµì°¨ ì°¸ì¡° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print('='*50)
    
    cross_ref_results = pipeline.search_with_cross_reference("ë§¤ì¶œì•¡ ë³€ë™", n_results=3)
    print(f"êµì°¨ ì°¸ì¡° í‚¤ì›Œë“œ: {cross_ref_results['cross_ref_queries']}")
    print(f"1ì°¨ ê²€ìƒ‰ ê²°ê³¼: {len(cross_ref_results['primary_results'])}ê°œ")
    print(f"2ì°¨ ê²€ìƒ‰ ê²°ê³¼: {len(cross_ref_results['cross_reference_results'])}ê°œ")
    print(f"í†µí•© ê²°ê³¼: {len(cross_ref_results['unified_results'])}ê°œ")
    
    # 3. ê¸ˆìœµ íŠ¸ë Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸
    print(f"\n{'='*50}")
    print("ê¸ˆìœµ íŠ¸ë Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print('='*50)
    
    trend_analysis = pipeline.analyze_financial_trend("ë§¤ì¶œì•¡", years=[2022, 2023, 2024])
    print(f"ë¶„ì„ í•­ëª©: {trend_analysis['financial_item']}")
    print(f"ë¶„ì„ ê¸°ê°„: {trend_analysis['analysis_period']}")
    print(f"ì´ ë°œê²¬ ë¬¸ì„œ: {trend_analysis['total_documents_found']}ê°œ")
    
    for year, data in trend_analysis['yearly_data'].items():
        print(f"\n{year}ë…„:")
        print(f"  ë°œê²¬ ë¬¸ì„œ: {data['found_documents']}ê°œ")
        if data['key_findings']:
            for finding in data['key_findings'][:2]:
                print(f"  ì¶œì²˜: {finding['source']}")
                if finding['amounts']:
                    amounts_str = ', '.join([f"{k}: {v}" for k, v in list(finding['amounts'].items())[:2]])
                    print(f"  ê¸ˆì•¡: {amounts_str}")
    
    print(f"\nğŸ‰ RAG ìµœì í™” íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ë²¡í„°DBëŠ” ê³ í’ˆì§ˆ QA ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")