import streamlit as st
import json
import numpy as np
import faiss
import re
import os
from pathlib import Path
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from openai import OpenAI

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Legal RAG System",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# OpenAI API í‚¤ ì„¤ì • (í•˜ë“œì½”ë”©)
openai.api_key = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = openai.api_key
openai_client = OpenAI()

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("âš–ï¸ Legal RAG System")
st.sidebar.success("âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤")

system_choice = st.sidebar.selectbox(
    "ì‹œìŠ¤í…œ ì„ íƒ",
    ["ìƒë²• LLM", "K-IFRS RAG"],
    index=0
)

# ë©”ì¸ íƒ€ì´í‹€
st.title("âš–ï¸ Legal RAG System")
st.markdown("ê¸°ì¡´ ì„ë² ë”©ì„ í™œìš©í•œ ê°„ë‹¨í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
st.markdown("---")

# ìƒë²• LLM ì‹œìŠ¤í…œ (ê°„ì†Œí™”)
class SimpleCommercialLawRAG:
    def __init__(self):
        self.base_path = Path("/Users/kwonseoyoung/Desktop/ì•„ì¹´ì´ë¸Œ (1)")
        self.index_path = self.base_path / "kcc_index_json/kcc.index"
        self.ids_path = self.base_path / "kcc_index_json/ids.npy"
        self.metas_path = self.base_path / "kcc_index_json/metas.json"
        self.model_name = "intfloat/multilingual-e5-base"
        
        self.index = None
        self.ids = None
        self.metas = None
        self.model = None
        self.tfidf = None
        self.X_tfidf = None
        
    def load_resources(self):
        """ë¦¬ì†ŒìŠ¤ ë¡œë“œ"""
        try:
            # ì¸ë±ìŠ¤/ë©”íƒ€ ë¡œë“œ
            self.index = faiss.read_index(str(self.index_path))
            self.ids = np.load(self.ids_path, allow_pickle=True).tolist()
            with open(self.metas_path, "r", encoding="utf-8") as f:
                self.metas = json.load(f)
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = SentenceTransformer(self.model_name)
            
            # TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•
            self._build_tfidf()
            
            return True
        except Exception as e:
            st.error(f"ìƒë²• ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _build_tfidf(self):
        """TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•"""
        texts = []
        for m in self.metas:
            alias = " ".join(m.get("aliases", []))
            raw = m.get("raw_text", "")
            texts.append((alias + " " + raw).strip())
        
        self.tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95)
        self.X_tfidf = self.tfidf.fit_transform(texts)
    
    def normalize_query(self, q: str):
        """ì§ˆì˜ ì •ê·œí™”"""
        qn = q.strip()
        # ì¡°ë¬¸ ë²ˆí˜¸ ì •ê·œí™”
        m = re.search(r"(\d+)\s*ì˜\s*(\d+)", qn)
        if m:
            qn += f" {m.group(1)}-{m.group(2)} ì œ{m.group(1)}ì¡°ì˜{m.group(2)}"
        return qn
    
    def search_and_answer(self, query: str, topk: int = 5):
        """ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±"""
        if not all([self.index, self.model, self.tfidf]):
            return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        # ì§ˆì˜ ì •ê·œí™” ë° ì„ë² ë”©
        qn = self.normalize_query(query)
        qv = self.model.encode([qn], convert_to_numpy=True, normalize_embeddings=True)
        
        # FAISS ê²€ìƒ‰
        D, I = self.index.search(qv, topk*3)
        I, D = I[0], D[0]
        
        # TF-IDF ê²€ìƒ‰
        qv_t = self.tfidf.transform([qn])
        S_t = linear_kernel(qv_t, self.X_tfidf).ravel()
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²°í•© (ê°„ë‹¨íˆ)
        results = []
        seen = set()
        
        # ì„ë² ë”© ê²°ê³¼ ì¶”ê°€
        for i, score in zip(I, D):
            if i >= 0 and i not in seen:
                m = self.metas[int(i)]
                results.append({
                    "id": self.ids[int(i)],
                    "article_id": m.get("article_id"),
                    "title": m.get("title"),
                    "content": m.get("raw_text", ""),
                    "score": float(score)
                })
                seen.add(i)
        
        # TF-IDF ìƒìœ„ ê²°ê³¼ ë³´ì™„
        tfidf_top = S_t.argsort()[::-1][:topk]
        for i in tfidf_top:
            if i not in seen and len(results) < topk:
                m = self.metas[i]
                results.append({
                    "id": self.ids[i],
                    "article_id": m.get("article_id"),
                    "title": m.get("title"),
                    "content": m.get("raw_text", ""),
                    "score": float(S_t[i])
                })
                seen.add(i)
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ì„ íƒ
        results = results[:topk]
        
        # GPT ë‹µë³€ ìƒì„±
        answer = self._generate_answer(query, results)
        
        return answer, results
    
    def _generate_answer(self, query, results):
        """GPT ë‹µë³€ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for r in results:
            article_id = str(r["article_id"])
            header = f"ìƒë²• ì œ{article_id}ì¡°"
            if "ì˜" in article_id:
                header = f"ìƒë²• ì œ{article_id.replace('ì˜', 'ì¡°ì˜')}"
            
            title = r.get("title", "")
            if title:
                header += f"({title})"
            
            content = r.get("content", "")[:500]  # ê¸¸ì´ ì œí•œ
            context_parts.append(f"### {header}\n{content}")
        
        context = "\n\n".join(context_parts)
        
        system_msg = (
            "ë„ˆëŠ” í•œêµ­ ìƒë²• ì „ë¬¸ê°€ì•¼. ì œê³µëœ ì¡°ë¬¸ë“¤ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´. "
            "ë‹µë³€ ëì— ì°¸ì¡°í•œ ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•´."
        )
        
        user_msg = f"ì§ˆë¬¸: {query}\n\nê·¼ê±° ì¡°ë¬¸:\n{context}"
        
        try:
            resp = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                temperature=0.2,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ]
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# K-IFRS RAG ì‹œìŠ¤í…œ (ê°„ì†Œí™”)
class SimpleKIFRSRAG:
    def __init__(self):
        self.base_path = Path("/Users/kwonseoyoung/Desktop/ì•„ì¹´ì´ë¸Œ (1)")
        self.json_path = self.base_path / "ê¸°ì¤€ì„œ íŒŒì‹±.json"
        self.cache_dir = self.base_path / "hf_cache_3"
        self.title_emb_path = self.cache_dir / "title_emb_intfloat_multilingual-e5-large.npy"
        self.para_emb_path = self.cache_dir / "para_emb_intfloat_multilingual-e5-large.npy"
        self.model_name = "intfloat/multilingual-e5-large"
        
        self.docs = []
        self.paragraphs = []
        self.title_vecs = None
        self.para_vecs = None
        self.model = None
    
    def load_resources(self):
        """ë¦¬ì†ŒìŠ¤ ë¡œë“œ"""
        try:
            import time
            start_time = time.time()
            
            # JSON ë°ì´í„° ë¡œë“œ
            print(f"JSON ë¡œë”© ì‹œì‘...")
            with self.json_path.open(encoding='utf-8') as f:
                data = json.load(f)
            print(f"JSON ë¡œë”© ì™„ë£Œ: {time.time() - start_time:.2f}ì´ˆ")
            
            self.docs = data.get('documents', [])
            
            # ë¬¸ë‹¨ ë°ì´í„° êµ¬ì„±
            print(f"ë¬¸ë‹¨ ë°ì´í„° êµ¬ì„± ì‹œì‘...")
            para_start = time.time()
            for d in self.docs:
                std = d.get('standard_no')
                title = d.get('title', '')
                for p in d.get('paragraphs', []):
                    self.paragraphs.append({
                        "std": std,
                        "title": title,
                        "para_id": p.get('para_id'),
                        "text": p.get('text', ''),
                        "page": p.get('page')
                    })
            print(f"ë¬¸ë‹¨ ë°ì´í„° êµ¬ì„± ì™„ë£Œ: {time.time() - para_start:.2f}ì´ˆ (ì´ {len(self.paragraphs)}ê°œ)")
            
            # ì„ë² ë”© ë¡œë“œ
            print(f"ì„ë² ë”© íŒŒì¼ ë¡œë”© ì‹œì‘...")
            emb_start = time.time()
            self.title_vecs = np.load(self.title_emb_path)
            self.para_vecs = np.load(self.para_emb_path)
            print(f"ì„ë² ë”© ë¡œë”© ì™„ë£Œ: {time.time() - emb_start:.2f}ì´ˆ")
            print(f"Title ì„ë² ë”© í¬ê¸°: {self.title_vecs.shape}")
            print(f"Para ì„ë² ë”© í¬ê¸°: {self.para_vecs.shape}")
            
            # ëª¨ë¸ ë¡œë“œ
            print(f"ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            model_start = time.time()
            self.model = SentenceTransformer(self.model_name)
            print(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {time.time() - model_start:.2f}ì´ˆ")
            
            print(f"ì „ì²´ ë¡œë”© ì™„ë£Œ: {time.time() - start_time:.2f}ì´ˆ")
            return True
        except Exception as e:
            st.error(f"K-IFRS ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def search_and_answer(self, query: str, topk: int = 5):
        """ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±"""
        if not all([self.model, self.para_vecs is not None]):
            return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_emb = self.model.encode([f"query: {query}"], normalize_embeddings=True)[0]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ìµœì í™”: ìƒìœ„ topk*3ë§Œ ê³„ì‚°)
        similarities = np.dot(self.para_vecs, query_emb)
        
        # ìƒìœ„ ê²°ê³¼ ì„ íƒ (topk*3ì—ì„œ topk ì„ íƒìœ¼ë¡œ ì†ë„ í–¥ìƒ)
        top_indices = np.argpartition(similarities, -topk*3)[-topk*3:]
        top_indices = top_indices[np.argsort(similarities[top_indices])[::-1][:topk]]
        
        results = []
        for idx in top_indices:
            para = self.paragraphs[idx]
            results.append({
                "std": para["std"],
                "title": para["title"],
                "para_id": para["para_id"],
                "text": para["text"],
                "page": para.get("page"),
                "score": float(similarities[idx])
            })
        
        # GPT ë‹µë³€ ìƒì„±
        answer = self._generate_answer(query, results)
        
        return answer, results
    
    def _generate_answer(self, query, results):
        """GPT ë‹µë³€ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for r in results:
            header = f"[{r['std']}:{r['para_id']}] {r['title']} (p.{r.get('page', '?')})"
            text = r['text'][:500]  # ê¸¸ì´ ì œí•œ
            context_parts.append(f"{header}\n{text}")
        
        context = "\n\n".join(context_parts)
        
        system_msg = (
            "ë„ˆëŠ” K-IFRS íšŒê³„ê¸°ì¤€ ì „ë¬¸ê°€ì•¼. ì œê³µëœ ë¬¸ë‹¨ë“¤ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´. "
            "ë‹µë³€ ëì— ì°¸ì¡°í•œ ê¸°ì¤€ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•´."
        )
        
        user_msg = f"ì§ˆë¬¸: {query}\n\nê·¼ê±° ë¬¸ë‹¨:\n{context}"
        
        try:
            resp = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                temperature=0.2,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ]
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ìë™ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
@st.cache_resource
def load_commercial_rag():
    """ìƒë²• RAG ì‹œìŠ¤í…œ ë¡œë“œ (ìºì‹œë¨)"""
    rag = SimpleCommercialLawRAG()
    if rag.load_resources():
        return rag
    return None

@st.cache_resource
def load_kifrs_rag():
    """K-IFRS RAG ì‹œìŠ¤í…œ ë¡œë“œ (ìºì‹œë¨)"""
    rag = SimpleKIFRSRAG()
    if rag.load_resources():
        return rag
    return None

if system_choice == "ìƒë²• LLM":
    st.header("ğŸ›ï¸ ìƒë²• LLM ì‹œìŠ¤í…œ")
    
    # ìë™ ë¡œë”©
    with st.spinner("ìƒë²• ì‹œìŠ¤í…œ ë¡œë”© ì¤‘..."):
        commercial_rag = load_commercial_rag()
    
    if commercial_rag:
        st.success("âœ… ìƒë²• ì‹œìŠ¤í…œ ì¤€ë¹„ì™„ë£Œ")
        
        query = st.text_input(
            "ìƒë²• ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ì¤€ë¹„ê¸ˆì˜ ìë³¸ì „ì…ì€ ì´ì‚¬íšŒì—ì„œ ê²°ì •í•  ìˆ˜ ìˆëŠ”ê°€?"
        )
        
        col1, col2 = st.columns([1, 4])
        with col1:
            topk = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜", 3, 10, 5)
        
        if query and st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
            with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                answer, results = commercial_rag.search_and_answer(query, topk)
                
                st.markdown("### ğŸ“ ë‹µë³€")
                st.markdown(answer)
                
                st.markdown("### ğŸ” ì°¸ì¡° ì¡°ë¬¸")
                for i, result in enumerate(results, 1):
                    with st.expander(f"{i}. ìƒë²• ì œ{result['article_id']}ì¡° - {result.get('title', '')} (ì ìˆ˜: {result['score']:.3f})"):
                        st.write(result['content'][:800] + "..." if len(result['content']) > 800 else result['content'])
    else:
        st.error("ìƒë²• ì‹œìŠ¤í…œ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

else:  # K-IFRS RAG
    st.header("ğŸ“Š K-IFRS RAG ì‹œìŠ¤í…œ")
    
    # ìë™ ë¡œë”©
    with st.spinner("K-IFRS ì‹œìŠ¤í…œ ë¡œë”© ì¤‘..."):
        kifrs_rag = load_kifrs_rag()
    
    if kifrs_rag:
        st.success("âœ… K-IFRS ì‹œìŠ¤í…œ ì¤€ë¹„ì™„ë£Œ")
        
        query = st.text_input(
            "K-IFRS ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ê°œë°œë¹„ ìì‚° ì¸ì‹ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        )
        
        col1, col2 = st.columns([1, 4])
        with col1:
            topk = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜", 3, 10, 5)
        
        if query and st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
            with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                answer, results = kifrs_rag.search_and_answer(query, topk)
                
                st.markdown("### ğŸ“ ë‹µë³€")
                st.markdown(answer)
                
                st.markdown("### ğŸ” ì°¸ì¡° ë¬¸ë‹¨")
                for i, result in enumerate(results, 1):
                    with st.expander(f"{i}. [{result['std']}:{result['para_id']}] {result['title']} (p.{result.get('page', '?')}) - ì ìˆ˜: {result['score']:.3f}"):
                        st.write(result['text'])
    else:
        st.error("K-IFRS ì‹œìŠ¤í…œ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# í‘¸í„°
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray;'>
    âš–ï¸ Simple Legal RAG System | ê¸°ì¡´ ì„ë² ë”© í™œìš© ë²„ì „
    </div>
    """, 
    unsafe_allow_html=True
)
