{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c84d86a",
   "metadata": {},
   "source": [
    "# K-IFRS 기준서 처리 파이프라인 (통합버전)\n",
    "\n",
    "이 노트북은 K-IFRS 기준서를 처리하는 전체 파이프라인을 포함합니다:\n",
    "1. PDF 파싱 → JSON 변환\n",
    "2. JSON 전처리 및 정제\n",
    "3. 임베딩 생성\n",
    "4. 질의응답 시스템 구축\n",
    "\n",
    "## 환경 설정\n",
    "- 필요한 패키지: `pdfminer.six`, `sentence-transformers`, `transformers`, `accelerate`\n",
    "- 환경변수 설정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf757ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re, json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# 환경 설정 및 경로 설정\n",
    "BASE_DIR = Path(os.getenv('KIFRS_BASE_DIR', os.path.expanduser('~/Documents/kifrs')))\n",
    "PDF_DIR = BASE_DIR / 'raw_pdfs'\n",
    "CACHE_DIR = BASE_DIR / 'hf_cache'\n",
    "\n",
    "# 환경변수에서 설정 불러오기\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.pdf_dir = Path(os.getenv('KIFRS_PDF_DIR', PDF_DIR))\n",
    "        self.json_path = Path(os.getenv('KIFRS_JSON_PATH', BASE_DIR / 'kifrs_combined.json'))\n",
    "        self.cleaned_json_path = Path(os.getenv('KIFRS_CLEANED_JSON_PATH', BASE_DIR / 'kifrs_cleaned.json'))\n",
    "        self.cache_dir = Path(os.getenv('KIFRS_CACHE_DIR', CACHE_DIR))\n",
    "        self.embedding_model = os.getenv('KIFRS_EMBEDDING_MODEL', 'intfloat/multilingual-e5-large')\n",
    "        self.rebuild = os.getenv('KIFRS_REBUILD', 'False').lower() == 'true'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# 필요한 디렉토리 생성\n",
    "for path in [config.pdf_dir, config.cache_dir]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('설정 정보:')\n",
    "print(f'PDF 디렉토리: {config.pdf_dir}')\n",
    "print(f'JSON 경로: {config.json_path}')\n",
    "print(f'정제된 JSON 경로: {config.cleaned_json_path}')\n",
    "print(f'캐시 디렉토리: {config.cache_dir}')\n",
    "print(f'임베딩 모델: {config.embedding_model}')\n",
    "print(f'재빌드 모드: {config.rebuild}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec47d32",
   "metadata": {},
   "source": [
    "# 1. PDF 파싱 파이프라인\n",
    "\n",
    "아래 코드는 PDF 파일을 파싱하여 JSON 형식으로 변환합니다:\n",
    "- 문단 번호, 페이지 번호, 텍스트 내용 추출\n",
    "- 제목 및 기준서 번호 인식\n",
    "- 기본적인 텍스트 정제 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파싱을 위한 패턴 및 기본 함수\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "# 제목/번호(본문 또는 파일명), 문단번호 정규식\n",
    "RE_TITLE_LINE = re.compile(r\"(기업회계기준서\\s*제\\s*(\\d{4})\\s*호)\\s*([^\\n\\r]*)\", re.I)\n",
    "RE_TITLE_ALT  = re.compile(r\"K-IFRS[_\\s-]*제?\\s*(\\d{4})\\s*호[_\\s-]*([^\\n\\r]*)\", re.I)\n",
    "RE_PARA_NUM   = re.compile(r\"^\\s*(?P<num>(?:한)?\\d+(?:\\.\\d+)*)(?:\\.)?\\s+\", re.M)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    # 제어문자/여분 공백 정리 (줄바꿈은 '후처리'에서 쓰므로 여기선 보존)\n",
    "    s = re.sub(r\"[\\u200b\\ufeff\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \" \", s)\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s*\\n\\s*\", \"\\n\", s)  # 개행은 정규화만 하고 유지\n",
    "    return s.strip()\n",
    "\n",
    "def get_page_count(pdf_path: Path) -> int:\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        return sum(1 for _ in PDFPage.get_pages(f))\n",
    "\n",
    "def extract_title_and_no_from_text(sample_text: str, fallback_name: str):\n",
    "    std_no, title = None, None\n",
    "    m1 = RE_TITLE_LINE.search(sample_text)\n",
    "    if m1:\n",
    "        std_no = m1.group(2)\n",
    "        title  = (m1.group(3) or '').strip() or None\n",
    "    if not std_no:\n",
    "        m2 = RE_TITLE_ALT.search(fallback_name)\n",
    "        if m2:\n",
    "            std_no = m2.group(1)\n",
    "            title  = (m2.group(2) or '').replace('_', ' ').strip() or None\n",
    "    return std_no, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4dab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 후처리 함수\n",
    "def postprocess_text(text: str, *, has_next_para: bool) -> str:\n",
    "    \"\"\"\n",
    "    규칙:\n",
    "    1) (중복 줄 삭제) 줄바꿈 제거 전에:\n",
    "       - [규칙 A] '내용.' + 줄바꿈 + '내용' + (다음 문단번호) 형태면 '두 번째 내용' 삭제\n",
    "       - [규칙 B] '(숫자) 내용' + 줄바꿈 + '내용' + (다음 문단번호) 형태면 '두 번째 내용' 유지\n",
    "    2) 본문 안의 '- 숫자 -' 패턴 제거 (헤더/푸터 제거)\n",
    "    3) 줄바꿈 문자 제거 시 앞뒤를 붙여서 한 줄로 만들기\n",
    "    \"\"\"\n",
    "    # 0) 라인 단위 정리\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "\n",
    "    # 1) 끝부분 중복 줄 삭제 (다음 문단번호가 실제로 존재할 때만)\n",
    "    if has_next_para and len(lines) >= 2:\n",
    "        a = lines[-2]  # 줄바꿈 '앞' 줄\n",
    "        b = lines[-1]  # 줄바꿈 '뒤' 줄\n",
    "        # 앞줄이 마침표로 끝나고, '(숫자) '로 시작하지 않으면 비교\n",
    "        if a.endswith('.') and not re.match(r'^\\(\\d+\\)\\s+', a):\n",
    "            a_base = re.sub(r'\\.\\s*$', '', a).strip()  # 마침표만 제거한 a\n",
    "            if b == a_base:\n",
    "                lines = lines[:-1]  # b 삭제\n",
    "\n",
    "    s = \"\\n\".join(lines)\n",
    "\n",
    "    # 2) '- 숫자 -' 패턴 제거 (예: '- 12 -', '-12-', '- 3 -')\n",
    "    s = re.sub(r'\\s*-\\s*\\d+\\s*-\\s*', ' ', s)\n",
    "\n",
    "    # 3) 줄바꿈 제거(붙여쓰기) + 다중 공백 정리\n",
    "    s = s.replace('\\n', '')\n",
    "    s = re.sub(r'\\s{2,}', ' ', s).strip()\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81789ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 페이지 추출 및 문단 파싱 함수\n",
    "def extract_pages(pdf_path: Path) -> Tuple[str, List[Tuple[int, int, int]]]:\n",
    "    \"\"\"\n",
    "    각 페이지 텍스트를 추출해 하나의 큰 문자열로 합치고,\n",
    "    페이지별 (start_offset, end_offset, page_no) 리스트를 반환.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    ranges = []\n",
    "    offset = 0\n",
    "    page_count = get_page_count(pdf_path)\n",
    "    for p in range(page_count):\n",
    "        try:\n",
    "            t = extract_text(str(pdf_path), page_numbers=[p]) or ''\n",
    "        except Exception:\n",
    "            t = ''\n",
    "        t = clean_text(t)\n",
    "        start = offset\n",
    "        texts.append(t)\n",
    "        offset += len(t) + 1   # 페이지 사이에 '\\n' 하나 넣음\n",
    "        end = offset\n",
    "        ranges.append((start, end, p + 1))  # 1-based page\n",
    "    full = \"\\n\".join(texts)\n",
    "    return full, ranges\n",
    "\n",
    "def page_of_pos(pos: int, page_ranges: List[Tuple[int, int, int]]) -> int:\n",
    "    for (s, e, page) in page_ranges:\n",
    "        if s <= pos < e:\n",
    "            return page\n",
    "    return page_ranges[-1][2] if page_ranges else 1\n",
    "\n",
    "def parse_pdf_into_paragraphs(pdf_path: Path) -> Dict[str, Any]:\n",
    "    # 제목/번호: 1~2쪽 샘플 + 파일명 백업\n",
    "    try:\n",
    "        sample = (extract_text(str(pdf_path), page_numbers=[0]) or '') + \\\n",
    "                 (extract_text(str(pdf_path), page_numbers=[1]) or '')\n",
    "    except Exception:\n",
    "        sample = ''\n",
    "    std_no, title = extract_title_and_no_from_text(sample, pdf_path.name)\n",
    "\n",
    "    # 전체 텍스트 + 페이지 오프셋 맵\n",
    "    full, pranges = extract_pages(pdf_path)\n",
    "    if not full:\n",
    "        return {\n",
    "            'standard_no': std_no,\n",
    "            'title': title,\n",
    "            'source_file': pdf_path.name,\n",
    "            'paragraphs': []\n",
    "        }\n",
    "\n",
    "    # 문단 경계: 문단번호 토큰 매칭 위치 ~ 다음 토큰 직전까지\n",
    "    paragraphs = []\n",
    "    matches = list(RE_PARA_NUM.finditer(full))\n",
    "    for i, m in enumerate(matches):\n",
    "        para_id = m.group('num')  # 괄호 없는 숫자 계열만\n",
    "        start_text = m.end()\n",
    "        end_text = matches[i+1].start() if i + 1 < len(matches) else len(full)\n",
    "        raw_para = clean_text(full[start_text:end_text])\n",
    "        if not raw_para:\n",
    "            continue\n",
    "\n",
    "        page = page_of_pos(m.start(), pranges)\n",
    "\n",
    "        # 다음 문단번호가 실제로 존재하는지 플래그 (중복줄 삭제 규칙 활성화 여부)\n",
    "        has_next = (i + 1 < len(matches))\n",
    "        para_text = postprocess_text(raw_para, has_next_para=has_next)\n",
    "\n",
    "        paragraphs.append({\n",
    "            'para_id': para_id,\n",
    "            'page': page,\n",
    "            'text': para_text\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'standard_no': std_no,\n",
    "        'title': title,\n",
    "        'source_file': pdf_path.name,\n",
    "        'paragraphs': paragraphs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5acb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일 생성 함수\n",
    "def run_to_single_json(pdf_dir: Path = config.pdf_dir, out_path: Path = config.json_path):\n",
    "    \"\"\"\n",
    "    폴더 내 모든 PDF를 파싱해 하나의 JSON 파일로 저장.\n",
    "    구조:\n",
    "    {\n",
    "      \"documents\": [\n",
    "        {\n",
    "          \"standard_no\": \"1116\",\n",
    "          \"title\": \"리스\",\n",
    "          \"source_file\": \"K-IFRS_제1116호_리스.pdf\",\n",
    "          \"paragraphs\": [\n",
    "            {\"para_id\":\"1\",\"page\":5,\"text\":\"...\"},\n",
    "            ...\n",
    "          ]\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    pdfs = sorted([p for p in pdf_dir.glob('*.pdf')], key=lambda p: p.name)\n",
    "    combined = {'documents': []}\n",
    "    for i, pdf in enumerate(pdfs, 1):\n",
    "        try:\n",
    "            doc = parse_pdf_into_paragraphs(pdf)\n",
    "            combined['documents'].append(doc)\n",
    "            print(f\"[{i}/{len(pdfs)}] parsed: {pdf.name} (paras: {len(doc['paragraphs'])})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {pdf.name}: {e}\")\n",
    "\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(combined, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Saved ->\", out_path)\n",
    "\n",
    "# PDF -> JSON 변환 실행\n",
    "run_to_single_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c59fc",
   "metadata": {},
   "source": [
    "# 2. JSON 전처리\n",
    "\n",
    "전처리 규칙 적용:\n",
    "1. `[삭제됨]`/`[삭제함]` 등의 표시 제거\n",
    "2. 문장이 없는 문단(구두점 `.` 없음) 제거\n",
    "3. 문장 끝 불필요한 꼬리 제거\n",
    "4. 너무 짧은 문단 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 패턴 정의\n",
    "# 1) 부분 삭제: [ ... 삭제됨 ] / [ ... 삭제함 ]\n",
    "DEL_BRACKET_PATTERN = re.compile(r\"\\[[^\\]]*삭제(?:됨|함)\\]\", re.UNICODE)\n",
    "\n",
    "# 2) 문장 존재 여부: 간단히 '.' 포함 여부로 판단\n",
    "def has_sentence(text: str) -> bool:\n",
    "    return '.' in text\n",
    "\n",
    "# 3) 꼬리 허용 표식\n",
    "COLON_ANY = re.compile(r\":|：\")  # 콜론 유니코드 포함\n",
    "CIRCLED_ANY = re.compile(r\"[⑴-⑽①-⑳㉠-㉿]\")  # 특수 번호 기호\n",
    "PAREN_NUMBER_START = re.compile(r\"^\\s*[\\(（]\\s*\\d+\\s*[\\)）]\")  # (1), （1） 등\n",
    "PAREN_LETTER_START = re.compile(r\"^\\s*[\\(（]\\s*[A-Za-z가-힣一-龥]\\s*[\\)）]\")  # (A)/(가)/(一) 등\n",
    "\n",
    "def allow_tail(tail: str) -> bool:\n",
    "    # 꼬리에 콜론/특수기호가 '어디든' 포함되면 허용\n",
    "    if COLON_ANY.search(tail):\n",
    "        return True\n",
    "    if CIRCLED_ANY.search(tail):\n",
    "        return True\n",
    "    # 괄호 번호/문자는 '시작'에 있으면 허용\n",
    "    if PAREN_NUMBER_START.search(tail):\n",
    "        return True\n",
    "    if PAREN_LETTER_START.search(tail):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872487a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 전처리 함수\n",
    "def clean_json_data(input_path: Path = config.json_path, \n",
    "                   output_path: Path = config.cleaned_json_path):\n",
    "    from copy import deepcopy\n",
    "\n",
    "    # 원본 JSON 로드\n",
    "    with open(input_path, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    orig = deepcopy(data)\n",
    "    \n",
    "    # 통계 카운터\n",
    "    removed_no_sentence = 0\n",
    "    removed_short = 0\n",
    "    removed_empty_after_cut = 0\n",
    "    trimmed_tail = 0\n",
    "    modified_del_marks = 0\n",
    "\n",
    "    # 모든 문서에 대해 전처리 수행\n",
    "    for doc in data.get('documents', []):\n",
    "        new_paras = []\n",
    "        for p in doc.get('paragraphs', []):\n",
    "            text = (p.get('text') or '')\n",
    "            \n",
    "            # (1) 부분 삭제: [ ... 삭제됨/삭제함 ] -> 부분 문자열 제거\n",
    "            new_text = DEL_BRACKET_PATTERN.sub(\"\", text)\n",
    "            if new_text != text:\n",
    "                modified_del_marks += 1\n",
    "            # 공백 정리\n",
    "            new_text = re.sub(r\"\\s{2,}\", \" \", new_text).strip()\n",
    "\n",
    "            # (2) 부분 삭제 후 완전 비면 drop\n",
    "            if not new_text:\n",
    "                removed_empty_after_cut += 1\n",
    "                continue\n",
    "\n",
    "            # (3) 문장이 있는지 확인 ('.' 포함)\n",
    "            if not has_sentence(new_text):\n",
    "                removed_no_sentence += 1\n",
    "                continue\n",
    "\n",
    "            # (4) 마지막 '.' 이후의 꼬리 처리\n",
    "            last_dot = new_text.rfind('.')\n",
    "            if last_dot >= 0 and last_dot + 1 < len(new_text):\n",
    "                tail = new_text[last_dot + 1:].strip()\n",
    "                if tail and not allow_tail(tail):\n",
    "                    new_text = new_text[:last_dot + 1].strip()\n",
    "                    trimmed_tail += 1\n",
    "\n",
    "            # (5) 너무 짧은 문단은 제거 (10자 이하)\n",
    "            if len(new_text) <= 10:\n",
    "                removed_short += 1\n",
    "                continue\n",
    "\n",
    "            # 최종 정제된 텍스트로 업데이트\n",
    "            p['text'] = new_text\n",
    "            new_paras.append(p)\n",
    "\n",
    "        # 정제된 문단으로 교체\n",
    "        doc['paragraphs'] = new_paras\n",
    "\n",
    "    # 통계 출력\n",
    "    print('=== 전처리 통계 ===')\n",
    "    print(f'[삭제됨] 표시 제거: {modified_del_marks:,}')\n",
    "    print(f'제거된 빈 문단: {removed_empty_after_cut:,}')\n",
    "    print(f'제거된 무문장: {removed_no_sentence:,}')\n",
    "    print(f'제거된 짧은문단: {removed_short:,}')\n",
    "    print(f'꼬리 정리: {trimmed_tail:,}')\n",
    "\n",
    "    # 저장\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n저장 완료 -> {output_path}\")\n",
    "\n",
    "# 전처리 실행\n",
    "clean_json_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe81fe1",
   "metadata": {},
   "source": [
    "# 3. 임베딩 생성\n",
    "\n",
    "이 섹션에서는 문서의 제목과 문단에 대한 임베딩을 생성합니다:\n",
    "- 모델: `intfloat/multilingual-e5-large` (다국어 지원)\n",
    "- 제목/헤더 임베딩: 제목 + 소스파일명 + 첫 3개 문단\n",
    "- 문단 임베딩: 문단 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성을 위한 기본 설정\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩 캐시 파일 경로\n",
    "TITLE_EMB_PATH = config.cache_dir / f'title_emb_{config.embedding_model.replace(\"/\", \"_\")}.npy'\n",
    "PARA_EMB_PATH = config.cache_dir / f'para_emb_{config.embedding_model.replace(\"/\", \"_\")}.npy'\n",
    "\n",
    "# JSON 로드 & 인덱스 구성\n",
    "with config.cleaned_json_path.open(encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "docs = data.get('documents', [])\n",
    "title_keys = []\n",
    "title_texts = []\n",
    "title_to_para_indices = defaultdict(list)\n",
    "paragraphs = []\n",
    "\n",
    "for d in docs:\n",
    "    std = d.get('standard_no')\n",
    "    ttl = d.get('title') or ''\n",
    "    src = d.get('source_file') or ''\n",
    "    key = (std, ttl, src)\n",
    "    if key not in title_keys:\n",
    "        title_keys.append(key)\n",
    "        head = (d.get('paragraphs', [{}])[:3])\n",
    "        head_txt = \" \".join([(p.get('text') or '') for p in head])\n",
    "        title_texts.append(f\"{ttl}\\n{src}\\n{head_txt[:1000]}\")\n",
    "    base_idx = len(paragraphs)\n",
    "    for p in d.get('paragraphs', []):\n",
    "        paragraphs.append({\n",
    "            \"std\": std, \"title\": ttl, \"source\": src,\n",
    "            \"page\": p.get('page'), \"para_id\": p.get('para_id'), \"text\": p.get('text') or ''\n",
    "        })\n",
    "        title_to_para_indices[key].append(base_idx); base_idx += 1\n",
    "\n",
    "print('제목:', len(title_keys), '| 문단:', len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f26d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성 함수\n",
    "def create_embeddings(model_name: str = config.embedding_model, rebuild: bool = config.rebuild):\n",
    "    \"\"\"임베딩을 생성하거나 캐시에서 로드\"\"\"\n",
    "    if not rebuild and TITLE_EMB_PATH.exists() and PARA_EMB_PATH.exists():\n",
    "        print('캐시된 임베딩 로드 중...')\n",
    "        title_emb = np.load(TITLE_EMB_PATH)\n",
    "        para_emb = np.load(PARA_EMB_PATH)\n",
    "        print(f'제목 임베딩: {title_emb.shape} | 문단 임베딩: {para_emb.shape}')\n",
    "        return title_emb, para_emb\n",
    "\n",
    "    print(f'새로운 임베딩 생성 중 (모델: {model_name})...')\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    print('제목 임베딩 생성...')\n",
    "    title_emb = model.encode(title_texts, show_progress_bar=True, \n",
    "                           batch_size=32, normalize_embeddings=True)\n",
    "    np.save(TITLE_EMB_PATH, title_emb)\n",
    "    \n",
    "    print('문단 임베딩 생성...')\n",
    "    para_texts = [p['text'] for p in paragraphs]\n",
    "    para_emb = model.encode(para_texts, show_progress_bar=True,\n",
    "                          batch_size=32, normalize_embeddings=True)\n",
    "    np.save(PARA_EMB_PATH, para_emb)\n",
    "    \n",
    "    print(f'제목 임베딩: {title_emb.shape} | 문단 임베딩: {para_emb.shape}')\n",
    "    print(f'캐시 저장 완료 -> {TITLE_EMB_PATH}, {PARA_EMB_PATH}')\n",
    "    return title_emb, para_emb\n",
    "\n",
    "# 임베딩 생성 또는 로드\n",
    "title_embeddings, para_embeddings = create_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686aca66",
   "metadata": {},
   "source": [
    "# 4. 검색 시스템 구축\n",
    "\n",
    "하이브리드 검색 시스템 구현:\n",
    "1. BM25 검색 (키워드 기반)\n",
    "2. 벡터 검색 (의미 기반)\n",
    "3. Two-Track 라우팅 + Vector-only Fallback\n",
    "4. 옵션: 재랭킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d721a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 검색 구현\n",
    "def normalize(t: str) -> str:\n",
    "    return t.strip().lower()\n",
    "\n",
    "HAN_ENG_NUM = re.compile(r'[가-힣A-Za-z0-9]+', re.UNICODE)\n",
    "STOP = set(['그리고','등','및','또는','그러나','이는','그','이','저','것','수','등의'])\n",
    "\n",
    "def tokenize(t: str):\n",
    "    return HAN_ENG_NUM.findall(normalize(t))\n",
    "\n",
    "def filter_tokens(tokens: List[str]) -> List[str]:\n",
    "    return [w for w in tokens if w not in STOP and len(w) > 1]\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, docs_tokens, k1=1.5, b=0.75):\n",
    "        self.docs_tokens = docs_tokens\n",
    "        self.N = len(docs_tokens)\n",
    "        self.k1 = k1; self.b = b\n",
    "        self.avgdl = sum(len(d) for d in docs_tokens) / max(1, self.N)\n",
    "        self.df = Counter()\n",
    "        for doc in docs_tokens:\n",
    "            for term in set(doc):\n",
    "                self.df[term] += 1\n",
    "        self.idf = {t: math.log(1 + (self.N - df + 0.5)/(df + 0.5)) \n",
    "                   for t, df in self.df.items()}\n",
    "        self.tf = [Counter(doc) for doc in docs_tokens]\n",
    "\n",
    "    def _score_doc(self, q_tokens, i):\n",
    "        score, tf, dl = 0.0, self.tf[i], len(self.docs_tokens[i])\n",
    "        for term in q_tokens:\n",
    "            idf = self.idf.get(term)\n",
    "            if idf is None: continue\n",
    "            f = tf.get(term, 0)\n",
    "            if f == 0: continue\n",
    "            denom = f + self.k1 * (1 - self.b + self.b * dl / self.avgdl)\n",
    "            score += idf * (f * (self.k1 + 1)) / denom\n",
    "        return score\n",
    "\n",
    "    def search(self, q_tokens, topk=50):\n",
    "        scores = []\n",
    "        for i in range(self.N):\n",
    "            s = self._score_doc(q_tokens, i)\n",
    "            if s != 0.0:\n",
    "                scores.append((i, s))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:topk]\n",
    "\n",
    "    def search_subset(self, q_tokens, allowed: set, topk=50):\n",
    "        scores = []\n",
    "        for i in allowed:\n",
    "            s = self._score_doc(q_tokens, i)\n",
    "            if s != 0.0:\n",
    "                scores.append((i, s))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:topk]\n",
    "\n",
    "# BM25 인덱스 구축\n",
    "para_tokens = [filter_tokens(tokenize(p['text'])) for p in paragraphs]\n",
    "bm25 = BM25(para_tokens)\n",
    "print('BM25 인덱스 구축 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 검색 및 하이브리드 검색 구현\n",
    "class SearchEngine:\n",
    "    def __init__(self, model_name: str = config.embedding_model):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.title_emb = title_embeddings\n",
    "        self.para_emb = para_embeddings\n",
    "        self.bm25 = bm25\n",
    "\n",
    "    def _vector_sim(self, q_emb, emb_matrix, topk=50):\n",
    "        scores = (emb_matrix @ q_emb).squeeze()\n",
    "        idxs = np.argsort(scores)[-topk:][::-1]\n",
    "        return [(int(i), float(scores[i])) for i in idxs]\n",
    "\n",
    "    def search(self, query: str, mode: str = 'hybrid', topk: int = 10):\n",
    "        # 쿼리 임베딩\n",
    "        q_emb = self.model.encode([query], normalize_embeddings=True)\n",
    "        \n",
    "        # 제목 검색 (벡터)\n",
    "        title_matches = self._vector_sim(q_emb, self.title_emb, topk=3)\n",
    "        title_para_ids = set()\n",
    "        for idx, _ in title_matches:\n",
    "            title_para_ids.update(title_to_para_indices[title_keys[idx]])\n",
    "\n",
    "        # BM25 검색 (해당 제목 내에서만)\n",
    "        q_tokens = filter_tokens(tokenize(query))\n",
    "        if mode in ('bm25', 'hybrid') and q_tokens:\n",
    "            bm_matches = self.bm25.search_subset(q_tokens, title_para_ids, topk=topk)\n",
    "            if bm_matches:  # BM25 결과가 있으면 사용\n",
    "                results = bm_matches\n",
    "            else:  # 없으면 벡터 검색으로 폴백\n",
    "                results = self._vector_sim(q_emb, self.para_emb, topk=topk)\n",
    "        else:  # 벡터 검색만 사용\n",
    "            results = self._vector_sim(q_emb, self.para_emb, topk=topk)\n",
    "\n",
    "        # 결과 포매팅\n",
    "        formatted = []\n",
    "        for idx, score in results:\n",
    "            p = paragraphs[idx]\n",
    "            formatted.append({\n",
    "                'score': round(score, 3),\n",
    "                'standard_no': p['std'],\n",
    "                'title': p['title'],\n",
    "                'para_id': p['para_id'],\n",
    "                'page': p['page'],\n",
    "                'text': p['text']\n",
    "            })\n",
    "        return formatted\n",
    "\n",
    "# 검색 엔진 초기화\n",
    "search_engine = SearchEngine()\n",
    "print('검색 엔진 준비 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f43783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 예시\n",
    "query = \"리스 계약의 식별 방법\"\n",
    "results = search_engine.search(query, mode='hybrid', topk=3)\n",
    "\n",
    "print(f'검색어: \"{query}\"\\n')\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f'[{i}] 점수: {r[\"score\"]:.3f}')\n",
    "    print(f'기준서: {r[\"standard_no\"]} ({r[\"title\"]})')\n",
    "    print(f'위치: {r[\"para_id\"]}항 (p.{r[\"page\"]})')\n",
    "    print(f'내용: {r[\"text\"][:200]}...\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
