{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6910258a",
   "metadata": {},
   "source": [
    "# 상법 RAG 시스템 (PDF 파싱 → 벡터 임베딩 → LLM 질의응답)\n",
    "\n",
    "이 노트북은 상법 PDF 문서를 파싱하고, 벡터 임베딩을 생성한 후 RAG(Retrieval-Augmented Generation) 시스템을 구축하여 질의응답을 수행합니다.\n",
    "\n",
    "1. PDF → JSON 파싱\n",
    "2. 임베딩 생성 및 FAISS 인덱스 구축\n",
    "3. RAG + LLM 질의응답\n",
    "\n",
    "## 환경 변수 설정\n",
    "- `OPENAI_API_KEY`: OpenAI API 키\n",
    "- `OPENAI_MODEL`: 사용할 OpenAI 모델 (기본값: gpt-4-turbo-preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 (필요한 경우)\n",
    "# %pip install -q openai sentence-transformers faiss-cpu pypdf2 python-dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# OpenAI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 임베딩\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# PDF 처리\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# 환경변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI 클라이언트 설정\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "if not client.api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY 환경변수가 설정되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 설정\n",
    "WORKSPACE_DIR = Path(__file__).parent.absolute()\n",
    "PDF_PATH = WORKSPACE_DIR / \"상법(법률)(제20991호)(20250722).pdf\"\n",
    "INDEX_DIR = WORKSPACE_DIR / \"kcc_index_json\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 모델 설정\n",
    "EMB_MODEL = \"text-embedding-3-large\"            # 임베딩 모델\n",
    "LLM_MODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4-turbo-preview\")  # 답변 생성 모델\n",
    "\n",
    "# 기타 설정\n",
    "BATCH_SIZE = 128\n",
    "TOP_K = 10\n",
    "\n",
    "print(f\"작업 경로: {WORKSPACE_DIR}\")\n",
    "print(f\"PDF 파일 존재: {PDF_PATH.exists()}\")\n",
    "print(f\"인덱스 경로: {INDEX_DIR}\")\n",
    "print(f\"사용 모델: {EMB_MODEL}, {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab978d44",
   "metadata": {},
   "source": [
    "## 1. PDF 파싱\n",
    "\n",
    "PDF 문서를 읽어서 다음과 같은 구조의 JSON으로 변환합니다:\n",
    "```json\n",
    "{\n",
    "  \"meta\": {\n",
    "    \"title_ko\": \"상법\",\n",
    "    \"source_file\": \"상법(법률)(제20991호)(20250722).pdf\",\n",
    "    \"note\": \"부칙 제외, 본문 줄바꿈 제거, '제n조의m' 지원\"\n",
    "  },\n",
    "  \"total_articles\": 877,\n",
    "  \"articles\": [\n",
    "    {\n",
    "      \"article_id\": \"1\",\n",
    "      \"article_number_base\": 1,\n",
    "      \"article_number_suffix\": null,\n",
    "      \"title\": \"상사적용법규\",\n",
    "      \"body\": \"상사에 관하여...\",\n",
    "      \"part\": null,\n",
    "      \"chapter\": null,\n",
    "      \"section\": null\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da028871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf_to_text(pdf_path: Path) -> str:\n",
    "    \"\"\"PDF 파일을 텍스트로 변환\"\"\"\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    pages = []\n",
    "    for p in reader.pages:\n",
    "        try:\n",
    "            text = p.extract_text() or \"\"\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "        pages.append(text)\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "def extract_main_text(raw_text: str) -> str:\n",
    "    \"\"\"부칙 이전까지의 본문만 추출\"\"\"\n",
    "    import re\n",
    "    m = re.search(r\"(?:^|\\n)\\s*부칙\\s*(?:\\n|$)\", raw_text)\n",
    "    if m:\n",
    "        return raw_text[:m.start()].strip()\n",
    "    return raw_text.strip()\n",
    "\n",
    "def parse_articles(text: str) -> List[Dict]:\n",
    "    \"\"\"본문을 조문 단위로 파싱\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # 조문 구분 패턴\n",
    "    rx_art = re.compile(r\"\"\"\n",
    "        (?:제\\s*)?(\\d+)               # 기본 조문 번호\n",
    "        (?:\\s*의\\s*(\\d+))?           # 옵션: 의2, 의3 등\n",
    "        \\s*조                        # '조' 글자\n",
    "        (?:\\s*\\(([^)]+)\\))?         # 옵션: (제목)\n",
    "        \\s*(.+?)                    # 본문 (non-greedy)\n",
    "        (?=\\s*제\\s*\\d+\\s*조|\\s*$)   # 다음 조문 시작 또는 끝\n",
    "    \"\"\", re.VERBOSE | re.DOTALL)\n",
    "    \n",
    "    blocks = []\n",
    "    for m in rx_art.finditer(text):\n",
    "        base = int(m.group(1))\n",
    "        suffix = int(m.group(2)) if m.group(2) else None\n",
    "        title = m.group(3)\n",
    "        body = m.group(4).replace(\"\\n\", \" \").strip()\n",
    "        \n",
    "        art_id = str(base) if suffix is None else f\"{base}의{suffix}\"\n",
    "        blocks.append({\n",
    "            \"article_id\": art_id,\n",
    "            \"article_number_base\": base,\n",
    "            \"article_number_suffix\": suffix,\n",
    "            \"title\": title,\n",
    "            \"body\": body,\n",
    "            \"part\": None,  # TODO: 편/장/절 구조 파싱 (필요시)\n",
    "            \"chapter\": None,\n",
    "            \"section\": None\n",
    "        })\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "# PDF 파싱 실행\n",
    "raw_text = parse_pdf_to_text(PDF_PATH)\n",
    "main_text = extract_main_text(raw_text)\n",
    "articles = parse_articles(main_text)\n",
    "\n",
    "# JSON 생성\n",
    "output = {\n",
    "    \"meta\": {\n",
    "        \"title_ko\": \"상법\",\n",
    "        \"source_file\": PDF_PATH.name,\n",
    "        \"note\": \"부칙 제외, 본문 줄바꿈 제거, '제n조의m' 지원\"\n",
    "    },\n",
    "    \"total_articles\": len(articles),\n",
    "    \"articles\": articles\n",
    "}\n",
    "\n",
    "# 저장\n",
    "out_json = INDEX_DIR.parent / \"상법_파싱.json\"\n",
    "with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"총 {len(articles)}개 조문 파싱 완료\")\n",
    "print(f\"저장 경로: {out_json} ({out_json.stat().st_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b750f3",
   "metadata": {},
   "source": [
    "## 2. 벡터 임베딩 생성\n",
    "\n",
    "파싱된 JSON에서 조문별 검색용 데이터를 구성하고 FAISS 인덱스를 생성합니다:\n",
    "\n",
    "1. 조문별 별칭(alias) 생성\n",
    "2. 텍스트 정규화\n",
    "3. 임베딩 생성\n",
    "4. FAISS 인덱스 구축 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_aliases(base: int, suf: Optional[int], title: Optional[str]) -> List[str]:\n",
    "    \"\"\"조문 번호와 제목으로부터 다양한 검색용 별칭 생성\"\"\"\n",
    "    aliases = []\n",
    "    # 한국어 기본형\n",
    "    if suf is None:\n",
    "        ko_core = f\"제{base}조\"\n",
    "    else:\n",
    "        ko_core = f\"제{base}조의{suf}\"\n",
    "    aliases.append(ko_core)\n",
    "    aliases.append(f\"상법 {ko_core}\")\n",
    "    aliases.append(f\"상법 {base}조\" if suf is None else f\"상법 {base}조의{suf}\")\n",
    "    aliases.append(f\"{base}조\" if suf is None else f\"{base}조의{suf}\")\n",
    "    if title:\n",
    "        aliases.append(f\"{ko_core}({title})\")\n",
    "        aliases.append(f\"상법 {ko_core}({title})\")\n",
    "    \n",
    "    # 하이픈 표기\n",
    "    hyf = f\"{base}\" if suf is None else f\"{base}-{suf}\"\n",
    "    aliases.append(hyf)\n",
    "    aliases.append(f\"제{hyf}조\")\n",
    "    aliases.append(f\"상법 제{hyf}조\")\n",
    "    \n",
    "    # 영어식\n",
    "    en_core = f\"Article {base}\" if suf is None else f\"Article {base}-{suf}\"\n",
    "    aliases.append(en_core)\n",
    "    aliases.append(f\"KCC {en_core}\")\n",
    "    aliases.append(f\"Korean Commercial Code {en_core}\")\n",
    "    \n",
    "    # 중복 제거 + 안정 정렬\n",
    "    seen = set(); out = []\n",
    "    for a in aliases:\n",
    "        if a not in seen:\n",
    "            seen.add(a); out.append(a)\n",
    "    return out\n",
    "\n",
    "def build_text(rec: Dict) -> str:\n",
    "    \"\"\"검색용 텍스트 구성\"\"\"\n",
    "    base = rec.get(\"article_number_base\")\n",
    "    suf  = rec.get(\"article_number_suffix\")\n",
    "    head = f\"상법 제{base}조\" if suf is None else f\"상법 제{base}조의{suf}\"\n",
    "    if rec.get(\"title\"):\n",
    "        head += f\" {rec['title']}\"\n",
    "    body = rec.get(\"body\",\"\").replace(\"\\n\", \" \").strip()\n",
    "    return f\"[{head}] {body}\".strip()\n",
    "\n",
    "# JSON 로드\n",
    "with open(INDEX_DIR.parent / \"상법_파싱.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "articles = data.get(\"articles\", [])\n",
    "print(f\"로드된 조문 수: {len(articles)}\")\n",
    "\n",
    "# 검색용 데이터 구성\n",
    "docs = []\n",
    "for rec in articles:\n",
    "    base = rec.get(\"article_number_base\")\n",
    "    suf  = rec.get(\"article_number_suffix\")\n",
    "    art_id = rec.get(\"article_id\") or (str(base) if suf is None else f\"{base}의{suf}\")\n",
    "    title = rec.get(\"title\")\n",
    "\n",
    "    aliases = make_aliases(base, suf, title)\n",
    "    text = build_text(rec)\n",
    "    \n",
    "    docs.append({\n",
    "        \"id\": f\"KCC-{art_id}\",\n",
    "        \"article_id\": art_id,\n",
    "        \"title\": title,\n",
    "        \"aliases\": aliases,\n",
    "        \"text\": text,\n",
    "        \"raw_text\": rec.get(\"body\", \"\"),\n",
    "        \"meta\": {\n",
    "            \"law\": \"상법\",\n",
    "            \"source_file\": data.get(\"meta\",{}).get(\"source_file\",\"\"),\n",
    "            \"part\": rec.get(\"part\"),\n",
    "            \"chapter\": rec.get(\"chapter\"),\n",
    "            \"section\": rec.get(\"section\"),\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"검색용 문서 생성: {len(docs)}개\")\n",
    "print(\"샘플 문서:\", {k:docs[0][k] for k in [\"id\",\"article_id\",\"title\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "def encode_texts(texts: List[str], batch_size: int = 128, normalize: bool = True) -> np.ndarray:\n",
    "    \"\"\"텍스트 리스트를 임베딩 벡터로 변환\"\"\"\n",
    "    emb = model.encode(texts, batch_size=batch_size, \n",
    "                      convert_to_numpy=True, show_progress_bar=True)\n",
    "    if normalize:\n",
    "        faiss.normalize_L2(emb)\n",
    "    return emb\n",
    "\n",
    "# 코퍼스 구성\n",
    "CORPUS = [d[\"text\"] for d in docs]\n",
    "IDS = [d[\"id\"] for d in docs]\n",
    "METAS = [{k:d[k] for k in [\"article_id\",\"title\",\"aliases\",\"raw_text\",\"meta\"]} for d in docs]\n",
    "\n",
    "# 임베딩 및 인덱스 생성\n",
    "print(\"임베딩 생성 중...\")\n",
    "EMB = encode_texts(CORPUS, batch_size=BATCH_SIZE, normalize=True)\n",
    "index = faiss.IndexFlatIP(EMB.shape[1])\n",
    "index.add(EMB)\n",
    "print(f\"인덱스 크기: {index.ntotal}, 차원: {EMB.shape[1]}\")\n",
    "\n",
    "# 저장\n",
    "print(\"인덱스 저장 중...\")\n",
    "faiss.write_index(index, str(INDEX_DIR / \"kcc.index\"))\n",
    "np.save(INDEX_DIR / \"ids.npy\", np.array(IDS))\n",
    "with (INDEX_DIR / \"metas.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(METAS, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"인덱스 저장 완료: {INDEX_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc58d7",
   "metadata": {},
   "source": [
    "## 3. RAG + LLM 질의응답 시스템\n",
    "\n",
    "생성된 임베딩과 FAISS 인덱스를 활용하여 다음과 같은 과정으로 질의응답을 수행합니다:\n",
    "\n",
    "1. 질의어 임베딩 → 코사인 유사도 기반 1차 검색\n",
    "2. LLM 기반 후보 재정렬 (정답 근거성 평가)\n",
    "3. 선택된 근거를 바탕으로 LLM 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ccff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 유틸리티\n",
    "RX_ART_NUM = re.compile(r\"(?:제\\s*)?(\\d+)(?:\\s*조\\s*의\\s*(\\\\d+)|\\\\s*조)\")\n",
    "\n",
    "def normalize_query(q: str) -> str:\n",
    "    \"\"\"검색어 정규화 (조문 번호 처리)\"\"\"\n",
    "    qn = q.strip()\n",
    "    m = re.search(r\"(\\d+)\\s*의\\s*(\\d+)\", qn)\n",
    "    extra = []\n",
    "    if m:\n",
    "        extra.append(f\"{m.group(1)}-{m.group(2)}\")\n",
    "        extra.append(f\"제{m.group(1)}조의{m.group(2)}\")\n",
    "    m2 = RX_ART_NUM.search(qn)\n",
    "    if m2:\n",
    "        base = m2.group(1); suf = m2.group(2)\n",
    "        if suf:\n",
    "            extra += [f\"{base}의{suf}\", f\"{base}-{suf}\", \n",
    "                     f\"제{base}조의{suf}\", f\"Article {base}-{suf}\"]\n",
    "        else:\n",
    "            extra += [f\"{base}\", f\"제{base}조\", f\"Article {base}\"]\n",
    "    if extra:\n",
    "        qn = qn + \" \" + \" \".join(sorted(set(extra)))\n",
    "    return qn\n",
    "\n",
    "def retrieve(query: str, top_k: int = 10) -> List[Dict]:\n",
    "    \"\"\"임베딩 기반 1차 검색\"\"\"\n",
    "    qv = client.embeddings.create(model=EMB_MODEL, input=[query]).data[0].embedding\n",
    "    qv = np.array(qv, dtype=np.float32)\n",
    "    D, I = index.search(qv.reshape(1, -1), top_k)\n",
    "    I, D = I[0], D[0]\n",
    "    \n",
    "    results = []\n",
    "    for i, s in zip(I, D):\n",
    "        if i < 0:  # FAISS의 패딩\n",
    "            continue\n",
    "        m = METAS[i]\n",
    "        results.append({\n",
    "            \"index\": int(i),\n",
    "            \"score\": float(s),\n",
    "            \"text\": m[\"raw_text\"],\n",
    "            \"meta\": m[\"meta\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def llm_rerank(query: str, candidates: List[Dict], take: int = 6, \n",
    "               model: str = LLM_MODEL) -> List[Dict]:\n",
    "    \"\"\"LLM 기반 후보 재정렬\"\"\"\n",
    "    # 컨텍스트 구성\n",
    "    bullet = []\n",
    "    for i, c in enumerate(candidates):\n",
    "        snippet = c[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        if len(snippet) > 600:\n",
    "            snippet = snippet[:600] + \" ...\"\n",
    "        bullet.append(f\"[{i}] {snippet}\")\n",
    "    \n",
    "    # LLM 프롬프트\n",
    "    prompt = f\"\"\"당신은 문서검색 재랭킹을 담당하는 평가자입니다.\n",
    "사용자 질문: {query}\n",
    "\n",
    "아래 후보 발췌문들 중에서, 질문에 대한 **정답 근거**가 가장 잘 담긴 순서로 재정렬하세요.\n",
    "가능하면 숫자/연도/표 제목 일치 여부를 중시하세요.\n",
    "\n",
    "후보:\n",
    "{chr(10).join(bullet)}\n",
    "\n",
    "출력 형식:\n",
    "- 상위 {take}개의 인덱스만 콤마로 나열 (예: 3,0,5,1,2,4)\n",
    "- 다른 설명은 쓰지 말 것\n",
    "\"\"\"\n",
    "    msg = [\n",
    "        {\"role\":\"system\",\"content\":\"You are a rigorous re-ranker that only outputs indices.\"},\n",
    "        {\"role\":\"user\",\"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # LLM 호출 및 파싱\n",
    "    resp = client.chat.completions.create(model=model, messages=msg, temperature=0)\n",
    "    line = resp.choices[0].message.content.strip()\n",
    "    idxs = []\n",
    "    for tok in line.replace(\" \", \"\").split(\",\"):\n",
    "        if tok.isdigit():\n",
    "            idxs.append(int(tok))\n",
    "    \n",
    "    # 안전장치\n",
    "    if not idxs:\n",
    "        idxs = list(range(min(take, len(candidates))))\n",
    "    idxs = idxs[:take]\n",
    "    \n",
    "    # 재정렬된 결과 반환\n",
    "    return [candidates[i] for i in idxs]\n",
    "\n",
    "def build_context_snippets(chunks: List[Dict], max_chars: int = 1800) -> str:\n",
    "    \"\"\"LLM 답변용 컨텍스트 구성\"\"\"\n",
    "    ctx_lines = []\n",
    "    for rank, c in enumerate(chunks, start=1):\n",
    "        tag = f\"[E{rank}]\"\n",
    "        snippet = c[\"text\"].strip()\n",
    "        if len(snippet) > 1200:\n",
    "            snippet = snippet[:1200] + \" ...\"\n",
    "        ctx_lines.append(f\"{tag} {snippet}\")\n",
    "    return \"\\n\\n\".join(ctx_lines)\n",
    "\n",
    "def answer_with_rag_llm(query: str, retrieve_k: int = 12, rerank_take: int = 6, \n",
    "                       model: str = LLM_MODEL, temperature: float = 0.0, \n",
    "                       debug: bool = False) -> Dict:\n",
    "    \"\"\"RAG + LLM 기반 질의응답\"\"\"\n",
    "    # 1) 초기 검색\n",
    "    cand = retrieve(query, top_k=retrieve_k)\n",
    "    \n",
    "    # 2) LLM 재정렬\n",
    "    top = llm_rerank(query, cand, take=rerank_take, model=model)\n",
    "    \n",
    "    # 3) 컨텍스트 구성\n",
    "    ctx = build_context_snippets(top)\n",
    "    \n",
    "    # 4) 답변 생성\n",
    "    prompt = f\"\"\"당신은 정확한 회계/법률 문서 QA 어시스턴트입니다.\n",
    "아래 '근거 발췌문'만을 **사실 근거**로 사용하여 질문에 답하세요.\n",
    "근거에 없는 내용은 추정하지 말고, '근거 부족'이라고 밝혀주세요.\n",
    "필요하면 답변 문장 끝에 [E1], [E2]처럼 근거 번호를 달아주세요.\n",
    "\n",
    "[질문]\n",
    "{query}\n",
    "\n",
    "[근거 발췌문]\n",
    "{ctx}\n",
    "\"\"\"\n",
    "    msgs = [\n",
    "        {\"role\":\"system\",\"content\":\"You are a precise, Korean-speaking assistant for financial/legal documents.\"},\n",
    "        {\"role\":\"user\",\"content\": prompt}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model, messages=msgs, temperature=temperature\n",
    "    )\n",
    "    answer = resp.choices[0].message.content.strip()\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"evidence\": top,\n",
    "        \"raw_candidates\": cand if debug else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf1ae8",
   "metadata": {},
   "source": [
    "## 4. 데모\n",
    "\n",
    "실제 질의응답을 수행해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20251fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 질의\n",
    "q = \"상법에서 소상인의 정의는 무엇입니까?\"\n",
    "out = answer_with_rag_llm(q, debug=True)\n",
    "\n",
    "print(\"=== 답변 ===\")\n",
    "print(out[\"answer\"])\n",
    "\n",
    "print(\"\\n=== 근거 ===\")\n",
    "for i, e in enumerate(out[\"evidence\"], start=1):\n",
    "    title = (e.get(\"meta\", {}) or {}).get(\"title\", \"\")\n",
    "    print(f\"[E{i}] score={e['score']:.4f}, title={title}\")\n",
    "\n",
    "def preview_text(evidence_list: List[Dict], which: int = 1):\n",
    "    \"\"\"근거 원문 확인\"\"\"\n",
    "    i = max(1, min(which, len(evidence_list))) - 1\n",
    "    print(evidence_list[i][\"text\"])\n",
    "\n",
    "# 첫 번째 근거 원문 확인\n",
    "preview_text(out[\"evidence\"], which=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
