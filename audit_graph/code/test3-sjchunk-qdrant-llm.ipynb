{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654b235f",
   "metadata": {},
   "source": [
    "## 1. ì„ë² ë”© & Qdrant ì—…ì„œíŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c307f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: mps\n",
      "[INFO] Loaded model: bge-ko (dragonkue/bge-m3-ko), dim=1024\n",
      "[INFO] Loaded corpus: 2292 chunks\n",
      "[INFO] Using existing collection: audit_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting (bge-ko): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:18<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Upsert done: 2292 â†’ DB path=/Users/dan/Desktop/snu_project/git_ì œì¶œìš©/data/vector_store/final-sjchunk/bge-ko-qdrant_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "# ===== ëª¨ë¸ ë§¤í•‘ =====\n",
    "MODEL_NAME = \"bge-ko\"   # â† ì—¬ê¸°ë§Œ ë°”ê¿”ì£¼ë©´ ë¨ (\"e5\", \"koe5\", \"bge-ko\", \"bge-m3\", \"kure\")\n",
    "\n",
    "MODEL_MAP = {\n",
    "    \"e5\": \"intfloat/multilingual-e5-small\",\n",
    "    \"e5-base\": \"intfloat/multilingual-e5-base\",\n",
    "    \"koe5\": \"intfloat/KoE5-large\",\n",
    "    \"bge-ko\": \"dragonkue/bge-m3-ko\",\n",
    "    \"bge-m3\": \"BAAI/bge-m3\",\n",
    "    \"kure\": \"nlpai-lab/KURE-v1\",\n",
    "}\n",
    "\n",
    "EMBED_MODEL_NAME = MODEL_MAP[MODEL_NAME]\n",
    "COLLECTION       = \"audit_chunks\"\n",
    "BATCH            = 64\n",
    "QDRANT_PATH = f\"/Users/dan/Desktop/snu_project/git_ì œì¶œìš©/data/vector_store/final-sjchunk/{MODEL_NAME}-qdrant_db\"\n",
    "CHUNK_FILE = Path(\"/Users/dan/Desktop/snu_project/git_ì œì¶œìš©/data/processed/enhanced_vector_chunks_9_24.jsonl\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € í¬í¬ ê²½ê³  ë„ê¸° (ê¶Œì¥)\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# ===== ë””ë°”ì´ìŠ¤ ì„ íƒ (MPS > CUDA > CPU) =====\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# ===== ëª¨ë¸ ë¡œë“œ =====\n",
    "model = SentenceTransformer(EMBED_MODEL_NAME, device=device)\n",
    "dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"[INFO] Loaded model: {MODEL_NAME} ({EMBED_MODEL_NAME}), dim={dim}\")\n",
    "\n",
    "# ===== corpus ë¡œë“œ =====\n",
    "assert CHUNK_FILE.exists(), f\"ì²­í‚¹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CHUNK_FILE}\"\n",
    "\n",
    "corpus = []\n",
    "with open(CHUNK_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        corpus.append(json.loads(line))\n",
    "\n",
    "print(f\"[INFO] Loaded corpus: {len(corpus)} chunks\")\n",
    "\n",
    "client = None\n",
    "try:\n",
    "    # ===== Qdrant ì—°ê²° (ì„ë² ë””ë“œ ëª¨ë“œ) =====\n",
    "    client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "    # ì»¬ë ‰ì…˜ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œë§Œ)\n",
    "    if not client.collection_exists(COLLECTION):\n",
    "        client.create_collection(\n",
    "            collection_name=COLLECTION,\n",
    "            vectors_config=qmodels.VectorParams(\n",
    "                size=dim,\n",
    "                distance=qmodels.Distance.COSINE\n",
    "            ),\n",
    "            optimizers_config=qmodels.OptimizersConfigDiff(indexing_threshold=20000),\n",
    "            hnsw_config=qmodels.HnswConfigDiff(m=32, ef_construct=256),\n",
    "        )\n",
    "        print(f\"[INFO] Created collection: {COLLECTION}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Using existing collection: {COLLECTION}\")\n",
    "\n",
    "    # ===== ì—…ì„œíŠ¸ =====\n",
    "    pending_points = []\n",
    "    for i in tqdm(range(0, len(corpus), BATCH), desc=f\"Upserting ({MODEL_NAME})\"):\n",
    "        batch = corpus[i:i+BATCH]\n",
    "        texts = [x[\"text\"] for x in batch]\n",
    "\n",
    "        vecs = model.encode(\n",
    "            texts,\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            batch_size=BATCH,\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        pending_points.clear()\n",
    "        for x, v in zip(batch, vecs):\n",
    "            pid = str(uuid.uuid4())   # âœ… ë¬´ì¡°ê±´ ì˜¬ë°”ë¥¸ UUID ìƒì„±\n",
    "            payload = {**x.get(\"metadata\", {}), \"text\": x[\"text\"]}\n",
    "            pending_points.append(\n",
    "                qmodels.PointStruct(id=pid, vector=v.tolist(), payload=payload)\n",
    "            )\n",
    "\n",
    "\n",
    "        client.upsert(collection_name=COLLECTION, points=pending_points, wait=False)\n",
    "\n",
    "    try:\n",
    "        client.update_collection(\n",
    "            collection_name=COLLECTION,\n",
    "            hnsw_config=qmodels.HnswConfigDiff(ef_construct=256),\n",
    "            optimizers_config=qmodels.OptimizersConfigDiff(default_segment_number=4),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[INFO] Upsert done: {len(corpus)} â†’ DB path={QDRANT_PATH}\")\n",
    "\n",
    "finally:\n",
    "    if client is not None:\n",
    "        try:\n",
    "            client.close()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a07f17",
   "metadata": {},
   "source": [
    "## 2. Retriever ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a34639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2014ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ìœ ë™ìì‚°ì€ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ìì‚°ëŠ” 62,054,773ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2014ë…„, (score=0.8690)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ìì‚°ëŠ” 62,054,773ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2014ë…„, (score=0.8690)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ìì‚°ëŠ” 62,054,773ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2014ë…„, (score=0.8690)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "62,054,773\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2014ë…„ í˜„ê¸ˆíë¦„í‘œ ìƒ ë‹¹ê¸° ì˜ì—…í™œë™ í˜„ê¸ˆíë¦„ì€ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ì˜ì—…í™œë™í˜„ê¸ˆíë¦„ëŠ” 18,653,817ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2014ë…„, (score=0.8249)\n",
      "\n",
      "2ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ì˜ì—…í™œë™í˜„ê¸ˆíë¦„ëŠ” 18,653,817ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2014ë…„, (score=0.8249)\n",
      "\n",
      "3ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ì˜ì—…í™œë™í˜„ê¸ˆíë¦„ëŠ” 18,653,817ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2014ë…„, (score=0.8249)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "18,653,817\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2015ë…„ ë‹¹ê¸° ë¹„ìœ ë™ìì‚°ì€ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2015ë…„ (ë‹¹ê¸°) ë¹„ìœ ë™ìì‚°ëŠ” 101,967,575ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2015ë…„, (score=0.8659)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2015ë…„ (ë‹¹ê¸°) ë¹„ìœ ë™ìì‚°ëŠ” 101,967,575ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2015ë…„, (score=0.8659)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2015ë…„ (ë‹¹ê¸°) ë¹„ìœ ë™ìì‚°ëŠ” 101,967,575ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2015ë…„, (score=0.8659)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "101,967,575\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2015ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸°ìˆœì´ìµì€ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2015ë…„ (ë‹¹ê¸°) ë‹¹ê¸°ìˆœì´ìµëŠ” 12,238,469ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2015ë…„, (score=0.8365)\n",
      "\n",
      "2ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2015ë…„ (ë‹¹ê¸°) ë‹¹ê¸°ìˆœì´ìµëŠ” 12,238,469ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2015ë…„, (score=0.8365)\n",
      "\n",
      "3ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2015ë…„ (ë‹¹ê¸°) ë‹¹ê¸°ìˆœì´ìµëŠ” 12,238,469ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2015ë…„, (score=0.8365)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "12,238,469\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2016ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ë‹¨ê¸°ê¸ˆìœµìƒí’ˆì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ë‹¨ê¸°ê¸ˆìœµìƒí’ˆëŠ” 30,170,656ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 5, 6, 7, 31\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.8399)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ë‹¨ê¸°ê¸ˆìœµìƒí’ˆëŠ” 30,170,656ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 5, 6, 7, 31\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.8399)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ë‹¨ê¸°ê¸ˆìœµìƒí’ˆëŠ” 30,170,656ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 5, 6, 7, 31\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.8399)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "30,170,656\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2016ë…„ í¬ê´„ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ì´í¬ê´„ì´ìµì€ ì–¼ë§ˆë‹ˆ?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: í¬ê´„ì†ìµê³„ì‚°ì„œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ì´í¬ê´„ì´ìµëŠ” 11,887,806ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.8544)\n",
      "\n",
      "2ìœ„: í¬ê´„ì†ìµê³„ì‚°ì„œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ì´í¬ê´„ì´ìµëŠ” 11,887,806ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.8544)\n",
      "\n",
      "3ìœ„: í¬ê´„ì†ìµê³„ì‚°ì„œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ì´í¬ê´„ì´ìµëŠ” 11,887,806ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.8544)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "11,887,806\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2016ë…„ ìë³¸ë³€ë™í‘œ ìƒ ìê¸°ì£¼ì‹ì˜ ì·¨ë“ì€ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ìê¸°ì£¼ì‹ì˜ì·¨ë“ëŠ” -7,707,938ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.7548)\n",
      "\n",
      "2ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ìê¸°ì£¼ì‹ì˜ì·¨ë“ëŠ” -7,707,938ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.7548)\n",
      "\n",
      "3ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2016ë…„ (ë‹¹ê¸°) ìê¸°ì£¼ì‹ì˜ì·¨ë“ëŠ” -7,707,938ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2016ë…„, (score=0.7548)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "(7,707,938)\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2017ë…„ ë‹¹ê¸° ë§¤ì¶œì±„ê¶Œì€ ì¬ë¬´ìƒíƒœí‘œì— ë”°ë¥´ë©´ ì–¼ë§ˆëƒ?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2017ë…„ (ë‹¹ê¸°) ë§¤ì¶œì±„ê¶ŒëŠ” 27,881,777ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 6, 7, 10, 31\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.7888)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2017ë…„ (ë‹¹ê¸°) ë§¤ì¶œì±„ê¶ŒëŠ” 27,881,777ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 6, 7, 10, 31\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.7888)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2017ë…„ (ë‹¹ê¸°) ë§¤ì¶œì±„ê¶ŒëŠ” 27,881,777ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 6, 7, 10, 31\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.7888)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "27,881,777\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2017ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ì „ê¸° í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ì€ ì–¼ë§ˆì…ë‹ˆê¹Œ?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2017ë…„ (ë‹¹ê¸°) í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ëŠ” 2,763,768ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 4, 6, 7, 31\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.7735)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2017ë…„ (ë‹¹ê¸°) í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ëŠ” 2,763,768ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 4, 6, 7, 31\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.7735)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2017ë…„ (ë‹¹ê¸°) í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ëŠ” 2,763,768ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 4, 6, 7, 31\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.7735)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "3,778,371\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.00, Recall@3=0.00, F1@3=0.00, nDCG@3=0.00, MRR=0.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2018ë…„ ë‹¹ê¸° ë¯¸ìˆ˜ê¸ˆì€ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2018ë…„ (ë‹¹ê¸°) ë¯¸ìˆ˜ê¸ˆëŠ” 1,515,079ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2018ë…„, (score=0.8794)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2018ë…„ (ë‹¹ê¸°) ë¯¸ìˆ˜ê¸ˆëŠ” 1,515,079ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2018ë…„, (score=0.8794)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2018ë…„ (ë‹¹ê¸°) ë¯¸ìˆ˜ê¸ˆëŠ” 1,515,079ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2018ë…„, (score=0.8794)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "1,515,079\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2018ë…„ ì†ìµê³„ì‚°ì„œìƒ ë§¤ì¶œì´ì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2018ë…„ (ë‹¹ê¸°) ë§¤ì¶œì´ì´ìµëŠ” 68,715,364ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2018ë…„, (score=0.8518)\n",
      "\n",
      "2ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2018ë…„ (ë‹¹ê¸°) ë§¤ì¶œì´ì´ìµëŠ” 68,715,364ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2018ë…„, (score=0.8518)\n",
      "\n",
      "3ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2018ë…„ (ë‹¹ê¸°) ë§¤ì¶œì´ì´ìµëŠ” 68,715,364ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2018ë…„, (score=0.8518)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "68,715,364\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2019ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ì¢…ì†ê¸°ì—…, ê´€ê³„ê¸°ì—… ë° ê³µë™ê¸°ì—… íˆ¬ìëŠ” ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ì¢…ì†ê¸°ì—…,ê´€ê³„ê¸°ì—…ë°ê³µë™ê¸°ì—…íˆ¬ìëŠ” 56,571,252ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.8636)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ì¢…ì†ê¸°ì—…,ê´€ê³„ê¸°ì—…ë°ê³µë™ê¸°ì—…íˆ¬ìëŠ” 56,571,252ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.8636)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ì¢…ì†ê¸°ì—…,ê´€ê³„ê¸°ì—…ë°ê³µë™ê¸°ì—…íˆ¬ìëŠ” 56,571,252ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.8636)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "56,571,252\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2019ë…„ í˜„ê¸ˆíë¦„í‘œ ìƒ ì´ìµì‰ì—¬ê¸ˆ ë°°ë‹¹ì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ë°°ë‹¹ê¸ˆì˜ì§€ê¸‰ëŠ” -9,618,210ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.7412)\n",
      "\n",
      "2ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ë°°ë‹¹ê¸ˆì˜ì§€ê¸‰ëŠ” -9,618,210ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.7412)\n",
      "\n",
      "3ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ë°°ë‹¹ê¸ˆì˜ì§€ê¸‰ëŠ” -9,618,210ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.7412)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "(9,618,210)\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2019ë…„ ì†ìµê³„ì‚°ì„œìƒ ê¸°ë³¸ì£¼ë‹¹ì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ê¸°ë³¸ì£¼ë‹¹ì´ìµ(ë‹¨ìœ„:ì›)ëŠ” 2,260ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.8476)\n",
      "\n",
      "2ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ê¸°ë³¸ì£¼ë‹¹ì´ìµ(ë‹¨ìœ„:ì›)ëŠ” 2,260ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.8476)\n",
      "\n",
      "3ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2019ë…„ (ë‹¹ê¸°) ê¸°ë³¸ì£¼ë‹¹ì´ìµ(ë‹¨ìœ„:ì›)ëŠ” 2,260ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2019ë…„, (score=0.8476)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "2,260\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2020ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ìì‚°ì´ê³„ëŠ”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2020ë…„ (ë‹¹ê¸°) ìì‚°ì´ê³„ëŠ” 229,664,427ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2020ë…„, (score=0.8454)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2020ë…„ (ë‹¹ê¸°) ìì‚°ì´ê³„ëŠ” 229,664,427ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2020ë…„, (score=0.8454)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2020ë…„ (ë‹¹ê¸°) ìì‚°ì´ê³„ëŠ” 229,664,427ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2020ë…„, (score=0.8454)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "229,664,427\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2020ë…„ ì†ìµê³„ì‚°ì„œ ìƒ íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2020ë…„ (ë‹¹ê¸°) íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„ëŠ” 29,038,798ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2020ë…„, (score=0.9002)\n",
      "\n",
      "2ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2020ë…„ (ë‹¹ê¸°) íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„ëŠ” 29,038,798ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2020ë…„, (score=0.9002)\n",
      "\n",
      "3ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2020ë…„ (ë‹¹ê¸°) íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„ëŠ” 29,038,798ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2020ë…„, (score=0.9002)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "29,038,798\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2021ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ê¸°íƒ€í¬ê´„ì†ìµ-ê³µì •ê°€ì¹˜ê¸ˆìœµìì‚°ì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2021ë…„ (ë‹¹ê¸°) ê¸°íƒ€í¬ê´„ì†ìµ-ê³µì •ê°€ì¹˜ê¸ˆìœµìì‚°ëŠ” 1,662,532ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 4, 6, 28\n",
      "   ì¶œì²˜: 2021ë…„, (score=0.8443)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2021ë…„ (ë‹¹ê¸°) ê¸°íƒ€í¬ê´„ì†ìµ-ê³µì •ê°€ì¹˜ê¸ˆìœµìì‚°ëŠ” 1,662,532ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 4, 6, 28\n",
      "   ì¶œì²˜: 2021ë…„, (score=0.8443)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2021ë…„ (ë‹¹ê¸°) ê¸°íƒ€í¬ê´„ì†ìµ-ê³µì •ê°€ì¹˜ê¸ˆìœµìì‚°ëŠ” 1,662,532ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 4, 6, 28\n",
      "   ì¶œì²˜: 2021ë…„, (score=0.8443)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "1,662,532\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2021ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ê¸ˆìœµë¹„ìš©ì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2021ë…„ (ë‹¹ê¸°) ê¸ˆìœµë¹„ìš©ëŠ” 3,698,675ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2021ë…„, (score=0.8720)\n",
      "\n",
      "2ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2021ë…„ (ë‹¹ê¸°) ê¸ˆìœµë¹„ìš©ëŠ” 3,698,675ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2021ë…„, (score=0.8720)\n",
      "\n",
      "3ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2021ë…„ (ë‹¹ê¸°) ê¸ˆìœµë¹„ìš©ëŠ” 3,698,675ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2021ë…„, (score=0.8720)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "3,698,675\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2022ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë¹„ìœ ë™ë¶€ì±„ëŠ” ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2022ë…„ (ë‹¹ê¸°) ë¹„ìœ ë™ë¶€ì±„ëŠ” 4,581,512ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8602)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2022ë…„ (ë‹¹ê¸°) ë¹„ìœ ë™ë¶€ì±„ëŠ” 4,581,512ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8602)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2022ë…„ (ë‹¹ê¸°) ë¹„ìœ ë™ë¶€ì±„ëŠ” 4,581,512ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8602)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "4,581,512\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2022ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ë²•ì¸ì„¸ë¹„ìš©ì€ ì–¼ë§ˆë‹ˆ?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2022ë…„ (ë‹¹ê¸°) ë²•ì¸ì„¸ë¹„ìš©ëŠ” 4,273,142ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8786)\n",
      "\n",
      "2ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2022ë…„ (ë‹¹ê¸°) ë²•ì¸ì„¸ë¹„ìš©ëŠ” 4,273,142ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8786)\n",
      "\n",
      "3ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2022ë…„ (ë‹¹ê¸°) ë²•ì¸ì„¸ë¹„ìš©ëŠ” 4,273,142ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8786)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "4,273,142\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2022ë…„ ë‹¹ê¸° í˜„ê¸ˆíë¦„í‘œ ìƒ íˆ¬ìí™œë™ í˜„ê¸ˆíë¦„ì€ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2022ë…„ (ë‹¹ê¸°) íˆ¬ìí™œë™í˜„ê¸ˆíë¦„ëŠ” -28,123,886ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8411)\n",
      "\n",
      "2ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2022ë…„ (ë‹¹ê¸°) íˆ¬ìí™œë™í˜„ê¸ˆíë¦„ëŠ” -28,123,886ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8411)\n",
      "\n",
      "3ìœ„: í˜„ê¸ˆíë¦„í‘œì—ì„œ 2022ë…„ (ë‹¹ê¸°) íˆ¬ìí™œë™í˜„ê¸ˆíë¦„ëŠ” -28,123,886ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2022ë…„, (score=0.8411)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "(28,123,886)\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2023ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ì¬ê³ ìì‚°ì€ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2023ë…„ (ë‹¹ê¸°) ì¬ê³ ìì‚°ëŠ” 29,338,151ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 8\n",
      "   ì¶œì²˜: 2023ë…„, (score=0.8332)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2023ë…„ (ë‹¹ê¸°) ì¬ê³ ìì‚°ëŠ” 29,338,151ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 8\n",
      "   ì¶œì²˜: 2023ë…„, (score=0.8332)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2023ë…„ (ë‹¹ê¸°) ì¬ê³ ìì‚°ëŠ” 29,338,151ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 8\n",
      "   ì¶œì²˜: 2023ë…„, (score=0.8332)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "29,338,151\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2023ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ì˜ì—…ì´ìµì€ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2023ë…„ (ë‹¹ê¸°) ì˜ì—…ì´ìµ(ì†ì‹¤)ëŠ” -11,526,297ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2023ë…„, (score=0.8269)\n",
      "\n",
      "2ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2023ë…„ (ë‹¹ê¸°) ì˜ì—…ì´ìµ(ì†ì‹¤)ëŠ” -11,526,297ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2023ë…„, (score=0.8269)\n",
      "\n",
      "3ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2023ë…„ (ë‹¹ê¸°) ì˜ì—…ì´ìµ(ì†ì‹¤)ëŠ” -11,526,297ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2023ë…„, (score=0.8269)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "(11,526,297)\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2024ë…„ì—ëŠ” ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë¬´í˜•ìì‚°ì´ ì–¼ë§ˆì•¼?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ë¬´í˜•ìì‚°ëŠ” 10,496,956ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 11\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.7829)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ë¬´í˜•ìì‚°ëŠ” 10,496,956ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 11\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.7829)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ë¬´í˜•ìì‚°ëŠ” 10,496,956ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 11\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.7829)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "10,496,956\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2024ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ìš°ì„ ì£¼ìë³¸ê¸ˆì€ ì–¼ë§ˆì¸ê°€?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ìš°ì„ ì£¼ìë³¸ê¸ˆëŠ” 119,467ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.8726)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ìš°ì„ ì£¼ìë³¸ê¸ˆëŠ” 119,467ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.8726)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ìš°ì„ ì£¼ìë³¸ê¸ˆëŠ” 119,467ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.8726)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "119,467\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2024ë…„ ì†ìµê³„ì‚°ì„œìƒ ë‹¹ê¸° ë²•ì¸ì„¸ë¹„ìš©ì€ ì–¼ë§ˆì•¼?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ë²•ì¸ì„¸ë¹„ìš©(ìˆ˜ìµ)ëŠ” -1,832,987ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.8317)\n",
      "\n",
      "2ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ë²•ì¸ì„¸ë¹„ìš©(ìˆ˜ìµ)ëŠ” -1,832,987ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.8317)\n",
      "\n",
      "3ìœ„: ì†ìµê³„ì‚°ì„œì—ì„œ 2024ë…„ (ë‹¹ê¸°) ë²•ì¸ì„¸ë¹„ìš©(ìˆ˜ìµ)ëŠ” -1,832,987ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2024ë…„, (score=0.8317)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "(1,832,987)\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.33, Recall@3=1.00, F1@3=0.50, nDCG@3=1.00, MRR=1.00\n",
      "====================================================================================================\n",
      "ì§ˆë¬¸: 2017ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë§¤ê°ì˜ˆì •ë¶„ë¥˜ìì‚°ì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
      "\n",
      "[ê²€ìƒ‰ ê²°ê³¼ Top-3]\n",
      "1ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2016ë…„ (ì „ê¸°) ë§¤ê°ì˜ˆì •ë¶„ë¥˜ìì‚°ëŠ” 283,690ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.6824)\n",
      "\n",
      "2ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2016ë…„ (ì „ê¸°) ë§¤ê°ì˜ˆì •ë¶„ë¥˜ìì‚°ëŠ” 283,690ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.6824)\n",
      "\n",
      "3ìœ„: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2016ë…„ (ì „ê¸°) ë§¤ê°ì˜ˆì •ë¶„ë¥˜ìì‚°ëŠ” 283,690ë°±ë§Œì›ì…ë‹ˆë‹¤.\n",
      "   ì¶œì²˜: 2017ë…„, (score=0.6824)\n",
      "\n",
      "\n",
      "[ì •ë‹µ]\n",
      "-\n",
      "\n",
      "[ì„±ëŠ¥ ì§€í‘œ]\n",
      " Precision@3=0.00, Recall@3=0.00, F1@3=0.00, nDCG@3=0.00, MRR=0.00\n",
      "\n",
      "====================================================================================================\n",
      "=== í‰ê·  ì„±ëŠ¥ ì§€í‘œ ===\n",
      "Precision@3    0.309\n",
      "Recall@3       0.926\n",
      "F1@3           0.463\n",
      "MRR            0.926\n",
      "nDCG@3         0.926\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "\n",
    "# ===== ì„ë² ë”© ëª¨ë¸ ë¡œë“œ =====\n",
    "embed_model = SentenceTransformer(\"dragonkue/bge-m3-ko\")\n",
    "\n",
    "# ===== ìˆ«ìë§Œ ì¶”ì¶œ í•¨ìˆ˜ =====\n",
    "def extract_numbers(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return \"\".join(re.findall(r\"\\d+\", text.replace(\",\", \"\")))\n",
    "\n",
    "# ===== Retriever í•¨ìˆ˜ =====\n",
    "def extract_year_from_query(query: str):\n",
    "    \"\"\"ì§ˆë¬¸ì—ì„œ ì—°ë„(4ìë¦¬ ìˆ«ì) ì¶”ì¶œ\"\"\"\n",
    "    match = re.search(r\"(20\\d{2}|19\\d{2})\", query)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "    \n",
    "def dense_search(query: str, model, client, collection_name: str,\n",
    "                 top_k: int = 3, ground_truth=None):\n",
    "    \"\"\"\n",
    "    Qdrant query_points ê¸°ë°˜ Dense Retriever í•¨ìˆ˜ + ì„±ëŠ¥ì§€í‘œ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    # 0. ì§ˆë¬¸ì—ì„œ ì—°ë„ ì¶”ì¶œ\n",
    "    year = extract_year_from_query(query)\n",
    "    \n",
    "    # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "    qv = model.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "    # 2. Qdrant ê²€ìƒ‰ ì‹¤í–‰ (ì—°ë„ í•„í„° ìˆìœ¼ë©´ ì ìš©)\n",
    "    query_filter = None\n",
    "    if year:\n",
    "        query_filter = qmodels.Filter(\n",
    "            must=[\n",
    "                qmodels.FieldCondition(\n",
    "                    key=\"report_year\",\n",
    "                    match=qmodels.MatchValue(value=year)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=query_filter  # âœ… í•„í„° ì ìš©\n",
    "    )\n",
    "\n",
    "    # 3. ê²°ê³¼ ì •ë¦¬ (payload ì „ì²´ ë°˜ì˜)\n",
    "    output = []\n",
    "    for r in results.points:\n",
    "        payload = r.payload or {}\n",
    "        result_item = {\n",
    "            \"score\": r.score,\n",
    "            \"text\": payload.get(\"text\")\n",
    "        }\n",
    "        # metadata ì•ˆì˜ ëª¨ë“  í‚¤-ê°’ì„ ì¶”ê°€\n",
    "        if \"metadata\" in payload:\n",
    "            result_item.update(payload[\"metadata\"])\n",
    "        else:\n",
    "            # í˜¹ì‹œ metadata í‚¤ ì—†ì´ flatí•˜ê²Œ ë“¤ì–´ì˜¨ ê²½ìš°\n",
    "            result_item.update(payload)\n",
    "        output.append(result_item)\n",
    "\n",
    "    # 4. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "    metrics = {}\n",
    "    if ground_truth:\n",
    "        # ì •ë‹µ ìˆ«ìë§Œ ì¶”ì¶œ\n",
    "        normalized_gt = [extract_numbers(gt) for gt in ground_truth]\n",
    "\n",
    "        # ê° ê²€ìƒ‰ ê²°ê³¼ê°€ ì •ë‹µê³¼ ë§¤ì¹­ë˜ëŠ”ì§€ ì—¬ë¶€\n",
    "        used = set()\n",
    "        relevances = []\n",
    "        for r in output:\n",
    "            nums = extract_numbers(r[\"text\"])\n",
    "            hit = 0\n",
    "            for gt in normalized_gt:\n",
    "                if gt and gt in nums and gt not in used:\n",
    "                    hit = 1\n",
    "                    used.add(gt)\n",
    "                    break\n",
    "            relevances.append(hit)\n",
    "\n",
    "        # Precision@3\n",
    "        precision = sum(relevances) / top_k if top_k > 0 else 0.0\n",
    "        metrics[\"Precision@3\"] = precision\n",
    "\n",
    "        # Recall@3 (ì „ì²´ ì •ë‹µ ëŒ€ë¹„ ë¹„ìœ¨)\n",
    "        recall = min(sum(relevances), len(normalized_gt)) / len(normalized_gt) if normalized_gt else 0.0\n",
    "        metrics[\"Recall@3\"] = recall\n",
    "\n",
    "        # F1@3\n",
    "        if precision + recall > 0:\n",
    "            metrics[\"F1@3\"] = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            metrics[\"F1@3\"] = 0.0\n",
    "\n",
    "        # MRR\n",
    "        rr = 0.0\n",
    "        for rank, rel in enumerate(relevances, 1):\n",
    "            if rel == 1:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        metrics[\"MRR\"] = rr\n",
    "\n",
    "        # nDCG@k\n",
    "        dcg = sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
    "        ideal_hits = min(len(normalized_gt), top_k)   # ìµœëŒ€ ì •ë‹µ ìˆ˜\n",
    "        idcg = sum(1.0 / np.log2(idx + 2) for idx in range(ideal_hits))\n",
    "        metrics[\"nDCG@3\"] = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    return output, metrics\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    QDRANT_PATH = \"/Users/dan/Desktop/snu_project/git_ì œì¶œìš©/data/vector_store/final-sjchunk/bge-ko-qdrant_db\"\n",
    "    client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "    collection_name = \"audit_chunks\"\n",
    "\n",
    "    questions = [\n",
    "    \"2014ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ìœ ë™ìì‚°ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2014ë…„ í˜„ê¸ˆíë¦„í‘œ ìƒ ë‹¹ê¸° ì˜ì—…í™œë™ í˜„ê¸ˆíë¦„ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2015ë…„ ë‹¹ê¸° ë¹„ìœ ë™ìì‚°ì€ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2015ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸°ìˆœì´ìµì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2016ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ë‹¨ê¸°ê¸ˆìœµìƒí’ˆì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"2016ë…„ í¬ê´„ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ì´í¬ê´„ì´ìµì€ ì–¼ë§ˆë‹ˆ?\",\n",
    "    \"2016ë…„ ìë³¸ë³€ë™í‘œ ìƒ ìê¸°ì£¼ì‹ì˜ ì·¨ë“ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2017ë…„ ë‹¹ê¸° ë§¤ì¶œì±„ê¶Œì€ ì¬ë¬´ìƒíƒœí‘œì— ë”°ë¥´ë©´ ì–¼ë§ˆëƒ?\",\n",
    "    \"2017ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ì „ê¸° í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ì€ ì–¼ë§ˆì…ë‹ˆê¹Œ?\",\n",
    "    \"2018ë…„ ë‹¹ê¸° ë¯¸ìˆ˜ê¸ˆì€ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2018ë…„ ì†ìµê³„ì‚°ì„œìƒ ë§¤ì¶œì´ì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"2019ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ì¢…ì†ê¸°ì—…, ê´€ê³„ê¸°ì—… ë° ê³µë™ê¸°ì—… íˆ¬ìëŠ” ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"2019ë…„ í˜„ê¸ˆíë¦„í‘œ ìƒ ì´ìµì‰ì—¬ê¸ˆ ë°°ë‹¹ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"2019ë…„ ì†ìµê³„ì‚°ì„œìƒ ê¸°ë³¸ì£¼ë‹¹ì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"2020ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ìì‚°ì´ê³„ëŠ”?\",\n",
    "    \"2020ë…„ ì†ìµê³„ì‚°ì„œ ìƒ íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"2021ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ê¸°íƒ€í¬ê´„ì†ìµ-ê³µì •ê°€ì¹˜ê¸ˆìœµìì‚°ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    #\"2021ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ìœ ë™ë¹„ìœ¨ì„ ê³„ì‚°í•˜ë©´ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"2021ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ê¸ˆìœµë¹„ìš©ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"2022ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë¹„ìœ ë™ë¶€ì±„ëŠ” ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2022ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ë²•ì¸ì„¸ë¹„ìš©ì€ ì–¼ë§ˆë‹ˆ?\",\n",
    "    \"2022ë…„ ë‹¹ê¸° í˜„ê¸ˆíë¦„í‘œ ìƒ íˆ¬ìí™œë™ í˜„ê¸ˆíë¦„ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2023ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ì¬ê³ ìì‚°ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2023ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ì˜ì—…ì´ìµì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2024ë…„ì—ëŠ” ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë¬´í˜•ìì‚°ì´ ì–¼ë§ˆì•¼?\",\n",
    "    \"2024ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ìš°ì„ ì£¼ìë³¸ê¸ˆì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "    \"2024ë…„ ì†ìµê³„ì‚°ì„œìƒ ë‹¹ê¸° ë²•ì¸ì„¸ë¹„ìš©ì€ ì–¼ë§ˆì•¼?\",\n",
    "    \"2017ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë§¤ê°ì˜ˆì •ë¶„ë¥˜ìì‚°ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    ]\n",
    "    \n",
    "    answers = [\n",
    "        \"62,054,773\",\n",
    "        \"18,653,817\",\n",
    "        \"101,967,575\",\n",
    "        \"12,238,469\",\n",
    "        \"30,170,656\",\n",
    "        \"11,887,806\",\n",
    "        \"(7,707,938)\",\n",
    "        \"27,881,777\",\n",
    "        \"3,778,371\",\n",
    "        \"1,515,079\",\n",
    "        \"68,715,364\",\n",
    "        \"56,571,252\",\n",
    "        \"(9,618,210)\",\n",
    "        \"2,260\",\n",
    "        \"229,664,427\",\n",
    "        \"29,038,798\",\n",
    "        \"1,662,532\",\n",
    "        #\"1.38\",\n",
    "        \"3,698,675\",\n",
    "        \"4,581,512\",\n",
    "        \"4,273,142\",\n",
    "        \"(28,123,886)\",\n",
    "        \"29,338,151\",\n",
    "        \"(11,526,297)\",\n",
    "        \"10,496,956\",\n",
    "        \"119,467\",\n",
    "        \"(1,832,987)\",\n",
    "        \"-\"\n",
    "    ]\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    for q, a in zip(questions, answers):\n",
    "        results, metrics = dense_search(\n",
    "            query=q,\n",
    "            model=embed_model,\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            top_k=3,\n",
    "            ground_truth=[a]\n",
    "        )\n",
    "\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"ì§ˆë¬¸: {q}\")\n",
    "\n",
    "        # Top-3 ê²°ê³¼ ëª¨ë‘ ì¶œë ¥\n",
    "        if results:\n",
    "            print(\"\\n[ê²€ìƒ‰ ê²°ê³¼ Top-3]\")\n",
    "            for rank, r in enumerate(results, 1):\n",
    "                print(f\"{rank}ìœ„: {r['text']}\")\n",
    "                print(f\"   ì¶œì²˜: {r['report_year']}ë…„, (score={r['score']:.4f})\\n\")\n",
    "        else:\n",
    "            print(\"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "\n",
    "        print(\"\\n[ì •ë‹µ]\")\n",
    "        print(a)\n",
    "\n",
    "        print(\"\\n[ì„±ëŠ¥ ì§€í‘œ]\")\n",
    "        print(\n",
    "            f\" Precision@3={metrics.get('Precision@3', 0):.2f},\"\n",
    "            f\" Recall@3={metrics.get('Recall@3', 0):.2f},\"\n",
    "            f\" F1@3={metrics.get('F1@3', 0):.2f},\"\n",
    "            f\" nDCG@3={metrics.get('nDCG@3', 0):.2f},\"\n",
    "            f\" MRR={metrics.get('MRR', 0):.2f}\"\n",
    "        )\n",
    "\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    # í‰ê·  ì„±ëŠ¥ ì§€í‘œ\n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"=== í‰ê·  ì„±ëŠ¥ ì§€í‘œ ===\")\n",
    "    print(df.mean().round(3))\n",
    "\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eba035-0b06-4606-86ca-99afa9140553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ’¬ ì§ˆë¬¸: 2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ìœ ë™ìì‚°ì˜ í•˜ìœ„ê³„ì¸µ ì •ë³´ë¥¼ ì „ë¶€ ì¤˜. metadataì—ì„œ hierarchy / levelì„ ê¼­ ì°¸ê³ í•´.\n",
      "\n",
      "ğŸ” ê²€ìƒ‰ ê²°ê³¼ (10ê°œ):\n",
      "====================================================================================================\n",
      "\n",
      "1. ìŠ¤ì½”ì–´: 0.6022\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ìì‚°ëŠ” 62,054,773ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ìœ ë™ìì‚°, account_name: ìœ ë™ìì‚°, parent_id: ìì‚°, level: 2, hierarchy: ['ìì‚°', 'ìœ ë™ìì‚°'], is_total: False, is_subtotal: False, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. ìŠ¤ì½”ì–´: 0.6022\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ìì‚°ëŠ” 62,054,773ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ìœ ë™ìì‚°, account_name: ìœ ë™ìì‚°, parent_id: ìì‚°, level: 2, hierarchy: ['ìì‚°', 'ìœ ë™ìì‚°'], is_total: False, is_subtotal: False, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. ìŠ¤ì½”ì–´: 0.6022\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ìì‚°ëŠ” 62,054,773ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ìœ ë™ìì‚°, account_name: ìœ ë™ìì‚°, parent_id: ìì‚°, level: 2, hierarchy: ['ìì‚°', 'ìœ ë™ìì‚°'], is_total: False, is_subtotal: False, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. ìŠ¤ì½”ì–´: 0.5628\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ë¶€ì±„ëŠ” 28,208,638ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ë¶€ì±„_ìœ ë™ë¶€ì±„, account_name: ìœ ë™ë¶€ì±„, parent_id: ë¶€ì±„, level: 2, hierarchy: ['ë¶€ì±„', 'ìœ ë™ë¶€ì±„'], is_total: False, is_subtotal: False, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. ìŠ¤ì½”ì–´: 0.5628\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ë¶€ì±„ëŠ” 28,208,638ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ë¶€ì±„_ìœ ë™ë¶€ì±„, account_name: ìœ ë™ë¶€ì±„, parent_id: ë¶€ì±„, level: 2, hierarchy: ['ë¶€ì±„', 'ìœ ë™ë¶€ì±„'], is_total: False, is_subtotal: False, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "6. ìŠ¤ì½”ì–´: 0.5628\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ë¶€ì±„ëŠ” 28,208,638ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ë¶€ì±„_ìœ ë™ë¶€ì±„, account_name: ìœ ë™ë¶€ì±„, parent_id: ë¶€ì±„, level: 2, hierarchy: ['ë¶€ì±„', 'ìœ ë™ë¶€ì±„'], is_total: False, is_subtotal: False, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "7. ìŠ¤ì½”ì–´: 0.5589\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ì„±ì‚¬ì±„ëŠ” 5,304ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 6, 16...\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ë¶€ì±„_ìœ ë™ë¶€ì±„_ìœ ë™ì„±ì‚¬ì±„, account_name: ìœ ë™ì„±ì‚¬ì±„, parent_id: ë¶€ì±„_ìœ ë™ë¶€ì±„, level: 3, hierarchy: ['ìœ ë™ë¶€ì±„', 'ìœ ë™ì„±ì‚¬ì±„'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "8. ìŠ¤ì½”ì–´: 0.5589\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ì„±ì‚¬ì±„ëŠ” 5,304ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 6, 16...\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ë¶€ì±„_ìœ ë™ë¶€ì±„_ìœ ë™ì„±ì‚¬ì±„, account_name: ìœ ë™ì„±ì‚¬ì±„, parent_id: ë¶€ì±„_ìœ ë™ë¶€ì±„, level: 3, hierarchy: ['ìœ ë™ë¶€ì±„', 'ìœ ë™ì„±ì‚¬ì±„'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "9. ìŠ¤ì½”ì–´: 0.5589\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ ë™ì„±ì‚¬ì±„ëŠ” 5,304ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 6, 16...\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ë¶€ì±„_ìœ ë™ë¶€ì±„_ìœ ë™ì„±ì‚¬ì±„, account_name: ìœ ë™ì„±ì‚¬ì±„, parent_id: ë¶€ì±„_ìœ ë™ë¶€ì±„, level: 3, hierarchy: ['ìœ ë™ë¶€ì±„', 'ìœ ë™ì„±ì‚¬ì±„'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "10. ìŠ¤ì½”ì–´: 0.5572\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ê¸°íƒ€ìœ ë™ìì‚°ëŠ” 821,079ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ìœ ë™ìì‚°_ê¸°íƒ€ìœ ë™ìì‚°, account_name: ê¸°íƒ€ìœ ë™ìì‚°, parent_id: ìì‚°_ìœ ë™ìì‚°, level: 3, hierarchy: ['ìœ ë™ìì‚°', 'ê¸°íƒ€ìœ ë™ìì‚°'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“‹ ìš”ì•½:\n",
      "   - ì´ 10ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "   - ê´€ë ¨ ì—°ë„: 2014\n",
      "   - ë°œê²¬ëœ ê³„ì¸µ ì •ë³´: 10ê°œ\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# from llama_cpp import Llama  # í•˜ë‹¨ì—ì„œ import\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "\n",
    "# ===== ì„ë² ë”© ëª¨ë¸ ë¡œë“œ =====\n",
    "embed_model = SentenceTransformer(\"dragonkue/bge-m3-ko\")\n",
    "\n",
    "# ===== Qdrant í´ë¼ì´ì–¸íŠ¸ =====\n",
    "QDRANT_PATH = \"/Users/dan/Desktop/snu_project/git_ì œì¶œìš©/data/vector_store/final-sjchunk/bge-ko-qdrant_db\"\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "collection_name = \"audit_chunks\"\n",
    "\n",
    "# ===== dense_search í•¨ìˆ˜ import =====\n",
    "# (ì˜ˆë¹ˆì”¨ê°€ ì´ì „ì— ì •ì˜í•œ dense_search ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •)\n",
    "def extract_year_from_query(query: str):\n",
    "    \"\"\"ì§ˆë¬¸ì—ì„œ ì—°ë„(4ìë¦¬ ìˆ«ì) ì¶”ì¶œ\"\"\"\n",
    "    match = re.search(r\"(20\\d{2}|19\\d{2})\", query)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_numbers(text: str) -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return \"\".join(re.findall(r\"\\d+\", text.replace(\",\", \"\")))\n",
    "    \n",
    "def dense_search(query: str, model, client, collection_name: str,\n",
    "                 top_k: int = 50, ground_truth=None):\n",
    "    \"\"\"\n",
    "    Qdrant query_points ê¸°ë°˜ Dense Retriever í•¨ìˆ˜ + ì„±ëŠ¥ì§€í‘œ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    # 0. ì§ˆë¬¸ì—ì„œ ì—°ë„ ì¶”ì¶œ\n",
    "    year = extract_year_from_query(query)\n",
    "    \n",
    "    # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "    qv = model.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "    # 2. Qdrant ê²€ìƒ‰ ì‹¤í–‰ (ì—°ë„ í•„í„° ìˆìœ¼ë©´ ì ìš©)\n",
    "    query_filter = None\n",
    "    if year:\n",
    "        query_filter = qmodels.Filter(\n",
    "            must=[\n",
    "                qmodels.FieldCondition(\n",
    "                    key=\"report_year\",\n",
    "                    match=qmodels.MatchValue(value=year)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=query_filter  # âœ… í•„í„° ì ìš©\n",
    "    )\n",
    "\n",
    "    # 3. ê²°ê³¼ ì •ë¦¬ (payload ì „ì²´ ë°˜ì˜)\n",
    "    output = []\n",
    "    for r in results.points:\n",
    "        payload = r.payload or {}\n",
    "        result_item = {\n",
    "            \"score\": r.score,\n",
    "            \"text\": payload.get(\"text\")\n",
    "        }\n",
    "        # metadata ì•ˆì˜ ëª¨ë“  í‚¤-ê°’ì„ ì¶”ê°€\n",
    "        if \"metadata\" in payload:\n",
    "            result_item.update(payload[\"metadata\"])\n",
    "        else:\n",
    "            # í˜¹ì‹œ metadata í‚¤ ì—†ì´ flatí•˜ê²Œ ë“¤ì–´ì˜¨ ê²½ìš°\n",
    "            result_item.update(payload)\n",
    "        output.append(result_item)\n",
    "\n",
    "    # 4. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "    metrics = {}\n",
    "    if ground_truth:\n",
    "        # ì •ë‹µ ìˆ«ìë§Œ ì¶”ì¶œ\n",
    "        normalized_gt = [extract_numbers(gt) for gt in ground_truth]\n",
    "\n",
    "        # ê° ê²€ìƒ‰ ê²°ê³¼ê°€ ì •ë‹µê³¼ ë§¤ì¹­ë˜ëŠ”ì§€ ì—¬ë¶€\n",
    "        used = set()\n",
    "        relevances = []\n",
    "        for r in output:\n",
    "            nums = extract_numbers(r[\"text\"])\n",
    "            hit = 0\n",
    "            for gt in normalized_gt:\n",
    "                if gt and gt in nums and gt not in used:\n",
    "                    hit = 1\n",
    "                    used.add(gt)\n",
    "                    break\n",
    "            relevances.append(hit)\n",
    "\n",
    "        # Precision@3\n",
    "        precision = sum(relevances) / top_k if top_k > 0 else 0.0\n",
    "        metrics[\"Precision@3\"] = precision\n",
    "\n",
    "        # Recall@3 (ì „ì²´ ì •ë‹µ ëŒ€ë¹„ ë¹„ìœ¨)\n",
    "        recall = min(sum(relevances), len(normalized_gt)) / len(normalized_gt) if normalized_gt else 0.0\n",
    "        metrics[\"Recall@3\"] = recall\n",
    "\n",
    "        # F1@3\n",
    "        if precision + recall > 0:\n",
    "            metrics[\"F1@3\"] = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            metrics[\"F1@3\"] = 0.0\n",
    "\n",
    "        # MRR\n",
    "        rr = 0.0\n",
    "        for rank, rel in enumerate(relevances, 1):\n",
    "            if rel == 1:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        metrics[\"MRR\"] = rr\n",
    "\n",
    "        # nDCG@k\n",
    "        dcg = sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
    "        ideal_hits = min(len(normalized_gt), top_k)   # ìµœëŒ€ ì •ë‹µ ìˆ˜\n",
    "        idcg = sum(1.0 / np.log2(idx + 2) for idx in range(ideal_hits))\n",
    "        metrics[\"nDCG@3\"] = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    return output, metrics\n",
    "    \n",
    "# ===== Zephyr ëª¨ë¸ ë¡œë“œ =====\n",
    "model_path = Path(\"/Users/dan/Desktop/snu_project/models/zephyr-7b-beta.Q4_K_M.gguf\").resolve()\n",
    "\n",
    "# from llama_cpp import Llama  # Cell 6ì—ì„œ ë¡œë“œë¨\n",
    "# llm = Llama(\n",
    "#     model_path=str(model_path),\n",
    "#     n_ctx=4096,\n",
    "#     n_threads=8,\n",
    "#     n_gpu_layers=35\n",
    "# )\n",
    "\n",
    "# ===== ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ =====\n",
    "def simple_search_test(query: str, model, client, collection_name: str, top_k: int = 10):\n",
    "    \"\"\"LLM ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ í™•ì¸í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    results, _ = dense_search(\n",
    "        query=query,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. ìŠ¤ì½”ì–´: {r['score']:.4f}\")\n",
    "        print(f\"   ì—°ë„: {r.get('report_year', 'N/A')}\")\n",
    "        print(f\"   í…ìŠ¤íŠ¸: {r['text'][:200]}...\")\n",
    "        \n",
    "        # metadata ì •ë³´ ì¶œë ¥\n",
    "        metadata_keys = ['account_id', 'account_name', 'parent_id', 'level', 'hierarchy', \n",
    "                        'is_total', 'is_subtotal', 'period_type', 'statement_type']\n",
    "        metadata_info = []\n",
    "        for key in metadata_keys:\n",
    "            if key in r:\n",
    "                metadata_info.append(f\"{key}: {r[key]}\")\n",
    "        if metadata_info:\n",
    "            print(f\"   ğŸ“Š ë©”íƒ€ë°ì´í„°: {', '.join(metadata_info)}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ===== RAG Pipeline (LLM ì—†ì´ ê²€ìƒ‰ë§Œ) =====\n",
    "def rag_pipeline_simple(query: str, model, client, collection_name: str, top_k: int = 10):\n",
    "    \"\"\"LLM ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜í•˜ëŠ” ê°„ë‹¨í•œ RAG\"\"\"\n",
    "    print(f\"ğŸ’¬ ì§ˆë¬¸: {query}\")\n",
    "    print()\n",
    "    \n",
    "    # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    results = simple_search_test(query, model, client, collection_name, top_k)\n",
    "    \n",
    "    # ê°„ë‹¨í•œ ìš”ì•½ ì •ë³´\n",
    "    print(f\"\\nğŸ“‹ ìš”ì•½:\")\n",
    "    print(f\"   - ì´ {len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    if results:\n",
    "        years = list(set([r.get('report_year', 'N/A') for r in results if r.get('report_year')]))\n",
    "        if years:\n",
    "            print(f\"   - ê´€ë ¨ ì—°ë„: {', '.join(map(str, sorted(years)))}\")\n",
    "        \n",
    "        # ê³„ì¸µ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°\n",
    "        hierarchies = [r.get('hierarchy', '') for r in results if r.get('hierarchy')]\n",
    "                 if hierarchies:\n",
    "             print(f\"   - ë°œê²¬ëœ ê³„ì¸µ ì •ë³´: {len(hierarchies)}ê°œ\")\n",
    "     \n",
    "     return results\n",
    "\n",
    "# ===== RAG Pipeline =====\n",
    "def rag_pipeline(query: str, model, client, collection_name: str, top_k: int = 3):\n",
    "    # 1) Retriever ë‹¨ê³„\n",
    "    results, _ = dense_search(\n",
    "        query=query,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    # 2) ê²€ìƒ‰ ê²°ê³¼ í•©ì¹˜ê¸°\n",
    "    context_text = \"\\n\".join([r[\"text\"] for r in results if r.get(\"text\")])\n",
    "\n",
    "    # 3) Reader í˜¸ì¶œ (Zephyr LLM) - íŠœë‹ëœ í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "    prompt = f\"\"\"<|system|>\n",
    "ë„ˆëŠ” ì¬ë¬´ë³´ê³ ì„œ ì „ë¬¸ê°€ì´ì ë°ì´í„° êµ¬ì¡°í™” ì „ë¬¸ê°€ë‹¤. \n",
    "ë„ˆì˜ ì„ë¬´ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì‚¬ìš©ìê°€ ìš”ì²­í•œ í•­ëª©ì„ \n",
    "metadataì˜ account_id, account_name, parent_id, is_total, is_subtotal, period_type,\n",
    "hierarchy, level ì •ë³´ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ê³„ì¸µ êµ¬ì¡°ë¥¼ ë°˜ì˜í•´ í‘œë¡œ ì •ë¦¬í•˜ëŠ” ê²ƒì´ë‹¤.\n",
    "\n",
    "ìš”êµ¬ì‚¬í•­:\n",
    "1. ë°˜ë“œì‹œ metadataì˜ account_id, account_name, parent_id, is_total, is_subtotal, period_type, hierarchy, level ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•˜ë¼.\n",
    "2. account_idì™€ account_nameìœ¼ë¡œ í•­ëª©ì„ ì‹ë³„í•˜ê³ , parent_idë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„-í•˜ìœ„ ê´€ê³„ë¥¼ ì—°ê²°í•˜ë¼.\n",
    "3. is_totalê³¼ is_subtotalì€ í•©ê³„/ì†Œê³„ ì—¬ë¶€ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ë¼.\n",
    "4. period_typeì€ \"ë‹¹ê¸°/ì „ê¸°/ëˆ„ì \" ë“±ì˜ ê¸°ê°„ êµ¬ë¶„ì„ ë°˜ë“œì‹œ í‘œì— í¬í•¨í•˜ë¼.\n",
    "5. level ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ í•˜ìœ„ í•­ëª©ì´ë¯€ë¡œ ë“¤ì—¬ì“°ê¸°ë¥¼ ì ìš©í•˜ê±°ë‚˜, í‘œì—ì„œ level ì—´ì„ í™œìš©í•˜ë¼.\n",
    "6. ì¶œë ¥ì€ ë°˜ë“œì‹œ í‘œ í˜•ì‹ìœ¼ë¡œ: \n",
    "   | account_id | account_name | parent_id | level | hierarchy | ê°’(ë°±ë§Œì›) | is_total | is_subtotal | period_type |\n",
    "7. ëª¨ë“  í•­ëª©ì„ ë¹ ì§ì—†ì´ ë³´ì—¬ì£¼ê³ , ì¶”ì¸¡í•˜ì§€ ë§ê³  ê²€ìƒ‰ëœ ë¬¸ì„œì™€ metadataë§Œ ê·¼ê±°ë¡œ ì‘ì„±í•˜ë¼.\n",
    "8. ë§Œì•½ ìœ ë™ìì‚°ê³¼ ê´€ë ¨ëœ ìœ ë™ë¶€ì±„ê°€ í•¨ê»˜ ì œê³µëœë‹¤ë©´, ìœ ë™ë¹„ìœ¨(Current Ratio = ìœ ë™ìì‚° Ã· ìœ ë™ë¶€ì±„)ì„ ê³„ì‚°í•˜ì—¬ í‘œ ë§¨ ì•„ë˜ì— ì¶”ê°€í•˜ë¼.\n",
    "9. ì¶œì²˜ëŠ” ë©”íƒ€ë°ì´í„°ì—ì„œ report_yearë¥¼ ì‚¬ìš©í•´ì„œ ì ì–´ë¼.\n",
    "10. ì ˆëŒ€ë¡œ account_nameì„ ì§€ì–´ì„œ ë§Œë“¤ì–´ë‚´ì§€ ë§ë¼.\n",
    "</s>\n",
    "<|user|>\n",
    "ë‹¤ìŒì€ ê²€ìƒ‰ëœ ë¬¸ì„œë‹¤:\n",
    "{context_text}\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=1024, stop=[\"</s>\"])\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# ===== ì‹¤í–‰ ì˜ˆì‹œ =====\n",
    "if __name__ == \"__main__\":\n",
    "    questions = [\n",
    "        # \"2014ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ìœ ë™ìì‚°ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2014ë…„ í˜„ê¸ˆíë¦„í‘œ ìƒ ë‹¹ê¸° ì˜ì—…í™œë™ í˜„ê¸ˆíë¦„ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2015ë…„ ë‹¹ê¸° ë¹„ìœ ë™ìì‚°ì€ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2015ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸°ìˆœì´ìµì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2016ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ë‹¨ê¸°ê¸ˆìœµìƒí’ˆì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2016ë…„ í¬ê´„ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ì´í¬ê´„ì´ìµì€ ì–¼ë§ˆë‹ˆ?\",\n",
    "        # \"2016ë…„ ìë³¸ë³€ë™í‘œ ìƒ ìê¸°ì£¼ì‹ì˜ ì·¨ë“ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2017ë…„ ë‹¹ê¸° ë§¤ì¶œì±„ê¶Œì€ ì¬ë¬´ìƒíƒœí‘œì— ë”°ë¥´ë©´ ì–¼ë§ˆëƒ?\",\n",
    "        # \"2017ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ì „ê¸° í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ì€ ì–¼ë§ˆì…ë‹ˆê¹Œ?\",\n",
    "        # \"2017ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë§¤ê°ì˜ˆì •ë¶„ë¥˜ìì‚°ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2018ë…„ ë‹¹ê¸° ë¯¸ìˆ˜ê¸ˆì€ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2018ë…„ ì†ìµê³„ì‚°ì„œìƒ ë§¤ì¶œì´ì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2019ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ì¢…ì†ê¸°ì—…, ê´€ê³„ê¸°ì—… ë° ê³µë™ê¸°ì—… íˆ¬ìëŠ” ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2019ë…„ í˜„ê¸ˆíë¦„í‘œ ìƒ ì´ìµì‰ì—¬ê¸ˆ ë°°ë‹¹ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2019ë…„ ì†ìµê³„ì‚°ì„œìƒ ê¸°ë³¸ì£¼ë‹¹ì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2020ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ìì‚°ì´ê³„ëŠ”?\",\n",
    "        # \"2020ë…„ ì†ìµê³„ì‚°ì„œ ìƒ íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2021ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ê¸°íƒ€í¬ê´„ì†ìµ-ê³µì •ê°€ì¹˜ê¸ˆìœµìì‚°ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2021ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ìœ ë™ë¹„ìœ¨ì„ ê³„ì‚°í•˜ë©´ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2021ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ê¸ˆìœµë¹„ìš©ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        # \"2022ë…„ ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë¹„ìœ ë™ë¶€ì±„ëŠ” ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2022ë…„ ì†ìµê³„ì‚°ì„œ ìƒ ë‹¹ê¸° ë²•ì¸ì„¸ë¹„ìš©ì€ ì–¼ë§ˆë‹ˆ?\",\n",
    "        # \"2022ë…„ ë‹¹ê¸° í˜„ê¸ˆíë¦„í‘œ ìƒ íˆ¬ìí™œë™ í˜„ê¸ˆíë¦„ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2023ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ì¬ê³ ìì‚°ì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2023ë…„ ë‹¹ê¸° ì˜ì—…ì´ìµì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2024ë…„ì—ëŠ” ì¬ë¬´ìƒíƒœí‘œìƒ ë‹¹ê¸° ë¬´í˜•ìì‚°ì´ ì–¼ë§ˆì•¼?\",\n",
    "        # \"2024ë…„ ì¬ë¬´ìƒíƒœí‘œ ìƒ ë‹¹ê¸° ìš°ì„ ì£¼ìë³¸ê¸ˆì€ ì–¼ë§ˆì¸ê°€?\",\n",
    "        # \"2024ë…„ ì†ìµê³„ì‚°ì„œìƒ ë‹¹ê¸° ë²•ì¸ì„¸ë¹„ìš©ì€ ì–¼ë§ˆì•¼?\",\n",
    "        #\"ì†ìµê³„ì‚°ì„œ ìƒ ë§¤ì¶œì•¡ì´ ì „ë…„ ëŒ€ë¹„ ì˜¤ë¥¸ ì—°ë„ë¥¼ ì „ë¶€ ì•Œë ¤ì¤˜\",\n",
    "        #\"ìœ ë™ë¹„ìœ¨(ìœ ë™ìì‚°/ìœ ë™ë¶€ì±„)ì„ 2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ê°’ì„ ì°¾ì•„ì„œ ê³„ì‚°í•´ë´\"\n",
    "        \"2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ìœ ë™ìì‚°ì˜ í•˜ìœ„ê³„ì¸µ ì •ë³´ë¥¼ ì „ë¶€ ì¤˜. metadataì—ì„œ hierarchy / levelì„ ê¼­ ì°¸ê³ í•´.\"\n",
    "    ]\n",
    "\n",
    "    for q in questions:\n",
    "        # LLM RAGëŠ” Cell 5ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤\n",
    "        print(\"âœ… ëª¨ë“  í•¨ìˆ˜ì™€ Zephyr ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "        print(\"ğŸ“ Cell 5ë¥¼ ì‹¤í–‰í•˜ì—¬ LLM RAG íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ì§ˆë¬¸:\", q)\n",
    "        print(\"ë‹µë³€:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104058bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== RAG Pipeline with LLM ==================\n",
    "def rag_pipeline(query: str, model, client, collection_name: str, top_k: int = 20):\n",
    "    \"\"\"ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ëŠ” RAG íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    # 1) ê²€ìƒ‰ ë‹¨ê³„\n",
    "    results, _ = dense_search(\n",
    "        query=query,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k,\n",
    "        fallback_min_hits=8\n",
    "    )\n",
    "    \n",
    "    # 2) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)\n",
    "    context_parts = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        metadata_info = []\n",
    "        metadata_keys = ['account_id', 'account_name', 'parent_id', 'level', 'hierarchy', \n",
    "                        'is_total', 'is_subtotal', 'period_type', 'statement_type', 'report_year']\n",
    "        for key in metadata_keys:\n",
    "            if key in r:\n",
    "                metadata_info.append(f\"{key}: {r[key]}\")\n",
    "        \n",
    "        context_part = f\"ë¬¸ì„œ {i}:\\n\"\n",
    "        context_part += f\"í…ìŠ¤íŠ¸: {r['text']}\\n\"\n",
    "        context_part += f\"ë©”íƒ€ë°ì´í„°: {', '.join(metadata_info)}\\n\"\n",
    "        context_parts.append(context_part)\n",
    "    \n",
    "    context_text = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 3) LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    prompt = f\"\"\"<|system|>\n",
    "ë„ˆëŠ” ì¬ë¬´ë³´ê³ ì„œ ì „ë¬¸ê°€ì´ì ë°ì´í„° êµ¬ì¡°í™” ì „ë¬¸ê°€ë‹¤.\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ metadataë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì¡°í™”ëœ í‘œë¥¼ ì‘ì„±í•´ë¼.\n",
    "\n",
    "**ì¤‘ìš” ê·œì¹™:**\n",
    "1. ë°˜ë“œì‹œ metadataì˜ account_id, account_name, parent_id, level, hierarchy ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•˜ë¼\n",
    "2. level ê°’ì´ í´ìˆ˜ë¡ í•˜ìœ„ í•­ëª©ì´ë¯€ë¡œ ê³„ì¸µ êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ë¼\n",
    "3. í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥: | account_id | account_name | parent_id | level | hierarchy | ê°’(ë°±ë§Œì›) | period_type |\n",
    "4. ëª¨ë“  ê²€ìƒ‰ëœ í•­ëª©ì„ ë¹ ì§ì—†ì´ í¬í•¨í•˜ë¼\n",
    "5. ì¶”ì¸¡í•˜ì§€ ë§ê³  ì œê³µëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ë¼\n",
    "6. hierarchyëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ í‘œì‹œí•˜ë¼\n",
    "7. ê°’ì€ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•œ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ë¼\n",
    "</s>\n",
    "<|user|>\n",
    "ì§ˆë¬¸: {query}\n",
    "\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:\n",
    "{context_text}\n",
    "\n",
    "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ í‘œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # 4) LLM ì‹¤í–‰\n",
    "    response = llm(prompt, max_tokens=2048, stop=[\"</s>\"], temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# ================== LLM í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ==================\n",
    "# Cell 4ì—ì„œ ê²€ìƒ‰ í•¨ìˆ˜ë“¤ì´ ë¨¼ì € ì‹¤í–‰ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ì œ LLM RAGë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤\n",
    "print(\"ğŸš€ LLM RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (Cell 4ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMìœ¼ë¡œ êµ¬ì¡°í™”)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "test_question = \"2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ë¹„ìœ ë™ìì‚°ì˜ í•˜ìœ„ê³„ì¸µ ì •ë³´ë¥¼ ì „ë¶€ ì¤˜. metadataì—ì„œ hierarchy / levelì„ ê¼­ ì°¸ê³ í•´.\"\n",
    "print(f\"ğŸ’¬ ì§ˆë¬¸: {test_question}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "try:\n",
    "    # LLMì„ ì‚¬ìš©í•œ êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±\n",
    "    answer = rag_pipeline(test_question, embed_model, client, collection_name, top_k=30)\n",
    "    \n",
    "    print(\"ğŸ¤– LLM ë‹µë³€:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"ğŸ“ Cell 4ë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== ğŸš€ ë…ë¦½ ì‹¤í–‰í˜• LLM RAG íŒŒì´í”„ë¼ì¸ ==================\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from llama_cpp import Llama\n",
    "\n",
    "print(\"ğŸš€ ë…ë¦½ ì‹¤í–‰í˜• LLM RAG íŒŒì´í”„ë¼ì¸\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "QDRANT_PATH = \"/Users/dan/Desktop/snu_project/git_ì œì¶œìš©/data/vector_store/final-sjchunk/bge-ko-qdrant_db\"\n",
    "MODEL_PATH = \"/Users/dan/Desktop/snu_project/models/zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "collection_name = \"audit_chunks\"\n",
    "\n",
    "# ===== ëª¨ë¸ ë¡œë“œ =====\n",
    "print(\"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "embed_model = SentenceTransformer(\"dragonkue/bge-m3-ko\")\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "llm = Llama(model_path=MODEL_PATH, n_ctx=4096, n_threads=8, n_gpu_layers=35)\n",
    "print(\"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ===== ìœ í‹¸ í•¨ìˆ˜ =====\n",
    "def extract_year_from_query(query: str):\n",
    "    m = re.search(r\"(20\\d{2}|19\\d{2})\", query)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def detect_statement_type(query: str) -> str:\n",
    "    q = query.replace(\" \", \"\")\n",
    "    if \"ì¬ë¬´ìƒíƒœí‘œ\" in q or \"ëŒ€ì°¨ëŒ€ì¡°í‘œ\" in q: return \"balance\"\n",
    "    if \"ì†ìµê³„ì‚°ì„œ\" in q or \"í¬ê´„ì†ìµ\" in q: return \"income\"\n",
    "    if \"í˜„ê¸ˆíë¦„í‘œ\" in q: return \"cashflow\"\n",
    "    if \"ìë³¸ë³€ë™í‘œ\" in q: return \"equity_changes\"\n",
    "    return \"balance\"\n",
    "\n",
    "def build_filter(year=None, statement_type=None, period_type=None, parent_id=None, min_level=None):\n",
    "    must = []\n",
    "    if year: must.append(qmodels.FieldCondition(key=\"report_year\", match=qmodels.MatchValue(value=year)))\n",
    "    if statement_type: must.append(qmodels.FieldCondition(key=\"statement_type\", match=qmodels.MatchValue(value=statement_type)))\n",
    "    if period_type: must.append(qmodels.FieldCondition(key=\"period_type\", match=qmodels.MatchValue(value=period_type)))\n",
    "    if parent_id: must.append(qmodels.FieldCondition(key=\"parent_id\", match=qmodels.MatchValue(value=parent_id)))\n",
    "    if min_level: must.append(qmodels.FieldCondition(key=\"level\", range=qmodels.Range(gte=min_level)))\n",
    "    must.append(qmodels.FieldCondition(key=\"is_total\", match=qmodels.MatchValue(value=False)))\n",
    "    return qmodels.Filter(must=must) if must else None\n",
    "\n",
    "# ===== ê²€ìƒ‰ í•¨ìˆ˜ =====\n",
    "def simple_search(query: str, top_k: int = 20):\n",
    "    year = extract_year_from_query(query)\n",
    "    statement_type = detect_statement_type(query)\n",
    "    period_type = \"current\"\n",
    "    \n",
    "    # ë¹„ìœ ë™ìì‚° ê²€ìƒ‰ì„ ìœ„í•œ í•„í„°\n",
    "    query_filter = build_filter(\n",
    "        year=year,\n",
    "        statement_type=statement_type,\n",
    "        period_type=period_type,\n",
    "        min_level=3  # í•˜ìœ„ í•­ëª©ë“¤\n",
    "    )\n",
    "    \n",
    "    qv = embed_model.encode(\"query: \" + query, normalize_embeddings=True).tolist()\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=query_filter\n",
    "    )\n",
    "    \n",
    "    output = []\n",
    "    seen_ids = set()\n",
    "    for r in results.points:\n",
    "        payload = r.payload or {}\n",
    "        text = payload.get(\"text\")\n",
    "        if not text: continue\n",
    "        \n",
    "        meta = payload.get(\"metadata\", {})\n",
    "        if not meta: meta = {k: v for k, v in payload.items() if k != \"text\"}\n",
    "        \n",
    "        account_id = meta.get(\"account_id\")\n",
    "        if account_id and account_id in seen_ids: continue\n",
    "        if account_id: seen_ids.add(account_id)\n",
    "        \n",
    "        # ë¹„ìœ ë™ìì‚° ê´€ë ¨ í•„í„°ë§\n",
    "        hierarchy = meta.get(\"hierarchy\", [])\n",
    "        if isinstance(hierarchy, list) and \"ë¹„ìœ ë™ìì‚°\" in hierarchy:\n",
    "            item = {\"score\": r.score, \"text\": text}\n",
    "            item.update(meta)\n",
    "            output.append(item)\n",
    "    \n",
    "    output.sort(key=lambda x: (x.get(\"level\", 999), str(x.get(\"account_name\", \"\"))))\n",
    "    return output\n",
    "\n",
    "# ===== RAG íŒŒì´í”„ë¼ì¸ =====\n",
    "def rag_pipeline(query: str, top_k: int = 20):\n",
    "    # 1) ê²€ìƒ‰\n",
    "    results = simple_search(query, top_k)\n",
    "    \n",
    "    # 2) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "    context_parts = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        metadata_keys = ['account_id', 'account_name', 'parent_id', 'level', 'hierarchy', \n",
    "                        'is_total', 'is_subtotal', 'period_type', 'statement_type', 'report_year']\n",
    "        metadata_info = [f\"{key}: {r[key]}\" for key in metadata_keys if key in r]\n",
    "        \n",
    "        context_part = f\"ë¬¸ì„œ {i}:\\ní…ìŠ¤íŠ¸: {r['text']}\\në©”íƒ€ë°ì´í„°: {', '.join(metadata_info)}\\n\"\n",
    "        context_parts.append(context_part)\n",
    "    \n",
    "    context_text = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 3) LLM í”„ë¡¬í”„íŠ¸\n",
    "    prompt = f\"\"\"<|system|>\n",
    "ë„ˆëŠ” ì¬ë¬´ë³´ê³ ì„œ ì „ë¬¸ê°€ë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ metadataë¥¼ í™œìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ í‘œë¥¼ ì‘ì„±í•´ë¼.\n",
    "\n",
    "**ê·œì¹™:**\n",
    "1. metadataì˜ account_id, account_name, parent_id, level, hierarchy ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©\n",
    "2. level ê°’ì´ í´ìˆ˜ë¡ í•˜ìœ„ í•­ëª©\n",
    "3. í‘œ í˜•ì‹: | account_id | account_name | parent_id | level | hierarchy | ê°’(ë°±ë§Œì›) | period_type |\n",
    "4. ëª¨ë“  ê²€ìƒ‰ëœ í•­ëª©ì„ í¬í•¨\n",
    "5. ì¶”ì¸¡í•˜ì§€ ë§ê³  ì œê³µëœ ë°ì´í„°ë§Œ ì‚¬ìš©\n",
    "6. ê°’ì€ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•œ ìˆ«ì ì‚¬ìš©\n",
    "</s>\n",
    "<|user|>\n",
    "ì§ˆë¬¸: {query}\n",
    "\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:\n",
    "{context_text}\n",
    "\n",
    "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ í‘œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # 4) LLM ì‹¤í–‰\n",
    "    response = llm(prompt, max_tokens=2048, stop=[\"</s>\"], temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# ===== ì‹¤í–‰ =====\n",
    "test_question = \"2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ë¹„ìœ ë™ìì‚°ì˜ í•˜ìœ„ê³„ì¸µ ì •ë³´ë¥¼ ì „ë¶€ ì¤˜. metadataì—ì„œ hierarchy / levelì„ ê¼­ ì°¸ê³ í•´.\"\n",
    "print(f\"ğŸ’¬ ì§ˆë¬¸: {test_question}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "try:\n",
    "    # 1) ê²€ìƒ‰\n",
    "    print(\"ğŸ” 1ë‹¨ê³„: ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...\")\n",
    "    search_results = simple_search(test_question, top_k=20)\n",
    "    print(f\"   âœ… {len(search_results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\")\n",
    "    \n",
    "    # 2) LLM ë‹µë³€ ìƒì„±\n",
    "    print(\"ğŸ¤– 2ë‹¨ê³„: LLM ë‹µë³€ ìƒì„± ì¤‘...\")\n",
    "    answer = rag_pipeline(test_question, top_k=20)\n",
    "    \n",
    "    print(\"ğŸ¯ ìµœì¢… ë‹µë³€:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(answer)\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e435868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== ğŸ¯ ìµœì¢… ì‹¤í–‰ ì•ˆë‚´ ==================\n",
    "print(\"ğŸ¯ ë…ë¦½ ì‹¤í–‰í˜• RAG ì‹œìŠ¤í…œ\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“ Cell 6ë§Œ ì‹¤í–‰í•˜ì‹œë©´ ëª¨ë“  ê¸°ëŠ¥ì´ ì‘ë™í•©ë‹ˆë‹¤!\")\n",
    "print(\"   - ëª¨ë¸ ë¡œë“œ\")\n",
    "print(\"   - ê²€ìƒ‰ ì‹¤í–‰\") \n",
    "print(\"   - LLM ë‹µë³€ ìƒì„±\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âš¡ ë‹¤ë¥¸ ì…€ë“¤ì€ ë¬´ì‹œí•˜ê³  Cell 6ë§Œ ì‹¤í–‰í•˜ì„¸ìš”!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0037927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8f9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ff6ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ’¬ ì§ˆë¬¸: 2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ìœ ë™ìì‚°ì˜ í•˜ìœ„ê³„ì¸µ ì •ë³´ë¥¼ ì „ë¶€ ì¤˜. metadataì—ì„œ hierarchy / levelì„ ê¼­ ì°¸ê³ í•´.\n",
      "\n",
      "ğŸ” ê²€ìƒ‰ ê²°ê³¼ (1ê°œ):\n",
      "====================================================================================================\n",
      "\n",
      "1. ìŠ¤ì½”ì–´: 0.5557\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ê¸°íƒ€ìœ ë™ìì‚°ëŠ” 821,079ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ìœ ë™ìì‚°_ê¸°íƒ€ìœ ë™ìì‚°, account_name: ê¸°íƒ€ìœ ë™ìì‚°, parent_id: ìì‚°_ìœ ë™ìì‚°, level: 3, hierarchy: ['ìœ ë™ìì‚°', 'ê¸°íƒ€ìœ ë™ìì‚°'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“‹ ìš”ì•½:\n",
      "   - ì´ 1ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "   - ê´€ë ¨ ì—°ë„: 2014\n",
      "   - ë°œê²¬ëœ ê³„ì¸µ ì •ë³´: 1ê°œ\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# from llama_cpp import Llama  # ì„ì‹œë¡œ ì£¼ì„ ì²˜ë¦¬ (ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŒ)\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "# ===== ì„ë² ë”© ëª¨ë¸ ë¡œë“œ =====\n",
    "embed_model = SentenceTransformer(\"dragonkue/bge-m3-ko\")\n",
    "\n",
    "# ===== Qdrant í´ë¼ì´ì–¸íŠ¸ =====\n",
    "QDRANT_PATH = \"/Users/dan/Desktop/snu_project/git_ì œì¶œìš©/data/vector_store/final-sjchunk/bge-ko-qdrant_db\"\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "collection_name = \"audit_chunks\"\n",
    "\n",
    "\n",
    "# =============== ìœ í‹¸ ===============\n",
    "def extract_year_from_query(query: str):\n",
    "    \"\"\"ì§ˆë¬¸ì—ì„œ ì—°ë„(4ìë¦¬ ìˆ«ì) ì¶”ì¶œ\"\"\"\n",
    "    match = re.search(r\"(20\\d{2}|19\\d{2})\", query)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_numbers(text: str) -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return \"\".join(re.findall(r\"\\d+\", text.replace(\",\", \"\")))\n",
    "\n",
    "\n",
    "# =============== (ì‹ ê·œ) í•„í„° ë¹Œë” ===============  ### ADDED\n",
    "def build_filter(\n",
    "    year: int = None,\n",
    "    statement_type: str = None,     # e.g., \"balance\"\n",
    "    period_type: str = None,        # e.g., \"current\" or \"previous\"\n",
    "    must_have_hierarchy: str = None,# e.g., \"ìœ ë™ìì‚°\"\n",
    "    min_level: int = None,          # e.g., 3\n",
    "    exclude_totals: bool = True,\n",
    "    exclude_subtotals: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Qdrant ì„œë²„ ì¸¡ í•„í„°ë¥¼ ì¡°ë¦½í•©ë‹ˆë‹¤.\n",
    "    - hierarchyëŠ” ë¦¬ìŠ¤íŠ¸ í•„ë“œë¼ê³  ê°€ì •í•˜ê³  'ìœ ë™ìì‚°' í¬í•¨ ì—¬ë¶€ë¥¼ MatchAnyë¡œ ì²´í¬í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    must = []\n",
    "\n",
    "    if year is not None:\n",
    "        must.append(qmodels.FieldCondition(\n",
    "            key=\"report_year\",\n",
    "            match=qmodels.MatchValue(value=year)\n",
    "        ))\n",
    "\n",
    "    if statement_type:\n",
    "        must.append(qmodels.FieldCondition(\n",
    "            key=\"statement_type\",\n",
    "            match=qmodels.MatchValue(value=statement_type)\n",
    "        ))\n",
    "\n",
    "    if period_type:\n",
    "        must.append(qmodels.FieldCondition(\n",
    "            key=\"period_type\",\n",
    "            match=qmodels.MatchValue(value=period_type)\n",
    "        ))\n",
    "\n",
    "    if must_have_hierarchy:\n",
    "        # hierarchyê°€ [\"ìì‚°\",\"ìœ ë™ìì‚°\",\"í˜„ê¸ˆë°...\"] ì²˜ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¼ê³  ê°€ì •\n",
    "        must.append(qmodels.FieldCondition(\n",
    "            key=\"hierarchy\",\n",
    "            match=qmodels.MatchAny(any=[must_have_hierarchy])\n",
    "        ))\n",
    "\n",
    "    if min_level is not None:\n",
    "        must.append(qmodels.FieldCondition(\n",
    "            key=\"level\",\n",
    "            range=qmodels.Range(gte=min_level)\n",
    "        ))\n",
    "\n",
    "    if exclude_totals:\n",
    "        must.append(qmodels.FieldCondition(\n",
    "            key=\"is_total\",\n",
    "            match=qmodels.MatchValue(value=False)\n",
    "        ))\n",
    "\n",
    "    if exclude_subtotals:\n",
    "        must.append(qmodels.FieldCondition(\n",
    "            key=\"is_subtotal\",\n",
    "            match=qmodels.MatchValue(value=False)\n",
    "        ))\n",
    "\n",
    "    return qmodels.Filter(must=must) if must else None\n",
    "\n",
    "\n",
    "# =============== dense_search ===============  ### CHANGED (í•µì‹¬ ìˆ˜ì •)\n",
    "def dense_search(query: str, model, client, collection_name: str,\n",
    "                 top_k: int = 50, ground_truth=None,\n",
    "                 score_threshold: float = None,\n",
    "                 strict_children_of: str = None):\n",
    "    \"\"\"\n",
    "    Qdrant query_points ê¸°ë°˜ Dense Retriever + ì„±ëŠ¥ì§€í‘œ ê³„ì‚°\n",
    "    ë³€ê²½ ì‚¬í•­:\n",
    "    - BGE ê³„ì—´ ê¶Œì¥: ì¿¼ë¦¬ì— \"query: \" í”„ë¦¬í”½ìŠ¤ ì ìš©\n",
    "    - ì„œë²„ í•„í„° ê°•í™”: balance/current/ì—°ë„/ìœ ë™ìì‚° í¬í•¨/level>=3/í† íƒˆ ì œì™¸\n",
    "    - ê²°ê³¼ ì¤‘ë³µ ì œê±°(account_id)\n",
    "    - strict_children_of(ì˜ˆ: 'ìœ ë™ìì‚°') ì¬ê²€ì¦\n",
    "    - ê²°ê³¼ ì •ë ¬(level, account_name)\n",
    "    - í•„ìš” ì‹œ score í•˜í•œì„  ì ìš©\n",
    "    \"\"\"\n",
    "    # 0) ì—°ë„ ì¶”ì¶œ\n",
    "    year = extract_year_from_query(query)\n",
    "\n",
    "    # 1) ì¿¼ë¦¬ ì„ë² ë”© (BGEëŠ” ì§ˆì˜ í”„ë¦¬í”½ìŠ¤ê°€ ë¯¸ì„¸í•˜ê²Œ ë„ì›€ë¨)\n",
    "    qv = model.encode(\"query: \" + query, normalize_embeddings=True).tolist()  # ### CHANGED\n",
    "\n",
    "    # 2) ì„œë²„ í•„í„° êµ¬ì„±\n",
    "    query_filter = build_filter(\n",
    "        year=year,\n",
    "        statement_type=\"balance\",           # ì¬ë¬´ìƒíƒœí‘œ ê°•ì œ\n",
    "        period_type=\"current\",              # ë‹¹ê¸°ê°’ ê°•ì œ\n",
    "        must_have_hierarchy=strict_children_of or \"ìœ ë™ìì‚°\",\n",
    "        min_level=3,                        # í•˜ìœ„ê³„ì¸µë§Œ\n",
    "        exclude_totals=True,                # í•©ê³„ ì œì™¸\n",
    "        exclude_subtotals=False             # í•„ìš” ì‹œ Trueë¡œ\n",
    "    )\n",
    "\n",
    "    # 3) ê²€ìƒ‰ ì‹¤í–‰\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=query_filter\n",
    "    )\n",
    "\n",
    "    # 4) ê²°ê³¼ ì •ë¦¬ + ì¤‘ë³µ ì œê±°(account_id)\n",
    "    seen_ids = set()\n",
    "    output = []\n",
    "    for r in results.points:\n",
    "        payload = r.payload or {}\n",
    "        text = payload.get(\"text\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # ë©”íƒ€ í‰íƒ„í™”\n",
    "        meta = {}\n",
    "        if \"metadata\" in payload and isinstance(payload[\"metadata\"], dict):\n",
    "            meta.update(payload[\"metadata\"])\n",
    "        else:\n",
    "            meta.update({k: v for k, v in payload.items() if k != \"text\"})\n",
    "\n",
    "        # account_id ê¸°ì¤€ ì¤‘ë³µ ì œê±°\n",
    "        account_id = meta.get(\"account_id\")\n",
    "        if account_id and account_id in seen_ids:\n",
    "            continue\n",
    "        if account_id:\n",
    "            seen_ids.add(account_id)\n",
    "\n",
    "        # ìŠ¤ì½”ì–´ ì»·\n",
    "        if score_threshold is not None and r.score < score_threshold:\n",
    "            continue\n",
    "\n",
    "        # ì—„ê²© ì¬ê²€ì¦: hierarchyì— íŠ¹ì • ë…¸ë“œê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•¨\n",
    "        if strict_children_of:\n",
    "            hier = meta.get(\"hierarchy\", [])\n",
    "            if isinstance(hier, list) and strict_children_of not in hier:\n",
    "                continue\n",
    "\n",
    "        result_item = {\"score\": r.score, \"text\": text}\n",
    "        result_item.update(meta)\n",
    "        output.append(result_item)\n",
    "\n",
    "    # 5) ê°€ë… ì •ë ¬: level ì˜¤ë¦„ì°¨ìˆœ â†’ account_name\n",
    "    output.sort(key=lambda x: (x.get(\"level\", 999), str(x.get(\"account_name\", \"\"))))\n",
    "\n",
    "    # 6) ì„±ëŠ¥ ì§€í‘œ (ì˜µì…˜)\n",
    "    metrics = {}\n",
    "    if ground_truth:\n",
    "        normalized_gt = [extract_numbers(gt) for gt in ground_truth]\n",
    "        used = set()\n",
    "        relevances = []\n",
    "        for r in output[:top_k]:\n",
    "            nums = extract_numbers(r[\"text\"])\n",
    "            hit = 0\n",
    "            for gt in normalized_gt:\n",
    "                if gt and gt in nums and gt not in used:\n",
    "                    hit = 1\n",
    "                    used.add(gt)\n",
    "                    break\n",
    "            relevances.append(hit)\n",
    "\n",
    "        precision = sum(relevances) / top_k if top_k > 0 else 0.0\n",
    "        recall = min(sum(relevances), len(normalized_gt)) / len(normalized_gt) if normalized_gt else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        rr = next((1.0 / (i+1) for i, rel in enumerate(relevances) if rel == 1), 0.0)\n",
    "        dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevances))\n",
    "        ideal_hits = min(len(normalized_gt), top_k)\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "        metrics = {\"Precision@3\": precision, \"Recall@3\": recall, \"F1@3\": f1, \"MRR\": rr, \"nDCG@3\": ndcg}\n",
    "\n",
    "    return output, metrics\n",
    "\n",
    "\n",
    "# =============== ê°„ë‹¨ ê²€ìƒ‰ ì¶œë ¥ ===============  ### CHANGED (ìƒˆ íŒŒë¼ë¯¸í„° ë°˜ì˜)\n",
    "def simple_search_test(query: str, model, client, collection_name: str, top_k: int = 10,\n",
    "                       score_threshold: float = None, strict_children_of: str = None):\n",
    "    \"\"\"LLM ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ í™•ì¸í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    results, _ = dense_search(\n",
    "        query=query,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k,\n",
    "        score_threshold=score_threshold,        # ### CHANGED\n",
    "        strict_children_of=strict_children_of   # ### CHANGED\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. ìŠ¤ì½”ì–´: {r['score']:.4f}\")\n",
    "        print(f\"   ì—°ë„: {r.get('report_year', 'N/A')}\")\n",
    "        print(f\"   í…ìŠ¤íŠ¸: {r['text'][:200]}...\")\n",
    "\n",
    "        metadata_keys = ['account_id', 'account_name', 'parent_id', 'level', 'hierarchy',\n",
    "                         'is_total', 'is_subtotal', 'period_type', 'statement_type']\n",
    "        metadata_info = []\n",
    "        for key in metadata_keys:\n",
    "            if key in r:\n",
    "                metadata_info.append(f\"{key}: {r[key]}\")\n",
    "        if metadata_info:\n",
    "            print(f\"   ğŸ“Š ë©”íƒ€ë°ì´í„°: {', '.join(metadata_info)}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# =============== RAG Pipeline (LLM ì—†ì´ ê²€ìƒ‰ë§Œ) ===============  ### CHANGED\n",
    "def rag_pipeline_simple(query: str, model, client, collection_name: str, top_k: int = 10,\n",
    "                        score_threshold: float = None, strict_children_of: str = None):\n",
    "    \"\"\"LLM ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜í•˜ëŠ” ê°„ë‹¨í•œ RAG\"\"\"\n",
    "    print(f\"ğŸ’¬ ì§ˆë¬¸: {query}\\n\")\n",
    "    results = simple_search_test(\n",
    "        query, model, client, collection_name, top_k=top_k,\n",
    "        score_threshold=score_threshold, strict_children_of=strict_children_of\n",
    "    )\n",
    "    print(f\"\\nğŸ“‹ ìš”ì•½:\")\n",
    "    print(f\"   - ì´ {len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    if results:\n",
    "        years = list({r.get('report_year') for r in results if r.get('report_year')})\n",
    "        if years:\n",
    "            print(f\"   - ê´€ë ¨ ì—°ë„: {', '.join(map(str, sorted(years)))}\")\n",
    "        hierarchies = [r.get('hierarchy', []) for r in results if r.get('hierarchy')]\n",
    "        if hierarchies:\n",
    "            print(f\"   - ë°œê²¬ëœ ê³„ì¸µ ì •ë³´: {len(hierarchies)}ê°œ\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# ===== Zephyr ëª¨ë¸ ë¡œë“œ (ì„ì‹œë¡œ ì£¼ì„ ì²˜ë¦¬) =====\n",
    "# model_path = Path(\"/Users/bag-yebin/Desktop/í /ìì—°ì–´ì²˜ë¦¬/samsun-audit-rag-qa/models/zephyr-7b-beta.Q4_K_M.gguf\").resolve()\n",
    "# llm = Llama(\n",
    "#     model_path=str(model_path),\n",
    "#     n_ctx=4096,\n",
    "#     n_threads=8,\n",
    "#     n_gpu_layers=35\n",
    "# )\n",
    "\n",
    "\n",
    "# =============== ì‹¤í–‰ ì˜ˆì‹œ ===============\n",
    "if __name__ == \"__main__\":\n",
    "    questions = [\n",
    "        \"2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ìœ ë™ìì‚°ì˜ í•˜ìœ„ê³„ì¸µ ì •ë³´ë¥¼ ì „ë¶€ ì¤˜. metadataì—ì„œ hierarchy / levelì„ ê¼­ ì°¸ê³ í•´.\"\n",
    "    ]\n",
    "\n",
    "    for q in questions:\n",
    "        print(\"=\" * 100)\n",
    "        # strict_children_of=\"ìœ ë™ìì‚°\" ì„ ì•ˆì „ë§ìœ¼ë¡œ í™œìš©\n",
    "        results = rag_pipeline_simple(\n",
    "            q, embed_model, client, collection_name,\n",
    "            top_k=50,\n",
    "            score_threshold=0.55,           # í•„ìš” ì‹œ ì¡°ì ˆ\n",
    "            strict_children_of=\"ìœ ë™ìì‚°\"   # 'ìœ ë™ìì‚°' subtreeë§Œ\n",
    "        )\n",
    "        print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa2aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M2 Pro) - 7888 MiB free\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /Users/dan/Desktop/snu_project/models/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = huggingfaceh4_zephyr-7b-beta\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 2 '</s>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device Metal, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4095.08 MiB, ( 7129.45 / 10922.67)\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  4095.07 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n",
      "...............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x3484ffda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x347c65ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x343a5c350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x39505b4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x395059b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x39505a390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x343a2e0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x3a0f4fca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x3a0f48570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x347dc91e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x39dc9b0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x39dc9b840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x337d9dc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x39505de90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x3a0f44e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x349780a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x337ddec20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x39dc9baa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x39dc9bd30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x347d22830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x3438bb8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x39dc9bf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x3438b1530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x3424171a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x342352ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x34827d2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x39a2a83a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x106b0a6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x101305100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x3421f1f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x102a0dfa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x106b09730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x3438c0f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x343856670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x347a97720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x102a0eb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x39505e0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x10130b880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x102a0f0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x102a0f330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x3a0f50220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x10130c9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x3438a3300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x3a0f514d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x3480745f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x39dc9c1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x10130b3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x39505ecf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x39dc9cbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106104950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x10610cd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10130c0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x10130cfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106b0bc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x10130e6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x10610da60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10610e6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x39dc9ce70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x3377c9640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x3423e52b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x3377a33d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x344d71530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x39dc9d0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x3486be330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x3422aeb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x39dc9d330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x34b8f4670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x39dc9d590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x3455b8460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x39dc9d7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x39dc9da50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x39dc9dec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x3438830d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x342205ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x3427dff90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x39dc9e120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x34672b7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x3455b79f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x39a2a9840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x3a0f57c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x3a0f54460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x10610de40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x3460c8a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x343888d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x343894b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x102a10940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x334e445e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x10130db10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x106b0b310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x348036310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x3460f5170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x3a0f52460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x3479eeb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x34793a5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x3427e0880 | th_max =  384 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x39a2a90c0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x39505e5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x39dc9e380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x345fa48c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x3a0f526c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x347e95d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x39dc9e5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x3427e0ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x3a0f60750 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x39a2a9320 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x3463180b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x3463a6960 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x34bdbc640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x39505e9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x39a2a9dc0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x39a2aa020 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x39a2aa540 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x334e803a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x3427e0d40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x34bee9020 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x346359bf0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x334e84e10 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x39505f4b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x34bd20b80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x34646cd40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x3464fbbd0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x34649da70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x34baf21c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x346553420 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x39a2aab80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x39a2aade0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x334e26480 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x3466783e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x346657390 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x3427e0fd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x39a2ab0d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x34ba62140 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x39505f710 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x39dc9e840 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x3a0f609b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x3a0f60c10 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x34622e620 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x3a0f60ea0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x3a0f61100 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x3427e1260 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x39dc9ead0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x3950608f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x395060e30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x39a2abf70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x344a603d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x3a0f61390 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x334e11d00 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x3435b4240 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x334e928e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x334e4deb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x334e27920 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x334eb9180 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x346296880 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x334e7b9e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x3427e14c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x39dc9ed60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x39a2ac540 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x39dc9efc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x3427e1780 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x3427e19e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x327593c80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x3275fd2e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x34367a260 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x34369e1e0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x343668740 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x348f44810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x39a2ad5f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x3427e1c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x3a0f61620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x3a0f61880 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x3436519b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x343607a30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x3427e1ea0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x3487d4880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x3a0f61ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x3a0f61d40 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x343670a50 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x3a0f61fa0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x3352abc70 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x33523d290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x3427e2100 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x3374d0340 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x3275a5b20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x3427e2360 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x3427e25c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x3a0f62200 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x3a0f62460 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x39a2addc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x3a0f626c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x3950615b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x39a2ac8c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x3275cce00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x3a0f62920 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x3a0f62b80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x39dc9f220 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x39a2acfe0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x335b1a9f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x3427e2820 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x39dc9f480 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x335bafcd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x335baaba0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x3427e2a80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x3427e2ce0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x33524bd60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x39dc9f6e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x39dc9f940 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x395062690 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x335267400 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x395062dd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x395063030 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x39dc9fba0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x3375e6110 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x39a2af0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x39dc9fe00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x39a2af840 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x39dca0060 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x3352e9b00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x39a2aff50 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x39dca02c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x3352f4fd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x3352be420 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x3a0f62de0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x3a0f63040 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x395063760 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x3a0f632a0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x348bbb230 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x3a0f63500 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x335bcd990 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x3363b8a70 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x3a0f63760 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x3a0f639c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x3a0f63c20 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x3352702f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x3363390b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x395063290 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x39dca0520 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x3a0f63e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x3427e2f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x3427e31d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x3434b3b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x343b0d3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x395075900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x3a0f640e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x3a0f64340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x33636e080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x34340e070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x39dca0780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x3427e3460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x348518f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x39dca09e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x39dca0c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x337bf6c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x3a0f64630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x3a0f64890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x337bab760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x3a0f64af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x39a2ae910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x395076e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x395077440 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x395077c10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x3950781f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x3427e36c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x336daa140 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x3427e3920 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x343b697b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x3363f5e50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x33630d360 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x336382f00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x39a2b0750 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x3427e3bd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x3427e3e70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x343b80f30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x395078450 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x3363c1910 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x395076750 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x106b0c790 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10610ed20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10130ef50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10610f470 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x336d559d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x3363b2d50 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x347544e80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x3454972b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x3427e40d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x102a11080 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x3427e4330 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x39dca0ed0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x39dca1160 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106b0c9f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x3a0f64d50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x106b0d0a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x3454d9e20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10130f6d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x10130f930 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106b0d930 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x3427e4590 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x102a11810 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106b0ea50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x3a0f64fb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x101310920 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x3a0f65270 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106110310 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x343b47380 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x395079230 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x3427e47f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x3475875d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x3427e4ab0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x3475410f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x39a2b1090 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x336ed01f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x336e217e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x343b601b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x344eea2d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x3a0f654d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x33586ee70 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x3358080f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x3427e4d40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x344e76810 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x344e790b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x3373f7000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x33587fef0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x343bd0990 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x39a2b1820 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x343bd4650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x335818e10 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x395079a10 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x3358ee850 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x3358426e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106b0f260 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x33585c920 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x3358f9b60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x3a0f65730 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x3439a8b80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x3a0f65990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x33730a550 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x3427e4fd0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x3427e5260 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x3a0f65bf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x3a0f65f40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x3439c7f60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x3427e54c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x39507a1e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x343920170 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x3427e5720 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x3a0f661a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x3439ed540 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x39dca13c0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x3439d7840 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x3427e5980 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x3427e5be0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x344fa2140 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x33620f420 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x3362517f0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x3427e5f00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x3427e6160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x33626c310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x344f209c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x3362ba610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x39507ae80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x39507b5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x33620b2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x34391cd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x336273660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x33764d550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x336275860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x3427e63c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x3376c6880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x337612880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x3427e6620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x39507c2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x335704710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x3376adf80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x336bf9de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x335740150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x335707c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x34393fa20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x3357ac5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x39a2b0c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x3a0f66430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x3427e6b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x343915b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x39507bd70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x336b90110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x33576c630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x3357a6870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x3427e6d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x39507c630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x3439e1580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x3357de300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x33617e220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x345ef9300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x3361b56c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x345e7de90 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = Metal\n",
      "llama_kv_cache_unified: layer   1: dev = Metal\n",
      "llama_kv_cache_unified: layer   2: dev = Metal\n",
      "llama_kv_cache_unified: layer   3: dev = Metal\n",
      "llama_kv_cache_unified: layer   4: dev = Metal\n",
      "llama_kv_cache_unified: layer   5: dev = Metal\n",
      "llama_kv_cache_unified: layer   6: dev = Metal\n",
      "llama_kv_cache_unified: layer   7: dev = Metal\n",
      "llama_kv_cache_unified: layer   8: dev = Metal\n",
      "llama_kv_cache_unified: layer   9: dev = Metal\n",
      "llama_kv_cache_unified: layer  10: dev = Metal\n",
      "llama_kv_cache_unified: layer  11: dev = Metal\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   300.01 MiB\n",
      "llama_context:        CPU compute buffer size =    20.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'huggingfaceh4_zephyr-7b-beta'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ’¬ ì§ˆë¬¸: 2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ë¹„ìœ ë™ìì‚°ì˜ í•˜ìœ„ê³„ì¸µ ì •ë³´ë¥¼ ì „ë¶€ ì¤˜. metadataì—ì„œ hierarchy / levelì„ ê¼­ ì°¸ê³ í•´.\n",
      "\n",
      "ğŸ” ê²€ìƒ‰ ê²°ê³¼ (7ê°œ):\n",
      "====================================================================================================\n",
      "\n",
      "1. ìŠ¤ì½”ì–´: 0.5709\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ê¸°íƒ€ë¹„ìœ ë™ìì‚°ëŠ” 1,694,436ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ë¹„ìœ ë™ìì‚°_ê¸°íƒ€ë¹„ìœ ë™ìì‚°, account_name: ê¸°íƒ€ë¹„ìœ ë™ìì‚°, parent_id: ìì‚°_ë¹„ìœ ë™ìì‚°, level: 3, hierarchy: ['ë¹„ìœ ë™ìì‚°', 'ê¸°íƒ€ë¹„ìœ ë™ìì‚°'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. ìŠ¤ì½”ì–´: 0.5290\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ë¬´í˜•ìì‚°ëŠ” 3,051,564ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 14...\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ë¹„ìœ ë™ìì‚°_ë¬´í˜•ìì‚°, account_name: ë¬´í˜•ìì‚°, parent_id: ìì‚°_ë¹„ìœ ë™ìì‚°, level: 3, hierarchy: ['ë¹„ìœ ë™ìì‚°', 'ë¬´í˜•ìì‚°'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. ìŠ¤ì½”ì–´: 0.4806\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìˆœí™•ì •ê¸‰ì—¬ìì‚°ëŠ” 135,951ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 17...\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ë¹„ìœ ë™ìì‚°_ìˆœí™•ì •ê¸‰ì—¬ìì‚°, account_name: ìˆœí™•ì •ê¸‰ì—¬ìì‚°, parent_id: ìì‚°_ë¹„ìœ ë™ìì‚°, level: 3, hierarchy: ['ë¹„ìœ ë™ìì‚°', 'ìˆœí™•ì •ê¸‰ì—¬ìì‚°'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. ìŠ¤ì½”ì–´: 0.5072\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ìœ í˜•ìì‚°ëŠ” 43,744,259ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 13...\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ë¹„ìœ ë™ìì‚°_ìœ í˜•ìì‚°, account_name: ìœ í˜•ìì‚°, parent_id: ìì‚°_ë¹„ìœ ë™ìì‚°, level: 3, hierarchy: ['ë¹„ìœ ë™ìì‚°', 'ìœ í˜•ìì‚°'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. ìŠ¤ì½”ì–´: 0.4657\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ì¥ê¸°ë§¤ë„ê°€ëŠ¥ê¸ˆìœµìì‚°ëŠ” 7,106,234ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 6, 9...\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ë¹„ìœ ë™ìì‚°_ì¥ê¸°ë§¤ë„ê°€ëŠ¥ê¸ˆìœµìì‚°, account_name: ì¥ê¸°ë§¤ë„ê°€ëŠ¥ê¸ˆìœµìì‚°, parent_id: ìì‚°_ë¹„ìœ ë™ìì‚°, level: 3, hierarchy: ['ë¹„ìœ ë™ìì‚°', 'ì¥ê¸°ë§¤ë„ê°€ëŠ¥ê¸ˆìœµìì‚°'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "6. ìŠ¤ì½”ì–´: 0.4131\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ì¥ê¸°ì„ ê¸‰ë¹„ìš©ëŠ” 4,415,935ë°±ë§Œì›ì…ë‹ˆë‹¤....\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ë¹„ìœ ë™ìì‚°_ì¥ê¸°ì„ ê¸‰ë¹„ìš©, account_name: ì¥ê¸°ì„ ê¸‰ë¹„ìš©, parent_id: ìì‚°_ë¹„ìœ ë™ìì‚°, level: 3, hierarchy: ['ë¹„ìœ ë™ìì‚°', 'ì¥ê¸°ì„ ê¸‰ë¹„ìš©'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "7. ìŠ¤ì½”ì–´: 0.4869\n",
      "   ì—°ë„: 2014\n",
      "   í…ìŠ¤íŠ¸: ì¬ë¬´ìƒíƒœí‘œì—ì„œ 2014ë…„ (ë‹¹ê¸°) ì¢…ì†ê¸°ì—…,ê´€ê³„ê¸°ì—…ë°ê³µë™ê¸°ì—…íˆ¬ìëŠ” 41,857,431ë°±ë§Œì›ì…ë‹ˆë‹¤. ì£¼ì„: 12...\n",
      "   ğŸ“Š ë©”íƒ€ë°ì´í„°: account_id: ìì‚°_ë¹„ìœ ë™ìì‚°_ì¢…ì†ê¸°ì—…,ê´€ê³„ê¸°ì—…ë°ê³µë™ê¸°ì—…íˆ¬ì, account_name: ì¢…ì†ê¸°ì—…,ê´€ê³„ê¸°ì—…ë°ê³µë™ê¸°ì—…íˆ¬ì, parent_id: ìì‚°_ë¹„ìœ ë™ìì‚°, level: 3, hierarchy: ['ë¹„ìœ ë™ìì‚°', 'ì¢…ì†ê¸°ì—…,ê´€ê³„ê¸°ì—…ë°ê³µë™ê¸°ì—…íˆ¬ì'], is_total: False, is_subtotal: True, period_type: current, statement_type: balance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“‹ ìš”ì•½:\n",
      "   - ì´ 7ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "   - ê´€ë ¨ ì—°ë„: 2014\n",
      "   - ë°œê²¬ëœ ê³„ì¸µ ì •ë³´: 7ê°œ\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# from llama_cpp import Llama  # ì„ì‹œë¡œ ì£¼ì„ ì²˜ë¦¬ (ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŒ)\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "# ===== ì„ë² ë”© ëª¨ë¸ ë¡œë“œ =====\n",
    "embed_model = SentenceTransformer(\"dragonkue/bge-m3-ko\")\n",
    "\n",
    "# ===== Qdrant í´ë¼ì´ì–¸íŠ¸ =====\n",
    "QDRANT_PATH = \"/Users/dan/Desktop/snu_project/git_ì œì¶œìš©/data/vector_store/final-sjchunk/bge-ko-qdrant_db\"\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "collection_name = \"audit_chunks\"\n",
    "\n",
    "# ================== ìœ í‹¸ ==================\n",
    "KOREAN_SPACE_RE = re.compile(r\"(?:[ê°€-í£]\\s)+(?:[ê°€-í£])\")\n",
    "def collapse_ko_spaced(s: str) -> str:\n",
    "    \"\"\"'ìœ   ë™  ì  ì‚°' -> 'ìœ ë™ìì‚°'\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    return s.replace(\" \", \"\") if KOREAN_SPACE_RE.fullmatch(s) else s\n",
    "\n",
    "def extract_year_from_query(query: str):\n",
    "    m = re.search(r\"(20\\d{2}|19\\d{2})\", query)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def extract_numbers(text: str) -> str:\n",
    "    if text is None: return \"\"\n",
    "    return \"\".join(re.findall(r\"\\d+\", text.replace(\",\", \"\")))\n",
    "\n",
    "def detect_statement_type(query: str) -> str:\n",
    "    q = query.replace(\" \", \"\")\n",
    "    if \"ì¬ë¬´ìƒíƒœí‘œ\" in q or \"ëŒ€ì°¨ëŒ€ì¡°í‘œ\" in q: return \"balance\"\n",
    "    if \"ì†ìµê³„ì‚°ì„œ\" in q or \"í¬ê´„ì†ìµ\" in q: return \"income\"\n",
    "    if \"í˜„ê¸ˆíë¦„í‘œ\" in q: return \"cashflow\"\n",
    "    if \"ìë³¸ë³€ë™í‘œ\" in q: return \"equity_changes\"\n",
    "    # ê¸°ë³¸ê°’(ì§ˆë¬¸ì— ì—†ìœ¼ë©´ ì¬ë¬´ìƒíƒœí‘œë¡œ ê°€ì •)\n",
    "    return \"balance\"\n",
    "\n",
    "# ================== Qdrant Filter ë¹Œë” ==================\n",
    "def build_filter(\n",
    "    year: int = None,\n",
    "    statement_type: str = None,\n",
    "    period_type: str = None,\n",
    "    must_have_hierarchy: str = None,  # ë¦¬ìŠ¤íŠ¸ í¬í•¨ ì²´í¬\n",
    "    parent_id: str = None,\n",
    "    min_level: int = None,\n",
    "    exclude_totals: bool = True,\n",
    "    exclude_subtotals: bool = False,\n",
    "):\n",
    "    must = []\n",
    "    if year is not None:\n",
    "        must.append(qmodels.FieldCondition(key=\"report_year\", match=qmodels.MatchValue(value=year)))\n",
    "    if statement_type:\n",
    "        must.append(qmodels.FieldCondition(key=\"statement_type\", match=qmodels.MatchValue(value=statement_type)))\n",
    "    if period_type:\n",
    "        must.append(qmodels.FieldCondition(key=\"period_type\", match=qmodels.MatchValue(value=period_type)))\n",
    "    if must_have_hierarchy:\n",
    "        must.append(qmodels.FieldCondition(key=\"hierarchy\", match=qmodels.MatchAny(any=[must_have_hierarchy])))\n",
    "    if parent_id:\n",
    "        must.append(qmodels.FieldCondition(key=\"parent_id\", match=qmodels.MatchValue(value=parent_id)))\n",
    "    if min_level is not None:\n",
    "        must.append(qmodels.FieldCondition(key=\"level\", range=qmodels.Range(gte=min_level)))\n",
    "    if exclude_totals:\n",
    "        must.append(qmodels.FieldCondition(key=\"is_total\", match=qmodels.MatchValue(value=False)))\n",
    "    if exclude_subtotals:\n",
    "        must.append(qmodels.FieldCondition(key=\"is_subtotal\", match=qmodels.MatchValue(value=False)))\n",
    "    return qmodels.Filter(must=must) if must else None\n",
    "\n",
    "# ================== Parent ìë™ íƒìƒ‰ ==================\n",
    "def resolve_parent_node(\n",
    "    query: str,\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    year: int,\n",
    "    statement_type: str = \"balance\",\n",
    "    period_type: str = \"current\",\n",
    "    level: int = 2,\n",
    "    top_k: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    ì§ˆì˜ì—ì„œ 'ë¶€ëª¨ ë…¸ë“œ'(ì˜ˆ: ìœ ë™ìì‚°, ë¹„ìœ ë™ìì‚°, ìœ ë™ë¶€ì±„ ë“±)ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ ë°˜í™˜.\n",
    "    - ë°©ë²•: ì¿¼ë¦¬ ì„ë² ë”© + ì„œë²„ í•„í„°(year/statement/current/level=2)ë¡œ ìƒìœ„ í›„ë³´ë¥¼ ë°›ê³ ,\n",
    "            í›„ë³´ì˜ account_name/hierarchy ë§ë‹¨ì´ ì§ˆì˜ ë¬¸ìì—´ì— ë“±ì¥í•˜ëŠ”ì§€ ìš°ì„  ë§¤ì¹­.\n",
    "    - ë°˜í™˜: dict(account_id, account_name, hierarchy, level, report_year, ...)\n",
    "    \"\"\"\n",
    "    # 1) í•„í„°: í•´ë‹¹ ì—°ë„, í‘œ ì¢…ë¥˜, ë‹¹ê¸°, level=2 (ë¶€ëª¨ ë ˆë²¨)\n",
    "    filt = build_filter(\n",
    "        year=year,\n",
    "        statement_type=statement_type,\n",
    "        period_type=period_type,\n",
    "        min_level=level,\n",
    "        exclude_totals=True,\n",
    "        exclude_subtotals=False,\n",
    "    )\n",
    "    # 2) ì§ˆì˜ ì„ë² ë”©\n",
    "    qv = embed_model.encode(\"query: \" + query, normalize_embeddings=True).tolist()\n",
    "    # 3) ê²€ìƒ‰\n",
    "    res = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=filt\n",
    "    )\n",
    "\n",
    "    # 4) í›„ë³´ ì¤‘ 'ì§ˆë¬¸ì— ì´ë¦„ì´ ì§ì ‘ ë“±ì¥'í•˜ëŠ” ê²ƒì„ ìš°ì„  ì„ íƒ\n",
    "    q_norm = collapse_ko_spaced(query.replace(\" \", \"\"))\n",
    "    best = None\n",
    "    for p in res.points:\n",
    "        payload = p.payload or {}\n",
    "        meta = payload.get(\"metadata\", payload)\n",
    "        name = collapse_ko_spaced(str(meta.get(\"account_name\", \"\")))\n",
    "        # hierarchyì˜ ë§ë‹¨ ë…¸ë“œëª…ë„ ê²€ì‚¬\n",
    "        last_h = \"\"\n",
    "        hier = meta.get(\"hierarchy\", [])\n",
    "        if isinstance(hier, list) and len(hier) > 0:\n",
    "            last_h = collapse_ko_spaced(str(hier[-1]))\n",
    "        # ì§ì ‘ ë¬¸ìì—´ ë§¤ì¹­\n",
    "        hit = False\n",
    "        if name and name in q_norm:\n",
    "            hit = True\n",
    "        elif last_h and last_h in q_norm:\n",
    "            hit = True\n",
    "        # 'ìœ ë™ ìì‚°'ì²˜ëŸ¼ ë„ì–´ì“°ê¸° í¬í•¨ ì§ˆì˜ë¥¼ ëŒ€ë¹„í•´ ì¶•ì•½ ë§¤ì¹­ë„ ê²€ì‚¬ (ì´ë¯¸ q_normì—ì„œ ê³µë°± ì œê±°)\n",
    "        if hit:\n",
    "            best = meta\n",
    "            break\n",
    "\n",
    "    # 5) ë§¤ì¹­ì´ ì—†ë‹¤ë©´ ìŠ¤ì½”ì–´ 1ìˆœìœ„ë¡œ\n",
    "    if not best and res.points:\n",
    "        best_payload = res.points[0].payload or {}\n",
    "        best = best_payload.get(\"metadata\", best_payload)\n",
    "\n",
    "    return best  # ì‹¤íŒ¨ ì‹œ None\n",
    "\n",
    "# ================== ìì‹ ì „ëŸ‰ íšŒìˆ˜(scroll) ==================\n",
    "def scroll_children_by_parent(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    year: int,\n",
    "    parent_id: str,\n",
    "    statement_type: str = \"balance\",\n",
    "    period_type: str = \"current\",\n",
    "    min_level: int = 3,\n",
    "    exclude_totals: bool = True,\n",
    "    exclude_subtotals: bool = False,\n",
    "    limit: int = 256\n",
    "):\n",
    "    filt = build_filter(\n",
    "        year=year,\n",
    "        statement_type=statement_type,\n",
    "        period_type=period_type,\n",
    "        parent_id=parent_id,\n",
    "        min_level=min_level,\n",
    "        exclude_totals=exclude_totals,\n",
    "        exclude_subtotals=exclude_subtotals\n",
    "    )\n",
    "    out = []\n",
    "    next_offset = None\n",
    "    while True:\n",
    "        points, next_offset = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            scroll_filter=filt,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "            limit=limit,\n",
    "            offset=next_offset\n",
    "        )\n",
    "        for p in points:\n",
    "            payload = p.payload or {}\n",
    "            text = payload.get(\"text\")\n",
    "            if not text:\n",
    "                continue\n",
    "            meta = payload.get(\"metadata\", {})\n",
    "            if not meta:\n",
    "                meta = {k: v for k, v in payload.items() if k != \"text\"}\n",
    "            item = {\"score\": None, \"text\": text}\n",
    "            item.update(meta)\n",
    "            out.append(item)\n",
    "        if not next_offset:\n",
    "            break\n",
    "    out.sort(key=lambda x: (x.get(\"level\", 999), str(x.get(\"account_name\", \"\"))))\n",
    "    return out\n",
    "\n",
    "# ================== Dense Search (ìë™ parent ì‚¬ìš©) ==================\n",
    "def dense_search(query: str, model, client, collection_name: str,\n",
    "                 top_k: int = 50, ground_truth=None,\n",
    "                 score_threshold: float = None,\n",
    "                 strict_children_of: str = None,\n",
    "                 fallback_min_hits: int = 6):\n",
    "    \"\"\"\n",
    "    1) ì§ˆì˜ë¡œë¶€í„° ì—°ë„/í‘œì¢…ë¥˜/ë¶€ëª¨ë…¸ë“œ ìë™ ì¶”ì •\n",
    "    2) ë¶€ëª¨ account_idë¥¼ parent_id_hintë¡œ ì‚¬ìš©í•˜ì—¬ ìì‹ ê²€ìƒ‰\n",
    "    3) ë¶€ì¡±í•˜ë©´ scrollë¡œ ì „ëŸ‰ ë³´ê°•\n",
    "    \"\"\"\n",
    "    # 0) ì—°ë„/í‘œì¢…ë¥˜ ì¶”ë¡ \n",
    "    year = extract_year_from_query(query)\n",
    "    statement_type = detect_statement_type(query)\n",
    "    period_type = \"current\"  # ì§ˆë¬¸ì´ 'ë‹¹ê¸°' ì¤‘ì‹¬ì´ë¯€ë¡œ ê¸°ë³¸ê°’ current\n",
    "\n",
    "    # ì—°ë„ëŠ” ë°˜ë“œì‹œ í•„ìš”. ì—†ë‹¤ë©´ í•„í„° ì•½í™”(=ì „ ì—°ë„) ëŒ€ì‹  ê²°ê³¼ í’ˆì§ˆ ìœ„í•´ None í—ˆìš© but fallbackì—ì„œ year í•„ìš”\n",
    "    # â†’ year ì—†ìœ¼ë©´ ìš°ì„  denseë§Œ ìˆ˜í–‰í•˜ê³ , fallbackì€ ìƒëµí•˜ê±°ë‚˜ ê°€ì¥ ìµœê·¼ ì—°ë„ ì°¾ê¸° ë¡œì§ ì¶”ê°€ ê°€ëŠ¥\n",
    "    # ì—¬ê¸°ì„œëŠ” year ì—†ìœ¼ë©´ fallback ìƒëµ\n",
    "    # 1) ë¶€ëª¨ ë…¸ë“œ ìë™ íƒìƒ‰\n",
    "    parent_meta = None\n",
    "    if year is not None:\n",
    "        parent_meta = resolve_parent_node(\n",
    "            query=query,\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            year=year,\n",
    "            statement_type=statement_type,\n",
    "            period_type=period_type,\n",
    "            level=2,\n",
    "            top_k=10\n",
    "        )\n",
    "\n",
    "    # 2) ì¿¼ë¦¬ ì„ë² ë”©\n",
    "    qv = model.encode(\"query: \" + query, normalize_embeddings=True).tolist()\n",
    "\n",
    "    # 3) ì„œë²„ í•„í„° êµ¬ì„± (ë¶€ëª¨ë¥¼ ì°¾ì•˜ìœ¼ë©´ parent_id ê°•ì œ)\n",
    "    query_filter = build_filter(\n",
    "        year=year,\n",
    "        statement_type=statement_type,\n",
    "        period_type=period_type,\n",
    "        parent_id=parent_meta.get(\"account_id\") if parent_meta else None,\n",
    "        min_level=3,\n",
    "        exclude_totals=True,\n",
    "        exclude_subtotals=False\n",
    "    )\n",
    "\n",
    "    # 4) ê²€ìƒ‰ ì‹¤í–‰\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=query_filter\n",
    "    )\n",
    "\n",
    "    # 5) ê²°ê³¼ ì •ë¦¬ + ì¤‘ë³µ ì œê±°\n",
    "    seen_ids = set()\n",
    "    output = []\n",
    "    for r in results.points:\n",
    "        payload = r.payload or {}\n",
    "        text = payload.get(\"text\")\n",
    "        if not text:\n",
    "            continue\n",
    "        meta = payload.get(\"metadata\", {})\n",
    "        if not meta:\n",
    "            meta = {k: v for k, v in payload.items() if k != \"text\"}\n",
    "\n",
    "        account_id = meta.get(\"account_id\")\n",
    "        if account_id and account_id in seen_ids:\n",
    "            continue\n",
    "        if account_id:\n",
    "            seen_ids.add(account_id)\n",
    "\n",
    "        if score_threshold is not None and r.score < score_threshold:\n",
    "            continue\n",
    "\n",
    "        if strict_children_of:\n",
    "            hier = meta.get(\"hierarchy\", [])\n",
    "            if isinstance(hier, list) and strict_children_of not in hier:\n",
    "                continue\n",
    "\n",
    "        item = {\"score\": r.score, \"text\": text}\n",
    "        item.update(meta)\n",
    "        output.append(item)\n",
    "\n",
    "    # 6) ê°€ë… ì •ë ¬\n",
    "    output.sort(key=lambda x: (x.get(\"level\", 999), str(x.get(\"account_name\", \"\"))))\n",
    "\n",
    "    # 7) Fallback: ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œ parent_idë¡œ ì „ì²´ íšŒìˆ˜\n",
    "    if year is not None and parent_meta and len(output) < fallback_min_hits:\n",
    "        fallback_items = scroll_children_by_parent(\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            year=year,\n",
    "            parent_id=parent_meta[\"account_id\"],\n",
    "            statement_type=statement_type,\n",
    "            period_type=period_type,\n",
    "            min_level=3,\n",
    "            exclude_totals=True,\n",
    "            exclude_subtotals=False\n",
    "        )\n",
    "        # ë³‘í•©(ì¤‘ë³µ ì œê±°)\n",
    "        by_id = {x.get(\"account_id\"): x for x in output if x.get(\"account_id\")}\n",
    "        for it in fallback_items:\n",
    "            aid = it.get(\"account_id\")\n",
    "            if aid and aid not in by_id:\n",
    "                by_id[aid] = it\n",
    "        output = list(by_id.values())\n",
    "        output.sort(key=lambda x: (x.get(\"level\", 999), str(x.get(\"account_name\", \"\"))))\n",
    "\n",
    "    # 8) ë©”íŠ¸ë¦­(ì˜µì…˜)\n",
    "    metrics = {}\n",
    "    if ground_truth:\n",
    "        normalized_gt = [extract_numbers(gt) for gt in ground_truth]\n",
    "        used = set()\n",
    "        relevances = []\n",
    "        for r in output[:top_k]:\n",
    "            nums = extract_numbers(r[\"text\"])\n",
    "            hit = 0\n",
    "            for gt in normalized_gt:\n",
    "                if gt and gt in nums and gt not in used:\n",
    "                    hit = 1\n",
    "                    used.add(gt)\n",
    "                    break\n",
    "            relevances.append(hit)\n",
    "\n",
    "        precision = sum(relevances) / top_k if top_k > 0 else 0.0\n",
    "        recall = min(sum(relevances), len(normalized_gt)) / len(normalized_gt) if normalized_gt else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        rr = next((1.0 / (i+1) for i, rel in enumerate(relevances) if rel == 1), 0.0)\n",
    "        dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevances))\n",
    "        ideal_hits = min(len(normalized_gt), top_k)\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        metrics = {\"Precision@3\": precision, \"Recall@3\": recall, \"F1@3\": f1, \"MRR\": rr, \"nDCG@3\": ndcg}\n",
    "\n",
    "    return output, metrics\n",
    "\n",
    "# ================== ì¶œë ¥ í—¬í¼ ==================\n",
    "def simple_search_test(query: str, model, client, collection_name: str, top_k: int = 10,\n",
    "                       score_threshold: float = None, strict_children_of: str = None,\n",
    "                       fallback_min_hits: int = 6):\n",
    "    results, _ = dense_search(\n",
    "        query=query,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k,\n",
    "        score_threshold=score_threshold,\n",
    "        strict_children_of=strict_children_of,\n",
    "        fallback_min_hits=fallback_min_hits\n",
    "    )\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):\")\n",
    "    print(\"=\" * 100)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. ìŠ¤ì½”ì–´: {r.get('score', 0) if r.get('score') is not None else float('nan'):.4f}\")\n",
    "        print(f\"   ì—°ë„: {r.get('report_year', 'N/A')}\")\n",
    "        print(f\"   í…ìŠ¤íŠ¸: {r['text'][:200]}...\")\n",
    "        metadata_keys = ['account_id', 'account_name', 'parent_id', 'level', 'hierarchy',\n",
    "                         'is_total', 'is_subtotal', 'period_type', 'statement_type']\n",
    "        metadata_info = []\n",
    "        for key in metadata_keys:\n",
    "            if key in r:\n",
    "                metadata_info.append(f\"{key}: {r[key]}\")\n",
    "        if metadata_info:\n",
    "            print(f\"   ğŸ“Š ë©”íƒ€ë°ì´í„°: {', '.join(metadata_info)}\")\n",
    "        print(\"-\" * 80)\n",
    "    return results\n",
    "\n",
    "def rag_pipeline_simple(query: str, model, client, collection_name: str, top_k: int = 10,\n",
    "                        score_threshold: float = None, strict_children_of: str = None,\n",
    "                        fallback_min_hits: int = 6):\n",
    "    print(f\"ğŸ’¬ ì§ˆë¬¸: {query}\\n\")\n",
    "    results = simple_search_test(\n",
    "        query, model, client, collection_name, top_k=top_k,\n",
    "        score_threshold=score_threshold, strict_children_of=strict_children_of,\n",
    "        fallback_min_hits=fallback_min_hits\n",
    "    )\n",
    "    print(f\"\\nğŸ“‹ ìš”ì•½:\")\n",
    "    print(f\"   - ì´ {len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    if results:\n",
    "        years = list({r.get('report_year') for r in results if r.get('report_year')})\n",
    "        if years:\n",
    "            print(f\"   - ê´€ë ¨ ì—°ë„: {', '.join(map(str, sorted(years)))}\")\n",
    "        hierarchies = [r.get('hierarchy', []) for r in results if r.get('hierarchy')]\n",
    "        if hierarchies:\n",
    "            print(f\"   - ë°œê²¬ëœ ê³„ì¸µ ì •ë³´: {len(hierarchies)}ê°œ\")\n",
    "    return results\n",
    "\n",
    "# ===== Zephyr ëª¨ë¸ ë¡œë“œ =====\n",
    "model_path = Path(\"/Users/dan/Desktop/snu_project/models/zephyr-7b-beta.Q4_K_M.gguf\").resolve()\n",
    "\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "    model_path=str(model_path),\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=35  # Apple Siliconì˜ ê²½ìš° Metal GPU ì‚¬ìš©\n",
    ")\n",
    "\n",
    "# ================== ì‹¤í–‰ ì˜ˆì‹œ ==================\n",
    "if __name__ == \"__main__\":\n",
    "    questions = [\n",
    "        \"2014ë…„ ì¬ë¬´ìƒíƒœí‘œì—ì„œ ë‹¹ê¸° ë¹„ìœ ë™ìì‚°ì˜ í•˜ìœ„ê³„ì¸µ ì •ë³´ë¥¼ ì „ë¶€ ì¤˜. metadataì—ì„œ hierarchy / levelì„ ê¼­ ì°¸ê³ í•´.\"\n",
    "    ]\n",
    "    for q in questions:\n",
    "        print(\"=\" * 100)\n",
    "        results = rag_pipeline_simple(\n",
    "            q, embed_model, client, collection_name,\n",
    "            top_k=50,\n",
    "            score_threshold=0.0,      # ì»· ì—†ì´ ë‹¤ ëª¨ìœ¼ê³  ë¶€ì¡±í•˜ë©´ fallback\n",
    "            strict_children_of=None,  # parent ìë™ í•´ì„ì„ ì“°ë¯€ë¡œ êµ³ì´ í•„ìš” ì—†ìŒ\n",
    "            fallback_min_hits=8\n",
    "        )\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a4fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
