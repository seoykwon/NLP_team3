{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd62e312",
   "metadata": {},
   "source": [
    "\n",
    "# K-IFRS RAG — HF v2 (From Scratch Builder)\n",
    "\n",
    "**원본 v2 로직**을 그대로 사용하면서, **임베딩을 처음부터 생성**하도록 구성한 버전입니다.  \n",
    "- `REBUILD=True`로 두면, 캐시가 있어도 **항상 다시 계산**합니다.  \n",
    "- 임베딩: `intfloat/multilingual-e5-large` (무료, 다국어)  \n",
    "- 재랭커(옵션): `jinaai/jina-reranker-v2-base-multilingual` (`trust_remote_code=True`)  \n",
    "- 기능: **Alias 확장 + Two-Track 라우팅 + Vector-only Fallback + BM25+벡터 하이브리드**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5957303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0) 설치 (최초 1회 필요)\n",
    "!pip install -qU sentence-transformers transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json, os, math, numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List\n",
    "\n",
    "# ===== 경로/설정 =====\n",
    "BASE = Path('.../raws/raws_K-ifrs')  # 필요 시 수정\n",
    "JSON_PATH = BASE / 'kifrs_cleaned_final.json'  # 기준서 JSON\n",
    "CACHE_DIR = BASE / 'hf_cache'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ★ 항상 처음부터 다시 만들려면 True\n",
    "REBUILD = True\n",
    "\n",
    "print('JSON exists:', JSON_PATH.exists(), JSON_PATH)\n",
    "print('Cache dir:', CACHE_DIR)\n",
    "print('REBUILD =', REBUILD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0881900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== JSON 로드 =====\n",
    "with JSON_PATH.open(encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "docs = data.get('documents', [])\n",
    "total_paras = sum(len(d.get('paragraphs', [])) for d in docs)\n",
    "print('Loaded standards:', len(docs), '| Total paragraphs:', total_paras)\n",
    "\n",
    "# ===== 토큰화 & BM25 =====\n",
    "import re\n",
    "HAN_ENG_NUM = re.compile(r'[가-힣A-Za-z0-9]+', re.UNICODE)\n",
    "STOP = set(['그리고','등','및','또는','그러나','이는','그','이','저','것','수','등의'])\n",
    "\n",
    "def normalize(t: str) -> str:\n",
    "    return t.strip().lower()\n",
    "\n",
    "def tokenize(t: str):\n",
    "    return HAN_ENG_NUM.findall(normalize(t))\n",
    "\n",
    "def filter_tokens(tokens: List[str]) -> List[str]:\n",
    "    return [w for w in tokens if w not in STOP and len(w) > 1]\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, docs_tokens, k1=1.5, b=0.75):\n",
    "        self.docs_tokens = docs_tokens\n",
    "        self.N = len(docs_tokens)\n",
    "        self.k1 = k1; self.b = b\n",
    "        self.avgdl = sum(len(d) for d in docs_tokens) / max(1, self.N)\n",
    "        self.df = Counter()\n",
    "        for doc in docs_tokens:\n",
    "            for term in set(doc):\n",
    "                self.df[term] += 1\n",
    "        self.idf = {t: math.log(1 + (self.N - df + 0.5)/(df + 0.5)) for t, df in self.df.items()}\n",
    "        self.tf = [Counter(doc) for doc in docs_tokens]\n",
    "\n",
    "    def _score_doc(self, q_tokens, i):\n",
    "        score, tf, dl = 0.0, self.tf[i], len(self.docs_tokens[i])\n",
    "        for term in q_tokens:\n",
    "            idf = self.idf.get(term)\n",
    "            if idf is None: \n",
    "                continue\n",
    "            f = tf.get(term, 0)\n",
    "            if f == 0: \n",
    "                continue\n",
    "            denom = f + self.k1 * (1 - self.b + self.b * dl / self.avgdl)\n",
    "            score += idf * (f * (self.k1 + 1)) / denom\n",
    "        return score\n",
    "\n",
    "    def search(self, q_tokens, topk=50):\n",
    "        scores = []\n",
    "        for i in range(self.N):\n",
    "            s = self._score_doc(q_tokens, i)\n",
    "            if s != 0.0:\n",
    "                scores.append((i, s))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:topk]\n",
    "\n",
    "    def search_subset(self, q_tokens, allowed: set, topk=50):\n",
    "        scores = []\n",
    "        for i in allowed:\n",
    "            s = self._score_doc(q_tokens, i)\n",
    "            if s != 0.0:\n",
    "                scores.append((i, s))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:topk]\n",
    "\n",
    "# ===== 라우팅 인덱스 =====\n",
    "title_keys = []\n",
    "title_texts = []\n",
    "title_to_para_indices = defaultdict(list)\n",
    "paragraphs = []\n",
    "\n",
    "for d in docs:\n",
    "    std = d.get('standard_no')\n",
    "    ttl = d.get('title') or ''\n",
    "    src = d.get('source_file') or ''\n",
    "    key = (std, ttl, src)\n",
    "    if key not in title_keys:\n",
    "        title_keys.append(key)\n",
    "        head = (d.get('paragraphs', [{}])[:3])\n",
    "        head_txt = \" \".join([(p.get('text') or '') for p in head])\n",
    "        title_texts.append(f\"{ttl}\\n{src}\\n{head_txt[:1000]}\")\n",
    "    base_idx = len(paragraphs)\n",
    "    for p in d.get('paragraphs', []):\n",
    "        paragraphs.append({\n",
    "            \"std\": std, \"title\": ttl, \"source\": src,\n",
    "            \"page\": p.get('page'), \"para_id\": p.get('para_id'), \"text\": p.get('text') or ''\n",
    "        })\n",
    "        title_to_para_indices[key].append(base_idx); base_idx += 1\n",
    "\n",
    "title_tokens = [filter_tokens(tokenize(t)) for t in title_texts]\n",
    "bm25_title = BM25(title_tokens)\n",
    "\n",
    "para_texts = [p['text'] for p in paragraphs]\n",
    "para_tokens = [filter_tokens(tokenize(t)) for t in para_texts]\n",
    "bm25_para = BM25(para_tokens)\n",
    "\n",
    "print('Titles:', len(title_keys), '| Paragraphs:', len(paragraphs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b988fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 임베딩 모델 =====\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "EMBED_MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
    "RERANK_MODEL_NAME = \"jinaai/jina-reranker-v2-base-multilingual\"  # 옵션\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "def embed_queries(texts: List[str]) -> np.ndarray:\n",
    "    inputs = [f\"query: {t}\" for t in texts]\n",
    "    return embed_model.encode(inputs, normalize_embeddings=True, convert_to_numpy=True, batch_size=64)\n",
    "\n",
    "def embed_passages(texts: List[str]) -> np.ndarray:\n",
    "    inputs = [f\"passage: {t}\" for t in texts]\n",
    "    return embed_model.encode(inputs, normalize_embeddings=True, convert_to_numpy=True, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fafc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 캐시 경로 & (재)생성 =====\n",
    "TITLE_EMB_PATH = (CACHE_DIR / f\"title_emb_{EMBED_MODEL_NAME.replace('/','_')}.npy\")\n",
    "PARA_EMB_PATH  = (CACHE_DIR / f\"para_emb_{EMBED_MODEL_NAME.replace('/','_')}.npy\")\n",
    "\n",
    "def build_or_load_embeddings(rebuild: bool = False):\n",
    "    if rebuild or (not TITLE_EMB_PATH.exists()):\n",
    "        print(\"Encoding titles from scratch...\")\n",
    "        title_vecs = embed_passages(title_texts)\n",
    "        np.save(TITLE_EMB_PATH, title_vecs)\n",
    "    else:\n",
    "        title_vecs = np.load(TITLE_EMB_PATH)\n",
    "\n",
    "    if rebuild or (not PARA_EMB_PATH.exists()):\n",
    "        print(\"Encoding paragraphs from scratch...\")\n",
    "        para_vecs = embed_passages(para_texts)\n",
    "        np.save(PARA_EMB_PATH, para_vecs)\n",
    "    else:\n",
    "        para_vecs = np.load(PARA_EMB_PATH)\n",
    "    return title_vecs, para_vecs\n",
    "\n",
    "title_vecs, para_vecs = build_or_load_embeddings(REBUILD)\n",
    "title_vecs.shape, para_vecs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 유사도 =====\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = float(np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0: return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def topk_by_cosine(q_vec: np.ndarray, mat: np.ndarray, k: int = 50):\n",
    "    sims = (mat @ q_vec)  # 정규화 가정\n",
    "    idxs = np.argpartition(-sims, kth=min(k, len(sims)-1))[:k]\n",
    "    idxs = idxs[np.argsort(-sims[idxs])]\n",
    "    return [(int(i), float(sims[i])) for i in idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Alias =====\n",
    "ALIAS = {\n",
    "    \"개발비\": [\"개발활동\", \"무형자산\"],\n",
    "    \"감가상각\": [\"정액법\", \"체감잔액법\", \"생산량비례법\"],\n",
    "    \"매출\": [\"수익\"],\n",
    "    \"선수수익\": [\"계약부채\"],\n",
    "    \"미수수익\": [\"계약자산\"],\n",
    "    \"비연결재무제표\": [\"별도재무제표\"],\n",
    "}\n",
    "\n",
    "def expand_query(q: str) -> str:\n",
    "    terms = set()\n",
    "    for k, syns in ALIAS.items():\n",
    "        if k in q:\n",
    "            terms.update(syns)\n",
    "    if terms:\n",
    "        return q + \" \" + \" \".join(sorted(terms))\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 라우팅 & 하이브리드 검색 =====\n",
    "def route_titles(query: str, topn: int = 5, w_bm25=0.4, w_vec=0.6, debug=False):\n",
    "    q_exp = expand_query(query)\n",
    "    qtok = filter_tokens(tokenize(q_exp))\n",
    "    bm_hits = bm25_title.search(qtok, topk=max(topn*6, 30))\n",
    "\n",
    "    qv = embed_queries([q_exp])[0]\n",
    "    vec_hits = topk_by_cosine(qv, title_vecs, k=max(topn*6, 30))\n",
    "    vec_dict = dict(vec_hits)\n",
    "\n",
    "    combined = []\n",
    "    seen = set([i for i,_ in bm_hits])\n",
    "    for i, s_bm in bm_hits:\n",
    "        s_vec = vec_dict.get(i, 0.0)\n",
    "        combined.append((i, w_bm25*s_bm + w_vec*s_vec, s_bm, s_vec))\n",
    "    for i, s_vec in vec_hits:\n",
    "        if i in seen: \n",
    "            continue\n",
    "        combined.append((i, w_vec*s_vec, 0.0, s_vec))\n",
    "\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    routed = combined[:topn]\n",
    "    if debug:\n",
    "        print(\"[Routing] top candidates:\")\n",
    "        for r in routed:\n",
    "            i, s, sb, sv = r\n",
    "            std, ttl, src = title_keys[i]\n",
    "            print(f\" - {std} | {ttl} | score={s:.3f} (bm25={sb:.3f}, vec={sv:.3f})\")\n",
    "    return routed\n",
    "\n",
    "def ensure_reranker():\n",
    "    global reranker\n",
    "    if 'reranker' not in globals() or reranker is None:\n",
    "        print(\"Loading reranker:\", RERANK_MODEL_NAME)\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        reranker = CrossEncoder(RERANK_MODEL_NAME, trust_remote_code=True)\n",
    "    return reranker\n",
    "\n",
    "def hybrid_search_within_titles(query: str, title_idxs: List[int],\n",
    "                                topk_bm25=150, topk_final=12,\n",
    "                                w_bm25=0.4, w_vec=0.6,\n",
    "                                use_reranker: bool = True, rerank_top: int = 20,\n",
    "                                debug=False):\n",
    "    allowed = set()\n",
    "    for ti in title_idxs:\n",
    "        allowed.update(title_to_para_indices[title_keys[ti]])\n",
    "    if debug:\n",
    "        print(f\"[Within] allowed paragraphs: {len(allowed)}\")\n",
    "\n",
    "    if not allowed:\n",
    "        if debug: print(\"[Within] allowed empty -> fallback to global search\")\n",
    "        return global_hybrid_search(query, topk_bm25=200, topk_final=15,\n",
    "                                    w_bm25=w_bm25, w_vec=w_vec,\n",
    "                                    use_reranker=use_reranker, rerank_top=rerank_top)\n",
    "\n",
    "    q_exp = expand_query(query)\n",
    "    qtok = filter_tokens(tokenize(q_exp))\n",
    "    bm_hits = bm25_para.search_subset(qtok, allowed, topk=topk_bm25)\n",
    "\n",
    "    # ★ BM25=0이면 벡터만으로 랭킹\n",
    "    if not bm_hits:\n",
    "        if debug: print(\"[Within] BM25 hits=0 -> vector-only ranking\")\n",
    "        qv = embed_queries([q_exp])[0]\n",
    "        idxs = list(allowed)\n",
    "        sims = np.dot(para_vecs[idxs], qv)\n",
    "        order = np.argsort(-sims)[:topk_final]\n",
    "        results = [{**paragraphs[idxs[i]], \"score\": float(sims[idxs[i]]), \"bm25\": 0.0, \"vector\": float(sims[idxs[i]])} for i in order]\n",
    "        if use_reranker and len(results) > 1:\n",
    "            ensure_reranker()\n",
    "            pairs = [(q_exp, r[\"text\"]) for r in results[:rerank_top]]\n",
    "            scores = reranker.predict(pairs)\n",
    "            order = np.argsort(-scores)\n",
    "            results = [results[int(i)] for i in order]\n",
    "        return results\n",
    "\n",
    "    qv = embed_queries([q_exp])[0]\n",
    "    vec_scores = {i: float(np.dot(para_vecs[i], qv)) for i, _ in bm_hits}\n",
    "\n",
    "    combined = []\n",
    "    for i, s_bm in bm_hits:\n",
    "        s_vec = vec_scores.get(i, 0.0)\n",
    "        combined.append((i, w_bm25*s_bm + w_vec*s_vec, s_bm, s_vec))\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    combined = combined[:topk_final]\n",
    "\n",
    "    results = [{**paragraphs[i], \"score\": s, \"bm25\": s_bm, \"vector\": s_vec} for (i, s, s_bm, s_vec) in combined]\n",
    "\n",
    "    if use_reranker and len(results) > 1:\n",
    "        ensure_reranker()\n",
    "        pairs = [(q_exp, r[\"text\"]) for r in results[:rerank_top]]\n",
    "        scores = reranker.predict(pairs)\n",
    "        order = np.argsort(-scores)\n",
    "        results = [results[int(i)] for i in order]\n",
    "    return results\n",
    "\n",
    "def global_hybrid_search(query: str, topk_bm25=200, topk_final=15,\n",
    "                         w_bm25=0.4, w_vec=0.6, use_reranker=True, rerank_top=20, debug=False):\n",
    "    q_exp = expand_query(query)\n",
    "    qtok = filter_tokens(tokenize(q_exp))\n",
    "    bm_hits = bm25_para.search(qtok, topk=topk_bm25)\n",
    "\n",
    "    # ★ BM25=0이면 벡터만으로 랭킹\n",
    "    if not bm_hits:\n",
    "        if debug: print(\"[Global] BM25 hits=0 -> vector-only ranking\")\n",
    "        qv = embed_queries([q_exp])[0]\n",
    "        sims = np.dot(para_vecs, qv)\n",
    "        order = np.argsort(-sims)[:topk_final]\n",
    "        results = [{**paragraphs[i], \"score\": float(sims[i]), \"bm25\": 0.0, \"vector\": float(sims[i])} for i in order]\n",
    "        if use_reranker and len(results) > 1:\n",
    "            ensure_reranker()\n",
    "            pairs = [(q_exp, r[\"text\"]) for r in results[:rerank_top]]\n",
    "            scores = reranker.predict(pairs)\n",
    "            order = np.argsort(-scores)\n",
    "            results = [results[int(i)] for i in order]\n",
    "        return results\n",
    "\n",
    "    qv = embed_queries([q_exp])[0]\n",
    "    vec_scores = {i: float(np.dot(para_vecs[i], qv)) for i, _ in bm_hits}\n",
    "\n",
    "    combined = []\n",
    "    for i, s_bm in bm_hits:\n",
    "        s_vec = vec_scores.get(i, 0.0)\n",
    "        combined.append((i, w_bm25*s_bm + w_vec*s_vec, s_bm, s_vec))\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    combined = combined[:topk_final]\n",
    "\n",
    "    results = [{**paragraphs[i], \"score\": s, \"bm25\": s_bm, \"vector\": s_vec} for (i, s, s_bm, s_vec) in combined]\n",
    "\n",
    "    if use_reranker and len(results) > 1:\n",
    "        ensure_reranker()\n",
    "        pairs = [(q_exp, r[\"text\"]) for r in results[:rerank_top]]\n",
    "        scores = reranker.predict(pairs)\n",
    "        order = np.argsort(-scores)\n",
    "        results = [results[int(i)] for i in order]\n",
    "    return results\n",
    "\n",
    "def hierarchical_search_two_track(query: str, top_titles=4, **kw):\n",
    "    print(f\"[Query] {query}\")\n",
    "    routed = route_titles(query, topn=top_titles, debug=True)\n",
    "    title_idx = [i for (i, *_ ) in routed]\n",
    "    if not title_idx:\n",
    "        print(\"[Two-Track] routed empty -> GLOBAL\")\n",
    "        return routed, global_hybrid_search(query, **kw)\n",
    "    results = hybrid_search_within_titles(query, title_idx, **kw)\n",
    "    return routed, results\n",
    "\n",
    "def print_titles(routed):\n",
    "    for rank, (i, s, s_bm, s_vec) in enumerate(routed, 1):\n",
    "        std, ttl, src = title_keys[i]\n",
    "        print(f\"{rank:>2}. [{std}] {ttl}  <{src}>  score={s:.3f} (bm25={s_bm:.3f}, vec={s_vec:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a23d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Demo =====\n",
    "queries = [\n",
    "    \"감가상각 방법에는 무엇이 있나?\",\n",
    "    \"수익 인식의 5단계는?\",\n",
    "    \"개발비 자산 인식 요건은?\",\n",
    "    \"선수수익과 계약부채 관계\",\n",
    "    \"환불부채 회계처리는 어떻게 되나?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n=== Q:\", q)\n",
    "    routed, results = hierarchical_search_two_track(q, top_titles=4,\n",
    "                                                    topk_bm25=150,\n",
    "                                                    topk_final=10,\n",
    "                                                    w_bm25=0.4, w_vec=0.6,\n",
    "                                                    use_reranker=True, rerank_top=10)\n",
    "    print(\"Top titles:\")\n",
    "    print_titles(routed)\n",
    "    for i, r in enumerate(results[:5], 1):\n",
    "        snippet = r['text'].replace('\\n', ' ')\n",
    "        if len(snippet) > 240: snippet = snippet[:240] + '…'\n",
    "        print(f\"  {i:>2}. [{r['std']}:{r['para_id']}] ({r['title']}) p.{r.get('page','?')} score={r['score']:.3f}\")\n",
    "        print(\"     \", snippet)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
